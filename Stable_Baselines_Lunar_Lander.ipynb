{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9iLyO1hrZekJy48EFkjh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emreCanIlik/EfficientMelanomaDetection-Using-EfficientNet-PyTorch/blob/main/Stable_Baselines_Lunar_Lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training DQN and PPO Agents on the LunarLander Environment: A Comparative Analysis"
      ],
      "metadata": {
        "id": "WYcKJrdf_Nlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d-py\n",
        "!apt-get update\n",
        "!apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01L7yNsdF88n",
        "outputId": "570e360e-157a-4d0b-980b-5cb20edc9232"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (496 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121920 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2349120 sha256=05e6888b12f1e6e29aaea16c20ce96a1023adbd86e4615eabdd019699a1175eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,037 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,756 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,371 kB]\n",
            "Fetched 7,306 kB in 3s (2,238 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=fce908ba6964d50e7079278b9ff2e68a72a5b4a53873a1b25e68c5d3e6ff7c2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x2JdsKBQzNg",
        "outputId": "ae4b7634-bde6-45a7-8dbd-fcea009429da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=a569f60b2e42fdad068a48e2e750e727fb45da212b6f3d998ef93517a6a0ecaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"RL_Agent\")"
      ],
      "metadata": {
        "id": "UY7AcMsW_NGB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/env_creator.py\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "def create_env(env_name: str, n_envs: int):\n",
        "    \"\"\"Creates environment for RL experiments.\n",
        "    Takes in environment name and number of environments, and\n",
        "    returns environment and evaluation environment.\"\"\"\n",
        "\n",
        "    env = make_vec_env(env_name, n_envs)\n",
        "    eval_env = make_vec_env(env_name, n_envs)\n",
        "\n",
        "    return env, eval_env\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fm--6eY_t8D",
        "outputId": "2e9bc65a-d1d9-49fc-fccf-91a3a9f6b78b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/env_creator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A942mxG-Gp0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "n_envs = 4"
      ],
      "metadata": {
        "id": "zvhE-KfYBzTs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import env_creator\n",
        "\n",
        "env, eval_env = env_creator.create_env(env_name=env_name, n_envs=n_envs)"
      ],
      "metadata": {
        "id": "dZItw8pbB1tP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env, eval_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_efPBO_THJjJ",
        "outputId": "dd52886b-63df-48fa-bde7-60470793f23e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x7e42e0a1e350>,\n",
              " <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x7e42e0a1e9b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(env), type(eval_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx4aUJ-8HKwS",
        "outputId": "4f47f7c6-46e7-4b2b-d18a-e81e3c487d52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv,\n",
              " stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "U2DnWHnwHOeO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_callback = EvalCallback(eval_env, best_model_save_path='Eval_logs_best_DQN',\n",
        "                                 log_path='Eval_logs_DQN', eval_freq=10,\n",
        "                                 verbose=1)"
      ],
      "metadata": {
        "id": "5IogrSFvHsaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/evaluator.py\n",
        "\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "def create_eval_callback(eval_env, best_model_save_path: str, log_path: str,\n",
        "                         eval_freq: int, verbose: int):\n",
        "\n",
        "    \"\"\"Creates an evaluation callback for RL training.\"\"\"\n",
        "    eval_callback = EvalCallback(\n",
        "        eval_env=eval_env,\n",
        "        best_model_save_path=best_model_save_path,\n",
        "        log_path=log_path,\n",
        "        eval_freq=eval_freq,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    return eval_callback\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWHLlj0FHs7l",
        "outputId": "0b68527c-6648-4e70-e3f4-0155ddf52c6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/evaluator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_save_path = \"best_model\"\n",
        "log_path = \"logs\"\n",
        "eval_freq = 100\n",
        "verbose = 1"
      ],
      "metadata": {
        "id": "W15H1Yu-I_l1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import evaluator\n",
        "\n",
        "eval_callback = evaluator.create_eval_callback(eval_env = eval_env,\n",
        "                                               best_model_save_path=best_model_save_path,\n",
        "                                               log_path=log_path,\n",
        "                                               eval_freq=eval_freq,\n",
        "                                               verbose=verbose)"
      ],
      "metadata": {
        "id": "sCpHNF0YJU3R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_callback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsXPzHeHJuuZ",
        "outputId": "2704f025-7b5d-4b2f-9450-2c75323ed36b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.common.callbacks.EvalCallback at 0x7e42e0a1cd30>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/agent_creator.py\n",
        "\n",
        "import torch\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "def create_agents(policy: str, env, learning_rate: float,\n",
        "                     tensorboard_log: str, verbose: int, device: torch.device):\n",
        "\n",
        "    \"\"\" Creates DQN and PPO agents \"\"\"\n",
        "\n",
        "    agent_dqn = DQN(policy, env, learning_rate=learning_rate,\n",
        "                tensorboard_log=tensorboard_log, verbose=verbose, device=device)\n",
        "\n",
        "    agent_ppo = PPO(policy, env, learning_rate=learning_rate,\n",
        "                tensorboard_log=tensorboard_log, verbose=verbose, device=device)\n",
        "\n",
        "\n",
        "    return agent_dqn, agent_ppo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMtoNOltJvvX",
        "outputId": "965110ba-3ca6-4b47-a077-a675862f664f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/agent_creator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "policy = \"MlpPolicy\"\n",
        "env = env\n",
        "learning_rate = 0.0001\n",
        "tensorboard_log = \"logs_tensorboard\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "y5WOlWA8Mzhp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import agent_creator\n",
        "\n",
        "dqn_agent, ppo_agent = agent_creator.create_agents(policy=policy,\n",
        "                            env=env,\n",
        "                            learning_rate=learning_rate,\n",
        "                            tensorboard_log=tensorboard_log,\n",
        "                            device=device,\n",
        "                            verbose=verbose)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLnjsg1GKYrs",
        "outputId": "42a5e40e-9715-41e5-c7aa-a038183b1086"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_agent, ppo_agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGwhQGOaOn0N",
        "outputId": "5134dbf1-6333-412f-82a2-815ea28ce43f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<stable_baselines3.dqn.dqn.DQN at 0x7e42e0a1d0f0>,\n",
              " <stable_baselines3.ppo.ppo.PPO at 0x7e418fd1ddb0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile RL_Agent/engine.py\n",
        "\n",
        "def train_agent(agent, total_timesteps: int, callback, tb_log_name: str):\n",
        "\n",
        "    \"\"\"Train the agent with specified parameters.\"\"\"\n",
        "\n",
        "    agent.learn(total_timesteps=total_timesteps, callback=callback, tb_log_name=tb_log_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-rfw_5zZWW",
        "outputId": "134bbd65-e368-4ec6-9e84-1a6c2e6fe5dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RL_Agent/engine.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tb_log_name = \"runs\"\n",
        "callback = eval_callback\n",
        "total_timesteps = 100_000"
      ],
      "metadata": {
        "id": "2bWJ-BHhz1TN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from RL_Agent import engine\n",
        "\n",
        "# Train the DQN agent\n",
        "engine.train_agent(agent=dqn_agent,\n",
        "                   total_timesteps=total_timesteps,\n",
        "                   callback=callback,\n",
        "                   tb_log_name=tb_log_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZlR_2Y51Q_F",
        "outputId": "ef6644a7-322f-41a5-a2a4-3863fe36c771"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to logs_tensorboard/runs_3\n",
            "Eval num_timesteps=400, episode_reward=-21.75 +/- 158.80\n",
            "Episode length: 786.20 +/- 262.81\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 786      |\n",
            "|    mean_reward      | -21.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.962    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 400      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.51     |\n",
            "|    n_updates        | 6355     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.5     |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.956    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 61       |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 468      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.38     |\n",
            "|    n_updates        | 6360     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=800, episode_reward=-159.68 +/- 91.77\n",
            "Episode length: 553.00 +/- 237.34\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 553      |\n",
            "|    mean_reward      | -160     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.924    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 800      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.645    |\n",
            "|    n_updates        | 6380     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.6     |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.913    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 62       |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total_timesteps  | 912      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.29     |\n",
            "|    n_updates        | 6387     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1200, episode_reward=97.45 +/- 121.53\n",
            "Episode length: 511.20 +/- 253.59\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 511      |\n",
            "|    mean_reward      | 97.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.886    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 1200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.35     |\n",
            "|    n_updates        | 6405     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.1     |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration_rate | 0.864    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 66       |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total_timesteps  | 1428     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 6420     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=1600, episode_reward=14.72 +/- 169.90\n",
            "Episode length: 482.20 +/- 214.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 482      |\n",
            "|    mean_reward      | 14.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.848    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 1600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.977    |\n",
            "|    n_updates        | 6430     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.833    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 69       |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total_timesteps  | 1756     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 6440     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=21.48 +/- 152.74\n",
            "Episode length: 578.40 +/- 216.67\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 578      |\n",
            "|    mean_reward      | 21.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.81     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 2000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 6455     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.6     |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.804    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 64       |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 2068     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.69     |\n",
            "|    n_updates        | 6460     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2400, episode_reward=-118.32 +/- 82.00\n",
            "Episode length: 497.40 +/- 110.44\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 497      |\n",
            "|    mean_reward      | -118     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.772    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 2400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.745    |\n",
            "|    n_updates        | 6480     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.1     |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.767    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 69       |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 2456     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.42     |\n",
            "|    n_updates        | 6484     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2800, episode_reward=12.43 +/- 154.68\n",
            "Episode length: 703.20 +/- 227.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 703      |\n",
            "|    mean_reward      | 12.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.734    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 2800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.962    |\n",
            "|    n_updates        | 6505     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.73     |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 70       |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 2844     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.78     |\n",
            "|    n_updates        | 6508     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3200, episode_reward=-107.13 +/- 73.58\n",
            "Episode length: 564.40 +/- 238.11\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 564      |\n",
            "|    mean_reward      | -107     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.696    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 3200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 6530     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.664    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 3536     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.56     |\n",
            "|    n_updates        | 6551     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3600, episode_reward=49.50 +/- 87.62\n",
            "Episode length: 474.00 +/- 278.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 474      |\n",
            "|    mean_reward      | 49.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.658    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 3600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 6555     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.626    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 3936     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 6576     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-34.64 +/- 204.84\n",
            "Episode length: 511.80 +/- 279.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 512      |\n",
            "|    mean_reward      | -34.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.62     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 4000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 6580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4400, episode_reward=53.71 +/- 62.12\n",
            "Episode length: 537.00 +/- 284.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 537      |\n",
            "|    mean_reward      | 53.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.582    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 4400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 6605     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -111     |\n",
            "|    exploration_rate | 0.561    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 82       |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 4620     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.875    |\n",
            "|    n_updates        | 6619     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=4800, episode_reward=93.76 +/- 109.74\n",
            "Episode length: 373.20 +/- 134.73\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 373      |\n",
            "|    mean_reward      | 93.8     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.544    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 4800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 6630     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 112      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.511    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 88       |\n",
            "|    time_elapsed     | 58       |\n",
            "|    total_timesteps  | 5148     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.819    |\n",
            "|    n_updates        | 6652     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=5200, episode_reward=95.21 +/- 135.74\n",
            "Episode length: 607.60 +/- 263.56\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 608      |\n",
            "|    mean_reward      | 95.2     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.506    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 5200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 6655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=5600, episode_reward=43.75 +/- 156.68\n",
            "Episode length: 614.00 +/- 330.88\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 614      |\n",
            "|    mean_reward      | 43.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.468    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 5600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.726    |\n",
            "|    n_updates        | 6680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-25.52 +/- 105.91\n",
            "Episode length: 443.60 +/- 94.29\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 444      |\n",
            "|    mean_reward      | -25.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.43     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 6000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 6705     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 118      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.419    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 6116     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.934    |\n",
            "|    n_updates        | 6713     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6400, episode_reward=-2.93 +/- 127.82\n",
            "Episode length: 759.40 +/- 303.90\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 759      |\n",
            "|    mean_reward      | -2.93    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.392    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 6400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.786    |\n",
            "|    n_updates        | 6730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=6800, episode_reward=56.14 +/- 81.27\n",
            "Episode length: 613.60 +/- 328.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 614      |\n",
            "|    mean_reward      | 56.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.354    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 6800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.983    |\n",
            "|    n_updates        | 6755     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=7200, episode_reward=87.39 +/- 85.15\n",
            "Episode length: 411.80 +/- 105.13\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 412      |\n",
            "|    mean_reward      | 87.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.316    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 7200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 6780     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 129      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.29     |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 84       |\n",
            "|    time_elapsed     | 88       |\n",
            "|    total_timesteps  | 7476     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.603    |\n",
            "|    n_updates        | 6798     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=7600, episode_reward=-52.22 +/- 52.14\n",
            "Episode length: 329.00 +/- 92.36\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 329      |\n",
            "|    mean_reward      | -52.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.278    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 7600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.609    |\n",
            "|    n_updates        | 6805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=39.09 +/- 154.99\n",
            "Episode length: 592.40 +/- 336.84\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 592      |\n",
            "|    mean_reward      | 39.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.24     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 8000     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 6830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8400, episode_reward=-8.07 +/- 129.92\n",
            "Episode length: 412.00 +/- 92.61\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 412      |\n",
            "|    mean_reward      | -8.07    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.202    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 8400     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.724    |\n",
            "|    n_updates        | 6855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=8800, episode_reward=-69.54 +/- 192.25\n",
            "Episode length: 501.40 +/- 256.49\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 501      |\n",
            "|    mean_reward      | -69.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.164    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 8800     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.06     |\n",
            "|    n_updates        | 6880     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=9200, episode_reward=-71.69 +/- 144.29\n",
            "Episode length: 482.60 +/- 271.24\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 483      |\n",
            "|    mean_reward      | -71.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.126    |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 9200     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 6905     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=9600, episode_reward=54.60 +/- 98.95\n",
            "Episode length: 606.40 +/- 327.19\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 606      |\n",
            "|    mean_reward      | 54.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.0884   |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 9600     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.39     |\n",
            "|    n_updates        | 6930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-41.63 +/- 138.46\n",
            "Episode length: 300.80 +/- 49.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 301      |\n",
            "|    mean_reward      | -41.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.0504   |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 6955     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10400, episode_reward=16.89 +/- 78.25\n",
            "Episode length: 670.00 +/- 278.14\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 670      |\n",
            "|    mean_reward      | 16.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 6980     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 160      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 125      |\n",
            "|    total_timesteps  | 10468    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 6985     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10800, episode_reward=5.27 +/- 141.64\n",
            "Episode length: 461.00 +/- 270.37\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 461      |\n",
            "|    mean_reward      | 5.27     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 10800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.793    |\n",
            "|    n_updates        | 7005     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=11200, episode_reward=110.73 +/- 103.00\n",
            "Episode length: 576.80 +/- 296.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 577      |\n",
            "|    mean_reward      | 111      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 11200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23     |\n",
            "|    n_updates        | 7030     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=11600, episode_reward=-38.46 +/- 153.06\n",
            "Episode length: 623.80 +/- 315.77\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 624      |\n",
            "|    mean_reward      | -38.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 11600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.15     |\n",
            "|    n_updates        | 7055     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 188      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 141      |\n",
            "|    total_timesteps  | 11844    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 7071     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=93.36 +/- 155.07\n",
            "Episode length: 779.60 +/- 281.99\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 780      |\n",
            "|    mean_reward      | 93.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 12000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.686    |\n",
            "|    n_updates        | 7080     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12400, episode_reward=-5.92 +/- 135.25\n",
            "Episode length: 432.00 +/- 73.22\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 432      |\n",
            "|    mean_reward      | -5.92    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 12400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.968    |\n",
            "|    n_updates        | 7105     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=12800, episode_reward=70.27 +/- 62.96\n",
            "Episode length: 758.40 +/- 304.86\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 758      |\n",
            "|    mean_reward      | 70.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 12800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 7130     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 198      |\n",
            "|    ep_rew_mean      | -94.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 79       |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 13048    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.956    |\n",
            "|    n_updates        | 7146     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=13200, episode_reward=88.70 +/- 156.11\n",
            "Episode length: 633.20 +/- 214.32\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 633      |\n",
            "|    mean_reward      | 88.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 13200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.87     |\n",
            "|    n_updates        | 7155     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=13600, episode_reward=48.95 +/- 183.22\n",
            "Episode length: 499.40 +/- 251.91\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 499      |\n",
            "|    mean_reward      | 49       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 13600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 7180     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=25.25 +/- 105.82\n",
            "Episode length: 671.20 +/- 294.39\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 671      |\n",
            "|    mean_reward      | 25.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 14000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.977    |\n",
            "|    n_updates        | 7205     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=14400, episode_reward=0.90 +/- 173.40\n",
            "Episode length: 414.20 +/- 71.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 414      |\n",
            "|    mean_reward      | 0.904    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 14400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.916    |\n",
            "|    n_updates        | 7230     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 208      |\n",
            "|    ep_rew_mean      | -89.9    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 180      |\n",
            "|    total_timesteps  | 14508    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 7237     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=14800, episode_reward=77.13 +/- 91.45\n",
            "Episode length: 504.00 +/- 250.50\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 504      |\n",
            "|    mean_reward      | 77.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 14800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 7255     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=15200, episode_reward=46.67 +/- 131.04\n",
            "Episode length: 748.80 +/- 311.39\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 749      |\n",
            "|    mean_reward      | 46.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 15200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.554    |\n",
            "|    n_updates        | 7280     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=15600, episode_reward=35.14 +/- 203.56\n",
            "Episode length: 413.00 +/- 33.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 413      |\n",
            "|    mean_reward      | 35.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 15600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.945    |\n",
            "|    n_updates        | 7305     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=112.58 +/- 118.78\n",
            "Episode length: 655.60 +/- 281.67\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 656      |\n",
            "|    mean_reward      | 113      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 16000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 7330     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=16400, episode_reward=6.34 +/- 79.33\n",
            "Episode length: 369.60 +/- 77.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 370      |\n",
            "|    mean_reward      | 6.34     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 16400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.99     |\n",
            "|    n_updates        | 7355     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=16800, episode_reward=122.05 +/- 76.14\n",
            "Episode length: 474.40 +/- 185.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 474      |\n",
            "|    mean_reward      | 122      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 16800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.83     |\n",
            "|    n_updates        | 7380     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 221      |\n",
            "|    ep_rew_mean      | -81.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 210      |\n",
            "|    total_timesteps  | 16964    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 7391     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=17200, episode_reward=41.68 +/- 145.03\n",
            "Episode length: 672.40 +/- 200.44\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 672      |\n",
            "|    mean_reward      | 41.7     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 17200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09     |\n",
            "|    n_updates        | 7405     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=17600, episode_reward=67.58 +/- 102.26\n",
            "Episode length: 564.00 +/- 236.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 564      |\n",
            "|    mean_reward      | 67.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 17600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 7430     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=151.77 +/- 61.82\n",
            "Episode length: 614.80 +/- 201.16\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 615      |\n",
            "|    mean_reward      | 152      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 18000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.823    |\n",
            "|    n_updates        | 7455     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=18400, episode_reward=24.87 +/- 181.39\n",
            "Episode length: 494.80 +/- 256.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 495      |\n",
            "|    mean_reward      | 24.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 18400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.736    |\n",
            "|    n_updates        | 7480     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=18800, episode_reward=26.82 +/- 180.77\n",
            "Episode length: 644.80 +/- 305.18\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 645      |\n",
            "|    mean_reward      | 26.8     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 18800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.35     |\n",
            "|    n_updates        | 7505     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=19200, episode_reward=-32.73 +/- 172.22\n",
            "Episode length: 536.80 +/- 248.13\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 537      |\n",
            "|    mean_reward      | -32.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 19200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.66     |\n",
            "|    n_updates        | 7530     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=19600, episode_reward=40.03 +/- 162.40\n",
            "Episode length: 528.60 +/- 264.56\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 529      |\n",
            "|    mean_reward      | 40       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 19600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.24     |\n",
            "|    n_updates        | 7555     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 244      |\n",
            "|    ep_rew_mean      | -69      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 246      |\n",
            "|    total_timesteps  | 19788    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.57     |\n",
            "|    n_updates        | 7567     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-63.10 +/- 84.54\n",
            "Episode length: 656.80 +/- 283.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 657      |\n",
            "|    mean_reward      | -63.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.44     |\n",
            "|    n_updates        | 7580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20400, episode_reward=94.32 +/- 31.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | 94.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 7605     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20800, episode_reward=56.51 +/- 108.62\n",
            "Episode length: 803.20 +/- 241.04\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 803      |\n",
            "|    mean_reward      | 56.5     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 20800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.22     |\n",
            "|    n_updates        | 7630     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=21200, episode_reward=29.02 +/- 140.53\n",
            "Episode length: 807.00 +/- 219.78\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 807      |\n",
            "|    mean_reward      | 29       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 21200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 7655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=21600, episode_reward=-12.68 +/- 94.57\n",
            "Episode length: 697.20 +/- 313.51\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 697      |\n",
            "|    mean_reward      | -12.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 21600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.838    |\n",
            "|    n_updates        | 7680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=84.33 +/- 100.07\n",
            "Episode length: 584.40 +/- 238.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 584      |\n",
            "|    mean_reward      | 84.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 7705     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 259      |\n",
            "|    ep_rew_mean      | -64.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 76       |\n",
            "|    time_elapsed     | 288      |\n",
            "|    total_timesteps  | 22032    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 7707     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22400, episode_reward=2.35 +/- 105.25\n",
            "Episode length: 497.00 +/- 264.37\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 497      |\n",
            "|    mean_reward      | 2.35     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.882    |\n",
            "|    n_updates        | 7730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=22800, episode_reward=-23.24 +/- 142.98\n",
            "Episode length: 358.20 +/- 78.36\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 358      |\n",
            "|    mean_reward      | -23.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 22800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.835    |\n",
            "|    n_updates        | 7755     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=23200, episode_reward=-37.04 +/- 129.79\n",
            "Episode length: 368.20 +/- 58.59\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 368      |\n",
            "|    mean_reward      | -37      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 23200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.979    |\n",
            "|    n_updates        | 7780     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=23600, episode_reward=76.86 +/- 147.97\n",
            "Episode length: 583.60 +/- 216.13\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 584      |\n",
            "|    mean_reward      | 76.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 23600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 7805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-10.42 +/- 111.57\n",
            "Episode length: 545.60 +/- 238.10\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 546      |\n",
            "|    mean_reward      | -10.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 24000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.881    |\n",
            "|    n_updates        | 7830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24400, episode_reward=102.86 +/- 88.77\n",
            "Episode length: 573.40 +/- 240.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 573      |\n",
            "|    mean_reward      | 103      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 24400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.775    |\n",
            "|    n_updates        | 7855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=24800, episode_reward=115.78 +/- 29.94\n",
            "Episode length: 735.80 +/- 229.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 736      |\n",
            "|    mean_reward      | 116      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 24800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.973    |\n",
            "|    n_updates        | 7880     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=25200, episode_reward=8.72 +/- 126.18\n",
            "Episode length: 933.60 +/- 132.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 934      |\n",
            "|    mean_reward      | 8.72     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 25200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 7905     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 290      |\n",
            "|    ep_rew_mean      | -61.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 75       |\n",
            "|    time_elapsed     | 336      |\n",
            "|    total_timesteps  | 25556    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 7928     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=25600, episode_reward=-3.64 +/- 100.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -3.64    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 25600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.672    |\n",
            "|    n_updates        | 7930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=45.87 +/- 132.42\n",
            "Episode length: 644.20 +/- 293.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 644      |\n",
            "|    mean_reward      | 45.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 26000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.843    |\n",
            "|    n_updates        | 7955     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26400, episode_reward=95.97 +/- 63.27\n",
            "Episode length: 755.60 +/- 303.90\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 756      |\n",
            "|    mean_reward      | 96       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 26400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 7980     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26800, episode_reward=-55.25 +/- 64.62\n",
            "Episode length: 394.20 +/- 43.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 394      |\n",
            "|    mean_reward      | -55.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 26800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 8005     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27200, episode_reward=58.65 +/- 101.15\n",
            "Episode length: 446.00 +/- 105.38\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 446      |\n",
            "|    mean_reward      | 58.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 27200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 8030     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27600, episode_reward=81.32 +/- 104.69\n",
            "Episode length: 574.20 +/- 255.02\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 574      |\n",
            "|    mean_reward      | 81.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 27600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.86     |\n",
            "|    n_updates        | 8055     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=99.93 +/- 128.89\n",
            "Episode length: 575.40 +/- 125.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 575      |\n",
            "|    mean_reward      | 99.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 28000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 8080     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 311      |\n",
            "|    ep_rew_mean      | -55.8    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 75       |\n",
            "|    time_elapsed     | 374      |\n",
            "|    total_timesteps  | 28388    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.35     |\n",
            "|    n_updates        | 8105     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28400, episode_reward=88.30 +/- 53.39\n",
            "Episode length: 846.40 +/- 194.32\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 846      |\n",
            "|    mean_reward      | 88.3     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 28400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28800, episode_reward=15.14 +/- 139.19\n",
            "Episode length: 696.60 +/- 175.15\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 697      |\n",
            "|    mean_reward      | 15.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 28800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 8130     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29200, episode_reward=79.92 +/- 87.09\n",
            "Episode length: 740.20 +/- 139.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 740      |\n",
            "|    mean_reward      | 79.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 29200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.4      |\n",
            "|    n_updates        | 8155     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29600, episode_reward=-39.35 +/- 112.08\n",
            "Episode length: 972.20 +/- 55.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 972      |\n",
            "|    mean_reward      | -39.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 29600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.83     |\n",
            "|    n_updates        | 8180     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=6.54 +/- 118.63\n",
            "Episode length: 808.60 +/- 247.29\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 809      |\n",
            "|    mean_reward      | 6.54     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.852    |\n",
            "|    n_updates        | 8205     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30400, episode_reward=24.13 +/- 148.50\n",
            "Episode length: 732.20 +/- 287.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 732      |\n",
            "|    mean_reward      | 24.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.88     |\n",
            "|    n_updates        | 8230     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30800, episode_reward=-42.21 +/- 112.88\n",
            "Episode length: 858.80 +/- 191.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 859      |\n",
            "|    mean_reward      | -42.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 30800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.611    |\n",
            "|    n_updates        | 8255     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31200, episode_reward=-56.73 +/- 81.79\n",
            "Episode length: 696.40 +/- 257.97\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 696      |\n",
            "|    mean_reward      | -56.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 31200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 8280     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 332      |\n",
            "|    ep_rew_mean      | -53.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 71       |\n",
            "|    time_elapsed     | 442      |\n",
            "|    total_timesteps  | 31548    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.23     |\n",
            "|    n_updates        | 8302     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31600, episode_reward=-151.11 +/- 24.49\n",
            "Episode length: 895.20 +/- 128.66\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 895      |\n",
            "|    mean_reward      | -151     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 31600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.598    |\n",
            "|    n_updates        | 8305     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-97.38 +/- 80.21\n",
            "Episode length: 833.20 +/- 213.63\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 833      |\n",
            "|    mean_reward      | -97.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 32000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.985    |\n",
            "|    n_updates        | 8330     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32400, episode_reward=-97.42 +/- 127.08\n",
            "Episode length: 771.20 +/- 197.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 771      |\n",
            "|    mean_reward      | -97.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 32400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.737    |\n",
            "|    n_updates        | 8355     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32800, episode_reward=-48.14 +/- 114.19\n",
            "Episode length: 849.40 +/- 185.52\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 849      |\n",
            "|    mean_reward      | -48.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 32800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.49     |\n",
            "|    n_updates        | 8380     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=33200, episode_reward=-63.96 +/- 140.96\n",
            "Episode length: 917.40 +/- 143.31\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 917      |\n",
            "|    mean_reward      | -64      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 33200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 8405     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=33600, episode_reward=26.44 +/- 142.90\n",
            "Episode length: 633.40 +/- 269.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 633      |\n",
            "|    mean_reward      | 26.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 33600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.68     |\n",
            "|    n_updates        | 8430     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 340      |\n",
            "|    ep_rew_mean      | -51      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 67       |\n",
            "|    time_elapsed     | 500      |\n",
            "|    total_timesteps  | 33724    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 8438     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=-83.54 +/- 90.62\n",
            "Episode length: 771.20 +/- 213.34\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 771      |\n",
            "|    mean_reward      | -83.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 34000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 8455     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34400, episode_reward=-102.18 +/- 158.31\n",
            "Episode length: 753.20 +/- 179.90\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 753      |\n",
            "|    mean_reward      | -102     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 34400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.47     |\n",
            "|    n_updates        | 8480     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34800, episode_reward=-45.27 +/- 106.98\n",
            "Episode length: 852.00 +/- 186.01\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 852      |\n",
            "|    mean_reward      | -45.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 34800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.7      |\n",
            "|    n_updates        | 8505     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35200, episode_reward=-101.85 +/- 65.19\n",
            "Episode length: 960.80 +/- 78.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 961      |\n",
            "|    mean_reward      | -102     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 35200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.612    |\n",
            "|    n_updates        | 8530     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35600, episode_reward=-95.27 +/- 141.97\n",
            "Episode length: 786.80 +/- 160.51\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 787      |\n",
            "|    mean_reward      | -95.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 35600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.52     |\n",
            "|    n_updates        | 8555     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=-20.17 +/- 112.50\n",
            "Episode length: 787.20 +/- 283.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 787      |\n",
            "|    mean_reward      | -20.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 36000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 8580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36400, episode_reward=7.87 +/- 94.31\n",
            "Episode length: 890.60 +/- 168.10\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 891      |\n",
            "|    mean_reward      | 7.87     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 36400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.958    |\n",
            "|    n_updates        | 8605     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36800, episode_reward=18.60 +/- 104.00\n",
            "Episode length: 805.60 +/- 166.69\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 806      |\n",
            "|    mean_reward      | 18.6     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 36800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.607    |\n",
            "|    n_updates        | 8630     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 358      |\n",
            "|    ep_rew_mean      | -52.1    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 64       |\n",
            "|    time_elapsed     | 570      |\n",
            "|    total_timesteps  | 37004    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 8643     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37200, episode_reward=18.05 +/- 106.42\n",
            "Episode length: 576.00 +/- 218.53\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 576      |\n",
            "|    mean_reward      | 18.1     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 37200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.938    |\n",
            "|    n_updates        | 8655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37600, episode_reward=-33.74 +/- 92.70\n",
            "Episode length: 578.20 +/- 233.54\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 578      |\n",
            "|    mean_reward      | -33.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 37600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.23     |\n",
            "|    n_updates        | 8680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=125.38 +/- 46.18\n",
            "Episode length: 793.20 +/- 253.66\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 793      |\n",
            "|    mean_reward      | 125      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 38000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.678    |\n",
            "|    n_updates        | 8705     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38400, episode_reward=-99.13 +/- 104.82\n",
            "Episode length: 920.20 +/- 117.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 920      |\n",
            "|    mean_reward      | -99.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 38400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 8730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38800, episode_reward=-25.00 +/- 154.19\n",
            "Episode length: 752.80 +/- 259.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 753      |\n",
            "|    mean_reward      | -25      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 38800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.76     |\n",
            "|    n_updates        | 8755     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 376      |\n",
            "|    ep_rew_mean      | -50.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 63       |\n",
            "|    time_elapsed     | 612      |\n",
            "|    total_timesteps  | 39156    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.34     |\n",
            "|    n_updates        | 8778     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39200, episode_reward=49.75 +/- 131.71\n",
            "Episode length: 517.80 +/- 250.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 518      |\n",
            "|    mean_reward      | 49.8     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 39200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.31     |\n",
            "|    n_updates        | 8780     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39600, episode_reward=-109.26 +/- 80.65\n",
            "Episode length: 870.20 +/- 217.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 870      |\n",
            "|    mean_reward      | -109     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 39600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 8805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=24.91 +/- 163.29\n",
            "Episode length: 663.00 +/- 278.35\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 663      |\n",
            "|    mean_reward      | 24.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.994    |\n",
            "|    n_updates        | 8830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40400, episode_reward=-59.00 +/- 138.03\n",
            "Episode length: 820.00 +/- 223.08\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 820      |\n",
            "|    mean_reward      | -59      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.1      |\n",
            "|    n_updates        | 8855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40800, episode_reward=-84.62 +/- 136.80\n",
            "Episode length: 726.40 +/- 214.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 726      |\n",
            "|    mean_reward      | -84.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 40800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 8880     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=41200, episode_reward=-9.67 +/- 138.36\n",
            "Episode length: 692.80 +/- 173.64\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 693      |\n",
            "|    mean_reward      | -9.67    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 41200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 8905     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=41600, episode_reward=59.42 +/- 172.40\n",
            "Episode length: 765.40 +/- 224.84\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 765      |\n",
            "|    mean_reward      | 59.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 41600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.82     |\n",
            "|    n_updates        | 8930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=-93.84 +/- 128.77\n",
            "Episode length: 780.80 +/- 229.63\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 781      |\n",
            "|    mean_reward      | -93.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 42000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.41     |\n",
            "|    n_updates        | 8955     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | -43.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 62       |\n",
            "|    time_elapsed     | 680      |\n",
            "|    total_timesteps  | 42224    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 8969     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42400, episode_reward=-92.93 +/- 103.04\n",
            "Episode length: 982.00 +/- 36.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 982      |\n",
            "|    mean_reward      | -92.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 42400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.734    |\n",
            "|    n_updates        | 8980     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42800, episode_reward=39.37 +/- 141.70\n",
            "Episode length: 681.80 +/- 273.36\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 682      |\n",
            "|    mean_reward      | 39.4     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 42800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.841    |\n",
            "|    n_updates        | 9005     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43200, episode_reward=-50.38 +/- 118.86\n",
            "Episode length: 745.00 +/- 225.19\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 745      |\n",
            "|    mean_reward      | -50.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 43200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 9030     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43600, episode_reward=-26.41 +/- 152.52\n",
            "Episode length: 774.00 +/- 276.83\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 774      |\n",
            "|    mean_reward      | -26.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 43600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.03     |\n",
            "|    n_updates        | 9055     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=-114.36 +/- 13.23\n",
            "Episode length: 949.00 +/- 102.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 949      |\n",
            "|    mean_reward      | -114     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 44000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 9080     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 424      |\n",
            "|    ep_rew_mean      | -38.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 60       |\n",
            "|    time_elapsed     | 729      |\n",
            "|    total_timesteps  | 44388    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.883    |\n",
            "|    n_updates        | 9105     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44400, episode_reward=-130.67 +/- 50.03\n",
            "Episode length: 866.80 +/- 244.99\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 867      |\n",
            "|    mean_reward      | -131     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 44400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44800, episode_reward=-147.40 +/- 45.02\n",
            "Episode length: 972.60 +/- 33.68\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 973      |\n",
            "|    mean_reward      | -147     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 44800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.08     |\n",
            "|    n_updates        | 9130     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45200, episode_reward=-135.24 +/- 50.07\n",
            "Episode length: 954.80 +/- 81.70\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 955      |\n",
            "|    mean_reward      | -135     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 45200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 9155     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45600, episode_reward=-64.77 +/- 106.83\n",
            "Episode length: 673.00 +/- 279.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 673      |\n",
            "|    mean_reward      | -64.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 45600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.86     |\n",
            "|    n_updates        | 9180     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=-61.69 +/- 68.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -61.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 46000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.793    |\n",
            "|    n_updates        | 9205     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46400, episode_reward=-81.93 +/- 129.61\n",
            "Episode length: 706.40 +/- 242.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 706      |\n",
            "|    mean_reward      | -81.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 46400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.89     |\n",
            "|    n_updates        | 9230     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46800, episode_reward=-123.27 +/- 61.46\n",
            "Episode length: 849.20 +/- 252.83\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 849      |\n",
            "|    mean_reward      | -123     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 46800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 9255     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47200, episode_reward=-72.75 +/- 76.19\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -72.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 47200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.816    |\n",
            "|    n_updates        | 9280     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -40.3    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 58       |\n",
            "|    time_elapsed     | 813      |\n",
            "|    total_timesteps  | 47264    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 9284     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47600, episode_reward=-142.76 +/- 17.86\n",
            "Episode length: 905.40 +/- 153.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 905      |\n",
            "|    mean_reward      | -143     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 47600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.13     |\n",
            "|    n_updates        | 9305     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-178.04 +/- 33.98\n",
            "Episode length: 836.80 +/- 83.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 837      |\n",
            "|    mean_reward      | -178     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 48000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.478    |\n",
            "|    n_updates        | 9330     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48400, episode_reward=-114.14 +/- 114.01\n",
            "Episode length: 918.60 +/- 117.56\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 919      |\n",
            "|    mean_reward      | -114     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 48400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 9355     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48800, episode_reward=-105.34 +/- 14.99\n",
            "Episode length: 667.00 +/- 275.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 667      |\n",
            "|    mean_reward      | -105     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 48800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 9380     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=49200, episode_reward=-91.61 +/- 81.05\n",
            "Episode length: 890.40 +/- 219.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 890      |\n",
            "|    mean_reward      | -91.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 49200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 4.13     |\n",
            "|    n_updates        | 9405     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=49600, episode_reward=26.00 +/- 149.61\n",
            "Episode length: 795.20 +/- 173.26\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 795      |\n",
            "|    mean_reward      | 26       |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 49600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 9430     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-14.20 +/- 126.48\n",
            "Episode length: 877.80 +/- 244.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 878      |\n",
            "|    mean_reward      | -14.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 9455     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50400, episode_reward=-145.49 +/- 44.50\n",
            "Episode length: 799.00 +/- 168.68\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 799      |\n",
            "|    mean_reward      | -145     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.57     |\n",
            "|    n_updates        | 9480     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 473      |\n",
            "|    ep_rew_mean      | -37.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 56       |\n",
            "|    time_elapsed     | 898      |\n",
            "|    total_timesteps  | 50404    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.23     |\n",
            "|    n_updates        | 9481     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50800, episode_reward=-95.12 +/- 18.30\n",
            "Episode length: 894.40 +/- 211.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 894      |\n",
            "|    mean_reward      | -95.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 50800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 9505     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51200, episode_reward=-139.14 +/- 38.49\n",
            "Episode length: 877.80 +/- 168.24\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 878      |\n",
            "|    mean_reward      | -139     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 51200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 9530     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51600, episode_reward=-100.15 +/- 27.59\n",
            "Episode length: 902.60 +/- 194.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 903      |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 51600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 9555     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=-45.74 +/- 91.95\n",
            "Episode length: 929.60 +/- 140.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 930      |\n",
            "|    mean_reward      | -45.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 9580     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52400, episode_reward=-100.37 +/- 39.47\n",
            "Episode length: 766.80 +/- 207.71\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 767      |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.953    |\n",
            "|    n_updates        | 9605     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52800, episode_reward=-61.97 +/- 73.43\n",
            "Episode length: 882.60 +/- 234.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 883      |\n",
            "|    mean_reward      | -62      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 52800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 9630     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 502      |\n",
            "|    ep_rew_mean      | -34.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 54       |\n",
            "|    time_elapsed     | 965      |\n",
            "|    total_timesteps  | 52988    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.66     |\n",
            "|    n_updates        | 9642     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53200, episode_reward=-114.83 +/- 46.70\n",
            "Episode length: 969.20 +/- 61.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 969      |\n",
            "|    mean_reward      | -115     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 53200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 9655     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53600, episode_reward=-124.75 +/- 50.71\n",
            "Episode length: 790.60 +/- 229.23\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 791      |\n",
            "|    mean_reward      | -125     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 53600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 9680     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=-108.26 +/- 19.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -108     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 54000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 9705     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54400, episode_reward=-100.27 +/- 27.61\n",
            "Episode length: 958.40 +/- 83.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 958      |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 54400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.761    |\n",
            "|    n_updates        | 9730     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54800, episode_reward=-80.96 +/- 61.49\n",
            "Episode length: 867.80 +/- 264.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 868      |\n",
            "|    mean_reward      | -81      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 54800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.718    |\n",
            "|    n_updates        | 9755     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55200, episode_reward=-99.50 +/- 145.88\n",
            "Episode length: 657.80 +/- 210.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 658      |\n",
            "|    mean_reward      | -99.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 55200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 9780     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55600, episode_reward=-61.07 +/- 109.11\n",
            "Episode length: 796.80 +/- 252.24\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 797      |\n",
            "|    mean_reward      | -61.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 55600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.3      |\n",
            "|    n_updates        | 9805     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-110.90 +/- 18.10\n",
            "Episode length: 868.80 +/- 166.84\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 869      |\n",
            "|    mean_reward      | -111     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 56000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 9830     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56400, episode_reward=-108.03 +/- 55.87\n",
            "Episode length: 743.00 +/- 228.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 743      |\n",
            "|    mean_reward      | -108     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 56400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.59     |\n",
            "|    n_updates        | 9855     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56800, episode_reward=-106.30 +/- 10.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -106     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 56800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.6      |\n",
            "|    n_updates        | 9880     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 528      |\n",
            "|    ep_rew_mean      | -31.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 53       |\n",
            "|    time_elapsed     | 1067     |\n",
            "|    total_timesteps  | 56848    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 9883     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=57200, episode_reward=-123.98 +/- 45.85\n",
            "Episode length: 997.80 +/- 4.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 998      |\n",
            "|    mean_reward      | -124     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 57200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 9905     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=57600, episode_reward=-81.96 +/- 35.23\n",
            "Episode length: 931.20 +/- 137.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 931      |\n",
            "|    mean_reward      | -82      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 57600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.955    |\n",
            "|    n_updates        | 9930     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=-108.56 +/- 28.41\n",
            "Episode length: 916.40 +/- 167.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 916      |\n",
            "|    mean_reward      | -109     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 58000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 9955     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58400, episode_reward=-81.32 +/- 112.86\n",
            "Episode length: 579.40 +/- 217.98\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 579      |\n",
            "|    mean_reward      | -81.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 58400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.775    |\n",
            "|    n_updates        | 9980     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58800, episode_reward=16.23 +/- 78.33\n",
            "Episode length: 981.20 +/- 37.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 981      |\n",
            "|    mean_reward      | 16.2     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 58800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.822    |\n",
            "|    n_updates        | 10005    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59200, episode_reward=-79.54 +/- 151.21\n",
            "Episode length: 683.00 +/- 254.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 683      |\n",
            "|    mean_reward      | -79.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 59200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.933    |\n",
            "|    n_updates        | 10030    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 548      |\n",
            "|    ep_rew_mean      | -28      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 52       |\n",
            "|    time_elapsed     | 1124     |\n",
            "|    total_timesteps  | 59264    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 10034    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59600, episode_reward=-94.97 +/- 25.98\n",
            "Episode length: 770.80 +/- 286.30\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 771      |\n",
            "|    mean_reward      | -95      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 59600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.959    |\n",
            "|    n_updates        | 10055    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-97.98 +/- 23.85\n",
            "Episode length: 885.20 +/- 229.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 885      |\n",
            "|    mean_reward      | -98      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.99     |\n",
            "|    n_updates        | 10080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60400, episode_reward=-93.10 +/- 20.75\n",
            "Episode length: 814.00 +/- 228.03\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 814      |\n",
            "|    mean_reward      | -93.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.732    |\n",
            "|    n_updates        | 10105    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60800, episode_reward=-72.59 +/- 131.79\n",
            "Episode length: 866.00 +/- 128.32\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 866      |\n",
            "|    mean_reward      | -72.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 60800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.844    |\n",
            "|    n_updates        | 10130    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61200, episode_reward=-79.62 +/- 20.56\n",
            "Episode length: 893.00 +/- 214.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 893      |\n",
            "|    mean_reward      | -79.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 61200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 10155    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61600, episode_reward=-94.58 +/- 19.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -94.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 61600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 10180    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=-125.99 +/- 43.21\n",
            "Episode length: 987.00 +/- 26.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 987      |\n",
            "|    mean_reward      | -126     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 62000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2        |\n",
            "|    n_updates        | 10205    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62400, episode_reward=-102.83 +/- 24.74\n",
            "Episode length: 938.60 +/- 122.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 939      |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 62400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 10230    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 579      |\n",
            "|    ep_rew_mean      | -27.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 51       |\n",
            "|    time_elapsed     | 1211     |\n",
            "|    total_timesteps  | 62780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 10254    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62800, episode_reward=-84.67 +/- 18.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 62800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.804    |\n",
            "|    n_updates        | 10255    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63200, episode_reward=-120.34 +/- 60.69\n",
            "Episode length: 962.20 +/- 73.62\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 962      |\n",
            "|    mean_reward      | -120     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 63200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 10280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63600, episode_reward=-80.94 +/- 11.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 63600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 10305    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-112.39 +/- 35.28\n",
            "Episode length: 971.60 +/- 56.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 972      |\n",
            "|    mean_reward      | -112     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 64000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.731    |\n",
            "|    n_updates        | 10330    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64400, episode_reward=-121.72 +/- 51.06\n",
            "Episode length: 946.20 +/- 100.27\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 946      |\n",
            "|    mean_reward      | -122     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 64400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 10355    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64800, episode_reward=-112.87 +/- 38.14\n",
            "Episode length: 922.60 +/- 95.07\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 923      |\n",
            "|    mean_reward      | -113     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 64800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 5.11     |\n",
            "|    n_updates        | 10380    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=65200, episode_reward=-104.56 +/- 29.74\n",
            "Episode length: 944.20 +/- 111.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 944      |\n",
            "|    mean_reward      | -105     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 65200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.661    |\n",
            "|    n_updates        | 10405    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=65600, episode_reward=-112.89 +/- 28.53\n",
            "Episode length: 974.20 +/- 51.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 974      |\n",
            "|    mean_reward      | -113     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 65600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 10430    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=-97.75 +/- 26.93\n",
            "Episode length: 910.20 +/- 179.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 910      |\n",
            "|    mean_reward      | -97.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 66000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.72     |\n",
            "|    n_updates        | 10455    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66400, episode_reward=-108.26 +/- 15.03\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -108     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 66400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.87     |\n",
            "|    n_updates        | 10480    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 614      |\n",
            "|    ep_rew_mean      | -29.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 50       |\n",
            "|    time_elapsed     | 1335     |\n",
            "|    total_timesteps  | 66780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.585    |\n",
            "|    n_updates        | 10504    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66800, episode_reward=-102.92 +/- 13.75\n",
            "Episode length: 892.60 +/- 214.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 893      |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 66800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.665    |\n",
            "|    n_updates        | 10505    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67200, episode_reward=-128.13 +/- 40.91\n",
            "Episode length: 975.20 +/- 49.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 975      |\n",
            "|    mean_reward      | -128     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 67200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 10530    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67600, episode_reward=-20.25 +/- 115.47\n",
            "Episode length: 797.60 +/- 249.05\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 798      |\n",
            "|    mean_reward      | -20.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 67600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.58     |\n",
            "|    n_updates        | 10555    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=-122.40 +/- 36.75\n",
            "Episode length: 816.80 +/- 200.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 817      |\n",
            "|    mean_reward      | -122     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 68000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.14     |\n",
            "|    n_updates        | 10580    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68400, episode_reward=-101.22 +/- 30.17\n",
            "Episode length: 956.00 +/- 88.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 956      |\n",
            "|    mean_reward      | -101     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 68400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.901    |\n",
            "|    n_updates        | 10605    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68800, episode_reward=-79.99 +/- 21.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 68800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.29     |\n",
            "|    n_updates        | 10630    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69200, episode_reward=-96.42 +/- 44.61\n",
            "Episode length: 986.40 +/- 27.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 986      |\n",
            "|    mean_reward      | -96.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 69200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.09     |\n",
            "|    n_updates        | 10655    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69600, episode_reward=-118.70 +/- 38.18\n",
            "Episode length: 837.60 +/- 151.75\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 838      |\n",
            "|    mean_reward      | -119     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 69600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.69     |\n",
            "|    n_updates        | 10680    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-117.25 +/- 59.87\n",
            "Episode length: 939.20 +/- 103.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 939      |\n",
            "|    mean_reward      | -117     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 10705    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70400, episode_reward=-90.69 +/- 27.82\n",
            "Episode length: 838.60 +/- 214.95\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 839      |\n",
            "|    mean_reward      | -90.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.74     |\n",
            "|    n_updates        | 10730    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 647      |\n",
            "|    ep_rew_mean      | -32.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 48       |\n",
            "|    time_elapsed     | 1445     |\n",
            "|    total_timesteps  | 70780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.787    |\n",
            "|    n_updates        | 10754    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70800, episode_reward=-115.60 +/- 43.15\n",
            "Episode length: 996.00 +/- 8.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 996      |\n",
            "|    mean_reward      | -116     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 70800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 10755    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71200, episode_reward=-93.75 +/- 28.15\n",
            "Episode length: 944.40 +/- 111.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 944      |\n",
            "|    mean_reward      | -93.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 71200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.605    |\n",
            "|    n_updates        | 10780    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71600, episode_reward=-111.45 +/- 40.63\n",
            "Episode length: 988.40 +/- 23.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 988      |\n",
            "|    mean_reward      | -111     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 71600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 10805    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-125.59 +/- 54.14\n",
            "Episode length: 960.80 +/- 78.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 961      |\n",
            "|    mean_reward      | -126     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 72000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.89     |\n",
            "|    n_updates        | 10830    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72400, episode_reward=-104.88 +/- 37.01\n",
            "Episode length: 802.40 +/- 253.74\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 802      |\n",
            "|    mean_reward      | -105     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 72400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 10855    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72800, episode_reward=-110.96 +/- 36.71\n",
            "Episode length: 994.60 +/- 10.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 995      |\n",
            "|    mean_reward      | -111     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 72800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.746    |\n",
            "|    n_updates        | 10880    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73200, episode_reward=-94.07 +/- 34.96\n",
            "Episode length: 987.80 +/- 24.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 988      |\n",
            "|    mean_reward      | -94.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 73200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.891    |\n",
            "|    n_updates        | 10905    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73600, episode_reward=-111.52 +/- 17.02\n",
            "Episode length: 935.40 +/- 129.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 935      |\n",
            "|    mean_reward      | -112     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 73600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.927    |\n",
            "|    n_updates        | 10930    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 667      |\n",
            "|    ep_rew_mean      | -32.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 47       |\n",
            "|    time_elapsed     | 1535     |\n",
            "|    total_timesteps  | 73668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.66     |\n",
            "|    n_updates        | 10935    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=-88.52 +/- 20.08\n",
            "Episode length: 913.00 +/- 174.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 913      |\n",
            "|    mean_reward      | -88.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 74000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 10955    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74400, episode_reward=-106.14 +/- 45.52\n",
            "Episode length: 998.40 +/- 3.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 998      |\n",
            "|    mean_reward      | -106     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 74400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.65     |\n",
            "|    n_updates        | 10980    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74800, episode_reward=-98.94 +/- 34.69\n",
            "Episode length: 933.80 +/- 132.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 934      |\n",
            "|    mean_reward      | -98.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 74800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 11005    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75200, episode_reward=-71.92 +/- 82.99\n",
            "Episode length: 913.00 +/- 174.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 913      |\n",
            "|    mean_reward      | -71.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 75200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.04     |\n",
            "|    n_updates        | 11030    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75600, episode_reward=-126.52 +/- 62.79\n",
            "Episode length: 899.00 +/- 175.91\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 899      |\n",
            "|    mean_reward      | -127     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 75600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.42     |\n",
            "|    n_updates        | 11055    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=-86.81 +/- 32.89\n",
            "Episode length: 780.00 +/- 270.75\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 780      |\n",
            "|    mean_reward      | -86.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 76000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.665    |\n",
            "|    n_updates        | 11080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76400, episode_reward=-40.12 +/- 113.97\n",
            "Episode length: 889.20 +/- 172.35\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 889      |\n",
            "|    mean_reward      | -40.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 76400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.604    |\n",
            "|    n_updates        | 11105    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76800, episode_reward=-63.15 +/- 13.37\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -63.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 76800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.5      |\n",
            "|    n_updates        | 11130    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77200, episode_reward=-64.42 +/- 22.68\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -64.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 77200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.87     |\n",
            "|    n_updates        | 11155    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77600, episode_reward=-88.02 +/- 19.46\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -88      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 77600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 11180    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 696      |\n",
            "|    ep_rew_mean      | -28.6    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 47       |\n",
            "|    time_elapsed     | 1644     |\n",
            "|    total_timesteps  | 77668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.483    |\n",
            "|    n_updates        | 11185    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=-111.69 +/- 42.49\n",
            "Episode length: 997.60 +/- 4.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 998      |\n",
            "|    mean_reward      | -112     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 78000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.913    |\n",
            "|    n_updates        | 11205    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78400, episode_reward=-98.75 +/- 43.57\n",
            "Episode length: 981.60 +/- 36.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 982      |\n",
            "|    mean_reward      | -98.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 78400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 11230    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78800, episode_reward=-77.87 +/- 18.66\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -77.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 78800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.858    |\n",
            "|    n_updates        | 11255    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79200, episode_reward=-97.07 +/- 44.14\n",
            "Episode length: 981.20 +/- 37.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 981      |\n",
            "|    mean_reward      | -97.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 79200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 11280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79600, episode_reward=-93.34 +/- 16.45\n",
            "Episode length: 894.60 +/- 210.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 895      |\n",
            "|    mean_reward      | -93.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 79600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.924    |\n",
            "|    n_updates        | 11305    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-89.97 +/- 57.48\n",
            "Episode length: 943.60 +/- 79.12\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 944      |\n",
            "|    mean_reward      | -90      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.61     |\n",
            "|    n_updates        | 11330    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80400, episode_reward=-67.62 +/- 29.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -67.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.39     |\n",
            "|    n_updates        | 11355    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80800, episode_reward=-83.56 +/- 26.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -83.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 80800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 11380    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=81200, episode_reward=-92.41 +/- 8.34\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -92.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 81200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.84     |\n",
            "|    n_updates        | 11405    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=81600, episode_reward=-95.47 +/- 18.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -95.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 81600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.41     |\n",
            "|    n_updates        | 11430    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 711      |\n",
            "|    ep_rew_mean      | -29.2    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 45       |\n",
            "|    time_elapsed     | 1777     |\n",
            "|    total_timesteps  | 81668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.743    |\n",
            "|    n_updates        | 11435    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=-80.73 +/- 13.04\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.816    |\n",
            "|    n_updates        | 11455    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82400, episode_reward=-76.35 +/- 26.36\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -76.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.764    |\n",
            "|    n_updates        | 11480    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82800, episode_reward=-87.53 +/- 12.35\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -87.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 82800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.43     |\n",
            "|    n_updates        | 11505    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83200, episode_reward=-125.85 +/- 51.71\n",
            "Episode length: 989.40 +/- 17.48\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 989      |\n",
            "|    mean_reward      | -126     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 83200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.737    |\n",
            "|    n_updates        | 11530    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83600, episode_reward=-73.53 +/- 16.33\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -73.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 83600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 11555    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=-102.12 +/- 39.63\n",
            "Episode length: 997.00 +/- 6.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 997      |\n",
            "|    mean_reward      | -102     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 84000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 11580    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84400, episode_reward=-87.55 +/- 26.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -87.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 84400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 11605    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84800, episode_reward=-105.99 +/- 55.73\n",
            "Episode length: 973.40 +/- 53.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 973      |\n",
            "|    mean_reward      | -106     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 84800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 11630    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=85200, episode_reward=-117.05 +/- 45.00\n",
            "Episode length: 899.80 +/- 127.47\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 900      |\n",
            "|    mean_reward      | -117     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 85200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 11655    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=85600, episode_reward=-96.59 +/- 18.15\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -96.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 85600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.674    |\n",
            "|    n_updates        | 11680    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 726      |\n",
            "|    ep_rew_mean      | -30.4    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 44       |\n",
            "|    time_elapsed     | 1917     |\n",
            "|    total_timesteps  | 85668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.86     |\n",
            "|    n_updates        | 11685    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=-79.25 +/- 18.75\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -79.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 86000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 11705    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86400, episode_reward=-113.96 +/- 24.64\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -114     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 86400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1        |\n",
            "|    n_updates        | 11730    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86800, episode_reward=-99.84 +/- 25.80\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -99.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 86800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.735    |\n",
            "|    n_updates        | 11755    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87200, episode_reward=-102.77 +/- 30.72\n",
            "Episode length: 952.40 +/- 95.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 952      |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 87200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.802    |\n",
            "|    n_updates        | 11780    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87600, episode_reward=-84.30 +/- 29.85\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 87600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.476    |\n",
            "|    n_updates        | 11805    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-84.03 +/- 18.93\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84      |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 88000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.484    |\n",
            "|    n_updates        | 11830    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88400, episode_reward=-67.85 +/- 34.11\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -67.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 88400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.787    |\n",
            "|    n_updates        | 11855    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88800, episode_reward=-109.65 +/- 60.49\n",
            "Episode length: 992.00 +/- 16.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 992      |\n",
            "|    mean_reward      | -110     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 88800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.904    |\n",
            "|    n_updates        | 11880    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89200, episode_reward=-129.51 +/- 40.17\n",
            "Episode length: 855.60 +/- 141.82\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 856      |\n",
            "|    mean_reward      | -130     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 89200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.946    |\n",
            "|    n_updates        | 11905    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89600, episode_reward=-84.33 +/- 66.33\n",
            "Episode length: 977.40 +/- 45.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 977      |\n",
            "|    mean_reward      | -84.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 89600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.12     |\n",
            "|    n_updates        | 11930    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 753      |\n",
            "|    ep_rew_mean      | -35      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 43       |\n",
            "|    time_elapsed     | 2044     |\n",
            "|    total_timesteps  | 89668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 11935    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-62.11 +/- 28.83\n",
            "Episode length: 960.40 +/- 79.20\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 960      |\n",
            "|    mean_reward      | -62.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.45     |\n",
            "|    n_updates        | 11955    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90400, episode_reward=-53.36 +/- 27.87\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -53.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 11980    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90800, episode_reward=-104.25 +/- 49.19\n",
            "Episode length: 996.60 +/- 6.80\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 997      |\n",
            "|    mean_reward      | -104     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 90800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.1      |\n",
            "|    n_updates        | 12005    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91200, episode_reward=-81.76 +/- 37.84\n",
            "Episode length: 985.80 +/- 28.40\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 986      |\n",
            "|    mean_reward      | -81.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 91200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.932    |\n",
            "|    n_updates        | 12030    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91600, episode_reward=-64.58 +/- 14.39\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -64.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 91600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 12055    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=-82.17 +/- 15.01\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -82.2    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 92000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 7.84     |\n",
            "|    n_updates        | 12080    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92400, episode_reward=-84.89 +/- 20.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 92400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.946    |\n",
            "|    n_updates        | 12105    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92800, episode_reward=-85.78 +/- 22.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -85.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 92800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.82     |\n",
            "|    n_updates        | 12130    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93200, episode_reward=-83.90 +/- 18.45\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -83.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 93200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 12155    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93600, episode_reward=-89.70 +/- 8.50\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -89.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 93600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.798    |\n",
            "|    n_updates        | 12180    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 777      |\n",
            "|    ep_rew_mean      | -37.5    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 42       |\n",
            "|    time_elapsed     | 2182     |\n",
            "|    total_timesteps  | 93668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.02     |\n",
            "|    n_updates        | 12185    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=-102.62 +/- 26.39\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -103     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 94000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.854    |\n",
            "|    n_updates        | 12205    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94400, episode_reward=-99.63 +/- 40.58\n",
            "Episode length: 921.20 +/- 157.60\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 921      |\n",
            "|    mean_reward      | -99.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 94400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.881    |\n",
            "|    n_updates        | 12230    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94800, episode_reward=-95.29 +/- 28.83\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -95.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 94800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.11     |\n",
            "|    n_updates        | 12255    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95200, episode_reward=-65.08 +/- 10.54\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -65.1    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 95200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 12280    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95600, episode_reward=-74.74 +/- 13.18\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -74.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 95600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.956    |\n",
            "|    n_updates        | 12305    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-81.67 +/- 23.96\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -81.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 96000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.14     |\n",
            "|    n_updates        | 12330    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96400, episode_reward=-79.47 +/- 20.84\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -79.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 96400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 3.83     |\n",
            "|    n_updates        | 12355    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96800, episode_reward=-79.49 +/- 19.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -79.5    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 96800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 12380    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97200, episode_reward=-81.94 +/- 38.29\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -81.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 97200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.56     |\n",
            "|    n_updates        | 12405    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97600, episode_reward=-84.68 +/- 17.56\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -84.7    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 97600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.91     |\n",
            "|    n_updates        | 12430    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 800      |\n",
            "|    ep_rew_mean      | -43      |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 42       |\n",
            "|    time_elapsed     | 2306     |\n",
            "|    total_timesteps  | 97668    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 12435    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=-92.94 +/- 19.37\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -92.9    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 98000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.639    |\n",
            "|    n_updates        | 12455    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98400, episode_reward=-78.61 +/- 7.70\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -78.6    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 98400    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.518    |\n",
            "|    n_updates        | 12480    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98800, episode_reward=-100.43 +/- 23.77\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -100     |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 98800    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 12505    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=99200, episode_reward=-94.82 +/- 35.91\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -94.8    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 99200    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 2.02     |\n",
            "|    n_updates        | 12530    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=99600, episode_reward=-80.45 +/- 10.90\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -80.4    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 99600    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 12555    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-75.26 +/- 14.72\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -75.3    |\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    total_timesteps  | 100000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.86     |\n",
            "|    n_updates        | 12580    |\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the PPO agent\n",
        "engine.train_agent(agent=ppo_agent,\n",
        "                   total_timesteps=total_timesteps,\n",
        "                   callback=callback,\n",
        "                   tb_log_name=tb_log_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8sIUbD142SZ",
        "outputId": "9d28b3ec-938d-4b2b-a6fb-263cde5b0ff8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to logs_tensorboard/runs_4\n",
            "Eval num_timesteps=400, episode_reward=-248.22 +/- 166.99\n",
            "Episode length: 64.20 +/- 11.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 64.2     |\n",
            "|    mean_reward     | -248     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 400      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=800, episode_reward=-190.70 +/- 130.87\n",
            "Episode length: 62.80 +/- 10.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 62.8     |\n",
            "|    mean_reward     | -191     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 800      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1200, episode_reward=-391.21 +/- 223.19\n",
            "Episode length: 66.40 +/- 9.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 66.4     |\n",
            "|    mean_reward     | -391     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1600, episode_reward=-270.09 +/- 177.91\n",
            "Episode length: 65.00 +/- 5.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 65       |\n",
            "|    mean_reward     | -270     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=-201.21 +/- 226.06\n",
            "Episode length: 73.00 +/- 10.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 73       |\n",
            "|    mean_reward     | -201     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2400, episode_reward=-369.06 +/- 200.13\n",
            "Episode length: 72.40 +/- 10.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 72.4     |\n",
            "|    mean_reward     | -369     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2400     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2800, episode_reward=-241.64 +/- 169.77\n",
            "Episode length: 58.60 +/- 2.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 58.6     |\n",
            "|    mean_reward     | -242     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3200, episode_reward=-193.74 +/- 105.23\n",
            "Episode length: 75.80 +/- 10.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 75.8     |\n",
            "|    mean_reward     | -194     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3600, episode_reward=-206.12 +/- 152.63\n",
            "Episode length: 67.00 +/- 7.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 67       |\n",
            "|    mean_reward     | -206     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-425.00 +/- 246.31\n",
            "Episode length: 81.20 +/- 22.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.2     |\n",
            "|    mean_reward     | -425     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4400, episode_reward=-282.52 +/- 202.72\n",
            "Episode length: 75.20 +/- 6.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 75.2     |\n",
            "|    mean_reward     | -283     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4400     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4800, episode_reward=-175.97 +/- 104.63\n",
            "Episode length: 61.00 +/- 7.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61       |\n",
            "|    mean_reward     | -176     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5200, episode_reward=-136.25 +/- 7.30\n",
            "Episode length: 61.20 +/- 4.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61.2     |\n",
            "|    mean_reward     | -136     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5600, episode_reward=-364.50 +/- 133.71\n",
            "Episode length: 60.00 +/- 9.25\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 60       |\n",
            "|    mean_reward     | -364     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-349.05 +/- 210.06\n",
            "Episode length: 66.40 +/- 10.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 66.4     |\n",
            "|    mean_reward     | -349     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6400, episode_reward=-203.05 +/- 147.81\n",
            "Episode length: 69.40 +/- 8.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 69.4     |\n",
            "|    mean_reward     | -203     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6400     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6800, episode_reward=-215.38 +/- 155.30\n",
            "Episode length: 65.80 +/- 4.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 65.8     |\n",
            "|    mean_reward     | -215     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7200, episode_reward=-236.75 +/- 131.93\n",
            "Episode length: 69.20 +/- 13.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 69.2     |\n",
            "|    mean_reward     | -237     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7600, episode_reward=-225.66 +/- 219.74\n",
            "Episode length: 64.40 +/- 12.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 64.4     |\n",
            "|    mean_reward     | -226     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=-285.86 +/- 178.81\n",
            "Episode length: 65.40 +/- 2.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 65.4     |\n",
            "|    mean_reward     | -286     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 89.8     |\n",
            "|    ep_rew_mean     | -187     |\n",
            "| time/              |          |\n",
            "|    fps             | 904      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8400, episode_reward=-278.29 +/- 238.66\n",
            "Episode length: 774.00 +/- 282.58\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 774         |\n",
            "|    mean_reward          | -278        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 8400        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008890146 |\n",
            "|    clip_fraction        | 0.0403      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.00494     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 349         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00801    |\n",
            "|    value_loss           | 1.57e+03    |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=8800, episode_reward=-258.94 +/- 227.30\n",
            "Episode length: 464.80 +/- 437.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 465      |\n",
            "|    mean_reward     | -259     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8800     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9200, episode_reward=-407.93 +/- 280.05\n",
            "Episode length: 522.40 +/- 394.26\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 522      |\n",
            "|    mean_reward     | -408     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9200     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9600, episode_reward=-180.38 +/- 219.99\n",
            "Episode length: 868.60 +/- 262.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 869      |\n",
            "|    mean_reward     | -180     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9600     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-540.81 +/- 245.06\n",
            "Episode length: 364.60 +/- 354.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 365      |\n",
            "|    mean_reward     | -541     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10400, episode_reward=-84.02 +/- 12.45\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -84      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10800, episode_reward=-406.29 +/- 248.13\n",
            "Episode length: 518.20 +/- 396.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 518      |\n",
            "|    mean_reward     | -406     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11200, episode_reward=-142.77 +/- 136.03\n",
            "Episode length: 845.40 +/- 309.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 845      |\n",
            "|    mean_reward     | -143     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11600, episode_reward=-168.24 +/- 158.90\n",
            "Episode length: 822.80 +/- 354.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 823      |\n",
            "|    mean_reward     | -168     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=-266.23 +/- 236.88\n",
            "Episode length: 694.40 +/- 374.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 694      |\n",
            "|    mean_reward     | -266     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12400, episode_reward=-386.61 +/- 243.62\n",
            "Episode length: 507.60 +/- 404.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 508      |\n",
            "|    mean_reward     | -387     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12800, episode_reward=-79.07 +/- 5.63\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -79.1    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13200, episode_reward=-307.48 +/- 283.33\n",
            "Episode length: 659.20 +/- 418.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 659      |\n",
            "|    mean_reward     | -307     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13600, episode_reward=-221.97 +/- 187.11\n",
            "Episode length: 664.40 +/- 412.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 664      |\n",
            "|    mean_reward     | -222     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=-218.95 +/- 196.62\n",
            "Episode length: 649.40 +/- 429.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 649      |\n",
            "|    mean_reward     | -219     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14400, episode_reward=-176.85 +/- 194.16\n",
            "Episode length: 853.20 +/- 293.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 853      |\n",
            "|    mean_reward     | -177     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14800, episode_reward=-242.51 +/- 192.93\n",
            "Episode length: 639.20 +/- 441.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 639      |\n",
            "|    mean_reward     | -243     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15200, episode_reward=-247.01 +/- 223.31\n",
            "Episode length: 702.60 +/- 373.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 703      |\n",
            "|    mean_reward     | -247     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15600, episode_reward=-290.97 +/- 269.12\n",
            "Episode length: 693.80 +/- 384.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 694      |\n",
            "|    mean_reward     | -291     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-205.46 +/- 139.21\n",
            "Episode length: 685.80 +/- 384.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 686      |\n",
            "|    mean_reward     | -205     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 95.5     |\n",
            "|    ep_rew_mean     | -161     |\n",
            "| time/              |          |\n",
            "|    fps             | 76       |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 214      |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16400, episode_reward=-465.11 +/- 45.40\n",
            "Episode length: 209.00 +/- 19.94\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 209         |\n",
            "|    mean_reward          | -465        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16400       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008114187 |\n",
            "|    clip_fraction        | 0.0247      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | -0.00756    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 336         |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00647    |\n",
            "|    value_loss           | 983         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=16800, episode_reward=-507.79 +/- 45.27\n",
            "Episode length: 238.40 +/- 35.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 238      |\n",
            "|    mean_reward     | -508     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17200, episode_reward=-552.34 +/- 155.31\n",
            "Episode length: 257.20 +/- 89.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 257      |\n",
            "|    mean_reward     | -552     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17600, episode_reward=-639.35 +/- 179.46\n",
            "Episode length: 288.60 +/- 97.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 289      |\n",
            "|    mean_reward     | -639     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=-533.22 +/- 25.29\n",
            "Episode length: 237.80 +/- 27.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 238      |\n",
            "|    mean_reward     | -533     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18400, episode_reward=-471.51 +/- 60.80\n",
            "Episode length: 254.00 +/- 34.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 254      |\n",
            "|    mean_reward     | -472     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18800, episode_reward=-545.10 +/- 163.81\n",
            "Episode length: 258.60 +/- 82.58\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 259      |\n",
            "|    mean_reward     | -545     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19200, episode_reward=-461.68 +/- 80.13\n",
            "Episode length: 236.60 +/- 31.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | -462     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19600, episode_reward=-477.35 +/- 40.22\n",
            "Episode length: 244.40 +/- 25.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 244      |\n",
            "|    mean_reward     | -477     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-498.35 +/- 128.72\n",
            "Episode length: 287.40 +/- 93.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | -498     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20400, episode_reward=-449.95 +/- 74.20\n",
            "Episode length: 226.00 +/- 34.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | -450     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20800, episode_reward=-564.65 +/- 146.52\n",
            "Episode length: 259.80 +/- 69.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 260      |\n",
            "|    mean_reward     | -565     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=21200, episode_reward=-503.14 +/- 52.67\n",
            "Episode length: 237.20 +/- 49.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | -503     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 21200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=21600, episode_reward=-534.37 +/- 104.95\n",
            "Episode length: 232.20 +/- 43.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 232      |\n",
            "|    mean_reward     | -534     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 21600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=-509.19 +/- 112.57\n",
            "Episode length: 253.00 +/- 69.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 253      |\n",
            "|    mean_reward     | -509     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22400, episode_reward=-561.83 +/- 207.52\n",
            "Episode length: 287.00 +/- 82.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | -562     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=22800, episode_reward=-491.36 +/- 82.47\n",
            "Episode length: 253.80 +/- 50.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 254      |\n",
            "|    mean_reward     | -491     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=23200, episode_reward=-437.15 +/- 33.65\n",
            "Episode length: 232.20 +/- 17.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 232      |\n",
            "|    mean_reward     | -437     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 23200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=23600, episode_reward=-547.60 +/- 237.19\n",
            "Episode length: 257.60 +/- 93.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | -548     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 23600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-468.81 +/- 62.03\n",
            "Episode length: 219.00 +/- 24.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 219      |\n",
            "|    mean_reward     | -469     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24400, episode_reward=-525.03 +/- 97.23\n",
            "Episode length: 300.00 +/- 84.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 300      |\n",
            "|    mean_reward     | -525     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 103      |\n",
            "|    ep_rew_mean     | -147     |\n",
            "| time/              |          |\n",
            "|    fps             | 98       |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 250      |\n",
            "|    total_timesteps | 24576    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=24800, episode_reward=-3264.85 +/- 122.95\n",
            "Episode length: 910.00 +/- 14.82\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 910         |\n",
            "|    mean_reward          | -3.26e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 24800       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004749673 |\n",
            "|    clip_fraction        | 0.0215      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | -0.0206     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 418         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00537    |\n",
            "|    value_loss           | 641         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=25200, episode_reward=-3214.85 +/- 160.40\n",
            "Episode length: 945.00 +/- 41.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 945       |\n",
            "|    mean_reward     | -3.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=25600, episode_reward=-3325.17 +/- 231.88\n",
            "Episode length: 930.60 +/- 32.51\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 931       |\n",
            "|    mean_reward     | -3.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=-3206.18 +/- 143.50\n",
            "Episode length: 923.00 +/- 27.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 923       |\n",
            "|    mean_reward     | -3.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26400, episode_reward=-3313.68 +/- 47.98\n",
            "Episode length: 911.00 +/- 7.54\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 911       |\n",
            "|    mean_reward     | -3.31e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=26800, episode_reward=-2819.46 +/- 593.47\n",
            "Episode length: 974.20 +/- 31.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 974       |\n",
            "|    mean_reward     | -2.82e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 26800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27200, episode_reward=-3175.62 +/- 127.67\n",
            "Episode length: 912.40 +/- 17.44\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 912       |\n",
            "|    mean_reward     | -3.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=27600, episode_reward=-3139.57 +/- 236.78\n",
            "Episode length: 906.40 +/- 12.50\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 906       |\n",
            "|    mean_reward     | -3.14e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=-2692.86 +/- 1182.80\n",
            "Episode length: 823.20 +/- 187.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 823       |\n",
            "|    mean_reward     | -2.69e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28400, episode_reward=-2943.89 +/- 580.38\n",
            "Episode length: 937.80 +/- 35.99\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 938       |\n",
            "|    mean_reward     | -2.94e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=28800, episode_reward=-3313.60 +/- 146.52\n",
            "Episode length: 951.20 +/- 42.07\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 951       |\n",
            "|    mean_reward     | -3.31e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29200, episode_reward=-3226.77 +/- 235.99\n",
            "Episode length: 933.40 +/- 34.67\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 933       |\n",
            "|    mean_reward     | -3.23e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=29600, episode_reward=-3237.93 +/- 90.43\n",
            "Episode length: 928.60 +/- 37.62\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 929       |\n",
            "|    mean_reward     | -3.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-3174.61 +/- 182.33\n",
            "Episode length: 918.40 +/- 31.70\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 918       |\n",
            "|    mean_reward     | -3.17e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30400, episode_reward=-3334.98 +/- 132.12\n",
            "Episode length: 943.40 +/- 38.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 943       |\n",
            "|    mean_reward     | -3.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30800, episode_reward=-3266.65 +/- 216.40\n",
            "Episode length: 928.60 +/- 31.13\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 929       |\n",
            "|    mean_reward     | -3.27e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31200, episode_reward=-3216.71 +/- 141.23\n",
            "Episode length: 932.20 +/- 34.20\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 932       |\n",
            "|    mean_reward     | -3.22e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=31600, episode_reward=-2654.32 +/- 1097.00\n",
            "Episode length: 826.20 +/- 187.26\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 826       |\n",
            "|    mean_reward     | -2.65e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-3333.58 +/- 91.01\n",
            "Episode length: 947.40 +/- 32.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 947       |\n",
            "|    mean_reward     | -3.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=32400, episode_reward=-2906.73 +/- 616.59\n",
            "Episode length: 924.00 +/- 38.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 924       |\n",
            "|    mean_reward     | -2.91e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32400     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 116      |\n",
            "|    ep_rew_mean     | -131     |\n",
            "| time/              |          |\n",
            "|    fps             | 68       |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 480      |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=32800, episode_reward=-1960.62 +/- 231.42\n",
            "Episode length: 334.40 +/- 24.94\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 334         |\n",
            "|    mean_reward          | -1.96e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 32800       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009660188 |\n",
            "|    clip_fraction        | 0.0569      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | -0.00132    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 203         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.007      |\n",
            "|    value_loss           | 529         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=33200, episode_reward=-2131.44 +/- 1022.56\n",
            "Episode length: 371.40 +/- 62.48\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 371       |\n",
            "|    mean_reward     | -2.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=33600, episode_reward=-1892.97 +/- 253.05\n",
            "Episode length: 350.80 +/- 37.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 351       |\n",
            "|    mean_reward     | -1.89e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=-2377.84 +/- 920.09\n",
            "Episode length: 377.20 +/- 76.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 377       |\n",
            "|    mean_reward     | -2.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34400, episode_reward=-1854.43 +/- 976.31\n",
            "Episode length: 349.60 +/- 54.44\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 350       |\n",
            "|    mean_reward     | -1.85e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=34800, episode_reward=-1726.08 +/- 183.76\n",
            "Episode length: 350.40 +/- 40.74\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 350       |\n",
            "|    mean_reward     | -1.73e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 34800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35200, episode_reward=-2036.42 +/- 127.41\n",
            "Episode length: 354.80 +/- 11.03\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 355       |\n",
            "|    mean_reward     | -2.04e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 35200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=35600, episode_reward=-1665.83 +/- 746.21\n",
            "Episode length: 364.40 +/- 57.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 364       |\n",
            "|    mean_reward     | -1.67e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 35600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=-1707.56 +/- 497.74\n",
            "Episode length: 350.80 +/- 30.39\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 351       |\n",
            "|    mean_reward     | -1.71e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36400, episode_reward=-1828.40 +/- 147.33\n",
            "Episode length: 363.20 +/- 41.30\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 363       |\n",
            "|    mean_reward     | -1.83e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=36800, episode_reward=-1561.72 +/- 241.32\n",
            "Episode length: 344.60 +/- 37.53\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 345       |\n",
            "|    mean_reward     | -1.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37200, episode_reward=-1426.88 +/- 575.82\n",
            "Episode length: 330.00 +/- 54.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 330       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=37600, episode_reward=-1577.31 +/- 623.53\n",
            "Episode length: 333.60 +/- 26.45\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 334       |\n",
            "|    mean_reward     | -1.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=-1703.14 +/- 281.89\n",
            "Episode length: 336.60 +/- 23.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 337      |\n",
            "|    mean_reward     | -1.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=38400, episode_reward=-1537.00 +/- 688.33\n",
            "Episode length: 346.00 +/- 49.63\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 346       |\n",
            "|    mean_reward     | -1.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 38400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=38800, episode_reward=-1804.32 +/- 1167.75\n",
            "Episode length: 342.80 +/- 62.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 343      |\n",
            "|    mean_reward     | -1.8e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=39200, episode_reward=-1690.18 +/- 288.00\n",
            "Episode length: 338.60 +/- 17.78\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 339       |\n",
            "|    mean_reward     | -1.69e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 39200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=39600, episode_reward=-1598.04 +/- 555.21\n",
            "Episode length: 329.80 +/- 41.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 330      |\n",
            "|    mean_reward     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 39600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-1972.39 +/- 297.39\n",
            "Episode length: 343.00 +/- 36.95\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 343       |\n",
            "|    mean_reward     | -1.97e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40400, episode_reward=-2392.48 +/- 1268.68\n",
            "Episode length: 376.80 +/- 103.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 377       |\n",
            "|    mean_reward     | -2.39e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40800, episode_reward=-2057.32 +/- 1130.68\n",
            "Episode length: 373.20 +/- 80.42\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 373       |\n",
            "|    mean_reward     | -2.06e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40800     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 115      |\n",
            "|    ep_rew_mean     | -126     |\n",
            "| time/              |          |\n",
            "|    fps             | 76       |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 535      |\n",
            "|    total_timesteps | 40960    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=41200, episode_reward=-4848.21 +/- 637.91\n",
            "Episode length: 745.80 +/- 42.19\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 746        |\n",
            "|    mean_reward          | -4.85e+03  |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 41200      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00626563 |\n",
            "|    clip_fraction        | 0.0411     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.29      |\n",
            "|    explained_variance   | -0.00885   |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 203        |\n",
            "|    n_updates            | 50         |\n",
            "|    policy_gradient_loss | -0.00496   |\n",
            "|    value_loss           | 492        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=41600, episode_reward=-4630.50 +/- 737.39\n",
            "Episode length: 758.40 +/- 37.66\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 758       |\n",
            "|    mean_reward     | -4.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 41600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=-4922.38 +/- 856.21\n",
            "Episode length: 757.60 +/- 57.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 758       |\n",
            "|    mean_reward     | -4.92e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=42400, episode_reward=-4698.02 +/- 562.87\n",
            "Episode length: 745.40 +/- 43.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 745      |\n",
            "|    mean_reward     | -4.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 42400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=42800, episode_reward=-5988.80 +/- 2572.82\n",
            "Episode length: 795.20 +/- 111.18\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 795       |\n",
            "|    mean_reward     | -5.99e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43200, episode_reward=-4435.67 +/- 1049.06\n",
            "Episode length: 740.40 +/- 43.72\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 740       |\n",
            "|    mean_reward     | -4.44e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=43600, episode_reward=-4157.77 +/- 329.74\n",
            "Episode length: 707.40 +/- 16.35\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 707       |\n",
            "|    mean_reward     | -4.16e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=-4439.91 +/- 667.72\n",
            "Episode length: 761.00 +/- 73.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 761       |\n",
            "|    mean_reward     | -4.44e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 44000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=44400, episode_reward=-6396.60 +/- 1982.59\n",
            "Episode length: 821.00 +/- 84.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 821      |\n",
            "|    mean_reward     | -6.4e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 44400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=44800, episode_reward=-4903.21 +/- 599.81\n",
            "Episode length: 761.20 +/- 49.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 761      |\n",
            "|    mean_reward     | -4.9e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 44800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=45200, episode_reward=-4567.35 +/- 555.98\n",
            "Episode length: 738.60 +/- 30.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 739       |\n",
            "|    mean_reward     | -4.57e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=45600, episode_reward=-5842.91 +/- 1293.69\n",
            "Episode length: 817.00 +/- 87.01\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 817       |\n",
            "|    mean_reward     | -5.84e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=-5679.48 +/- 2403.36\n",
            "Episode length: 790.00 +/- 117.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 790       |\n",
            "|    mean_reward     | -5.68e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46400, episode_reward=-4540.42 +/- 551.57\n",
            "Episode length: 738.60 +/- 46.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 739       |\n",
            "|    mean_reward     | -4.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=46800, episode_reward=-4815.59 +/- 1039.38\n",
            "Episode length: 748.80 +/- 61.84\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 749       |\n",
            "|    mean_reward     | -4.82e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47200, episode_reward=-4331.77 +/- 280.36\n",
            "Episode length: 711.60 +/- 15.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 712       |\n",
            "|    mean_reward     | -4.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=47600, episode_reward=-5172.51 +/- 1616.59\n",
            "Episode length: 773.80 +/- 87.03\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 774       |\n",
            "|    mean_reward     | -5.17e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-4545.10 +/- 777.71\n",
            "Episode length: 771.80 +/- 28.41\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 772       |\n",
            "|    mean_reward     | -4.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48400, episode_reward=-4476.91 +/- 556.56\n",
            "Episode length: 736.20 +/- 52.04\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 736       |\n",
            "|    mean_reward     | -4.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=48800, episode_reward=-4657.01 +/- 398.11\n",
            "Episode length: 774.00 +/- 57.93\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 774       |\n",
            "|    mean_reward     | -4.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 48800     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 113      |\n",
            "|    ep_rew_mean     | -103     |\n",
            "| time/              |          |\n",
            "|    fps             | 70       |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 696      |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=49200, episode_reward=-4257.52 +/- 1591.46\n",
            "Episode length: 713.60 +/- 75.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 714         |\n",
            "|    mean_reward          | -4.26e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 49200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008280546 |\n",
            "|    clip_fraction        | 0.0488      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.26       |\n",
            "|    explained_variance   | -0.00939    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 147         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00635    |\n",
            "|    value_loss           | 348         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=49600, episode_reward=-4533.66 +/- 977.23\n",
            "Episode length: 714.60 +/- 57.42\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 715       |\n",
            "|    mean_reward     | -4.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 49600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-3521.91 +/- 1894.30\n",
            "Episode length: 673.00 +/- 113.50\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 673       |\n",
            "|    mean_reward     | -3.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50400, episode_reward=-4482.17 +/- 816.37\n",
            "Episode length: 792.60 +/- 39.99\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 793       |\n",
            "|    mean_reward     | -4.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50800, episode_reward=-4478.56 +/- 793.79\n",
            "Episode length: 770.60 +/- 21.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 771       |\n",
            "|    mean_reward     | -4.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51200, episode_reward=-3730.09 +/- 567.52\n",
            "Episode length: 702.40 +/- 35.90\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 702       |\n",
            "|    mean_reward     | -3.73e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 51200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=51600, episode_reward=-4035.26 +/- 734.90\n",
            "Episode length: 715.80 +/- 36.94\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 716       |\n",
            "|    mean_reward     | -4.04e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 51600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=-3983.64 +/- 596.82\n",
            "Episode length: 712.80 +/- 53.12\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 713       |\n",
            "|    mean_reward     | -3.98e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 52000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=52400, episode_reward=-4504.84 +/- 1625.11\n",
            "Episode length: 784.00 +/- 74.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 784      |\n",
            "|    mean_reward     | -4.5e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 52400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=52800, episode_reward=-4707.24 +/- 1038.72\n",
            "Episode length: 733.40 +/- 78.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 733       |\n",
            "|    mean_reward     | -4.71e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 52800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53200, episode_reward=-4293.17 +/- 715.06\n",
            "Episode length: 734.20 +/- 53.30\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 734       |\n",
            "|    mean_reward     | -4.29e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 53200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=53600, episode_reward=-4281.55 +/- 1036.01\n",
            "Episode length: 742.60 +/- 106.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 743       |\n",
            "|    mean_reward     | -4.28e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 53600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=-4135.58 +/- 766.99\n",
            "Episode length: 693.00 +/- 41.76\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 693       |\n",
            "|    mean_reward     | -4.14e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54400, episode_reward=-4146.88 +/- 1270.10\n",
            "Episode length: 700.80 +/- 60.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 701       |\n",
            "|    mean_reward     | -4.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=54800, episode_reward=-4674.05 +/- 787.12\n",
            "Episode length: 752.20 +/- 42.56\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 752       |\n",
            "|    mean_reward     | -4.67e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 54800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55200, episode_reward=-3767.09 +/- 731.26\n",
            "Episode length: 703.20 +/- 64.98\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 703       |\n",
            "|    mean_reward     | -3.77e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 55200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=55600, episode_reward=-4531.69 +/- 896.56\n",
            "Episode length: 747.00 +/- 51.66\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 747       |\n",
            "|    mean_reward     | -4.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 55600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-3842.70 +/- 676.93\n",
            "Episode length: 683.00 +/- 41.05\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 683       |\n",
            "|    mean_reward     | -3.84e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 56000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56400, episode_reward=-3515.03 +/- 279.71\n",
            "Episode length: 672.60 +/- 27.55\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 673       |\n",
            "|    mean_reward     | -3.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 56400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=56800, episode_reward=-4096.78 +/- 918.93\n",
            "Episode length: 694.80 +/- 51.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 695      |\n",
            "|    mean_reward     | -4.1e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 56800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=57200, episode_reward=-3545.31 +/- 608.20\n",
            "Episode length: 701.80 +/- 52.43\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 702       |\n",
            "|    mean_reward     | -3.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 57200     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 129      |\n",
            "|    ep_rew_mean     | -99.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 67       |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 848      |\n",
            "|    total_timesteps | 57344    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=57600, episode_reward=-1451.35 +/- 189.99\n",
            "Episode length: 401.20 +/- 13.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 401         |\n",
            "|    mean_reward          | -1.45e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 57600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010022926 |\n",
            "|    clip_fraction        | 0.0409      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | -0.0642     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 198         |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00602    |\n",
            "|    value_loss           | 371         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=58000, episode_reward=-1631.56 +/- 268.57\n",
            "Episode length: 429.00 +/- 37.73\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 429       |\n",
            "|    mean_reward     | -1.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58400, episode_reward=-1448.76 +/- 278.55\n",
            "Episode length: 414.60 +/- 44.90\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 415       |\n",
            "|    mean_reward     | -1.45e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=58800, episode_reward=-1825.79 +/- 146.12\n",
            "Episode length: 461.80 +/- 49.60\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 462       |\n",
            "|    mean_reward     | -1.83e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 58800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59200, episode_reward=-1661.28 +/- 229.67\n",
            "Episode length: 438.40 +/- 47.22\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 438       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 59200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=59600, episode_reward=-1450.06 +/- 306.77\n",
            "Episode length: 401.60 +/- 57.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 402       |\n",
            "|    mean_reward     | -1.45e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 59600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-1738.39 +/- 224.15\n",
            "Episode length: 447.80 +/- 26.35\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 448       |\n",
            "|    mean_reward     | -1.74e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60400, episode_reward=-1518.42 +/- 200.28\n",
            "Episode length: 402.40 +/- 12.89\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 402       |\n",
            "|    mean_reward     | -1.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60800, episode_reward=-1675.14 +/- 155.14\n",
            "Episode length: 418.80 +/- 32.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 419       |\n",
            "|    mean_reward     | -1.68e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 60800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61200, episode_reward=-1484.41 +/- 150.26\n",
            "Episode length: 404.20 +/- 15.38\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 404       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 61200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=61600, episode_reward=-1560.11 +/- 206.02\n",
            "Episode length: 406.80 +/- 28.84\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 407       |\n",
            "|    mean_reward     | -1.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 61600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62000, episode_reward=-1627.64 +/- 276.83\n",
            "Episode length: 434.40 +/- 48.92\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 434       |\n",
            "|    mean_reward     | -1.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 62000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62400, episode_reward=-1643.91 +/- 196.53\n",
            "Episode length: 417.20 +/- 44.94\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 417       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 62400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=62800, episode_reward=-1699.18 +/- 217.33\n",
            "Episode length: 428.20 +/- 51.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 428      |\n",
            "|    mean_reward     | -1.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 62800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=63200, episode_reward=-1563.64 +/- 202.04\n",
            "Episode length: 418.20 +/- 48.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 418       |\n",
            "|    mean_reward     | -1.56e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 63200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=63600, episode_reward=-1507.66 +/- 297.08\n",
            "Episode length: 416.40 +/- 37.15\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 416       |\n",
            "|    mean_reward     | -1.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 63600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64000, episode_reward=-1639.26 +/- 200.84\n",
            "Episode length: 427.40 +/- 39.78\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 427       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64400, episode_reward=-1325.35 +/- 213.20\n",
            "Episode length: 395.80 +/- 13.56\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 396       |\n",
            "|    mean_reward     | -1.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=64800, episode_reward=-1433.99 +/- 349.05\n",
            "Episode length: 400.00 +/- 28.40\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 400       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 64800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=65200, episode_reward=-1409.91 +/- 359.71\n",
            "Episode length: 416.80 +/- 26.23\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 417       |\n",
            "|    mean_reward     | -1.41e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 65200     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | -90.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 71       |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 913      |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=65600, episode_reward=-1409.58 +/- 403.86\n",
            "Episode length: 418.20 +/- 65.86\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 418         |\n",
            "|    mean_reward          | -1.41e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 65600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011920803 |\n",
            "|    clip_fraction        | 0.0538      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | -0.0326     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 238         |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00679    |\n",
            "|    value_loss           | 391         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=66000, episode_reward=-1514.17 +/- 165.99\n",
            "Episode length: 400.80 +/- 32.28\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 401       |\n",
            "|    mean_reward     | -1.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66400, episode_reward=-1483.80 +/- 260.13\n",
            "Episode length: 406.40 +/- 52.22\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 406       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=66800, episode_reward=-1627.89 +/- 182.64\n",
            "Episode length: 436.60 +/- 57.18\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 437       |\n",
            "|    mean_reward     | -1.63e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 66800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67200, episode_reward=-1416.16 +/- 123.02\n",
            "Episode length: 389.40 +/- 15.42\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 389       |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 67200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=67600, episode_reward=-1434.16 +/- 225.87\n",
            "Episode length: 391.60 +/- 26.26\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 392       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 67600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68000, episode_reward=-1328.40 +/- 260.42\n",
            "Episode length: 394.80 +/- 37.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 395       |\n",
            "|    mean_reward     | -1.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68400, episode_reward=-1647.93 +/- 42.52\n",
            "Episode length: 419.60 +/- 14.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 420       |\n",
            "|    mean_reward     | -1.65e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=68800, episode_reward=-1339.80 +/- 250.23\n",
            "Episode length: 391.40 +/- 33.18\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 391       |\n",
            "|    mean_reward     | -1.34e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 68800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69200, episode_reward=-1617.10 +/- 324.81\n",
            "Episode length: 450.60 +/- 62.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 451       |\n",
            "|    mean_reward     | -1.62e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 69200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=69600, episode_reward=-1514.01 +/- 353.70\n",
            "Episode length: 426.40 +/- 39.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 426       |\n",
            "|    mean_reward     | -1.51e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 69600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-1242.13 +/- 256.20\n",
            "Episode length: 376.20 +/- 16.15\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 376       |\n",
            "|    mean_reward     | -1.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70400, episode_reward=-1484.20 +/- 252.47\n",
            "Episode length: 406.60 +/- 39.58\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 407       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70800, episode_reward=-1379.75 +/- 204.11\n",
            "Episode length: 397.60 +/- 22.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 398       |\n",
            "|    mean_reward     | -1.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 70800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=71200, episode_reward=-1299.61 +/- 284.12\n",
            "Episode length: 393.40 +/- 27.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 393      |\n",
            "|    mean_reward     | -1.3e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 71200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=71600, episode_reward=-1663.04 +/- 159.35\n",
            "Episode length: 434.80 +/- 45.97\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 435       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 71600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72000, episode_reward=-1339.08 +/- 300.75\n",
            "Episode length: 387.00 +/- 19.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 387       |\n",
            "|    mean_reward     | -1.34e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72400, episode_reward=-1638.04 +/- 142.65\n",
            "Episode length: 430.40 +/- 37.23\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 430       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=72800, episode_reward=-1550.12 +/- 114.52\n",
            "Episode length: 409.40 +/- 9.39\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 409       |\n",
            "|    mean_reward     | -1.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 72800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73200, episode_reward=-1460.21 +/- 182.29\n",
            "Episode length: 396.80 +/- 39.27\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 397       |\n",
            "|    mean_reward     | -1.46e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 73200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=73600, episode_reward=-1275.24 +/- 242.84\n",
            "Episode length: 380.80 +/- 24.99\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 381       |\n",
            "|    mean_reward     | -1.28e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 73600     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 178      |\n",
            "|    ep_rew_mean     | -76.5    |\n",
            "| time/              |          |\n",
            "|    fps             | 75       |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 982      |\n",
            "|    total_timesteps | 73728    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=74000, episode_reward=-1588.76 +/- 196.05\n",
            "Episode length: 422.20 +/- 51.27\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 422         |\n",
            "|    mean_reward          | -1.59e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 74000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007009428 |\n",
            "|    clip_fraction        | 0.0106      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.0571      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 77.1        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00239    |\n",
            "|    value_loss           | 231         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=74400, episode_reward=-1532.27 +/- 182.72\n",
            "Episode length: 411.20 +/- 45.33\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 411       |\n",
            "|    mean_reward     | -1.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 74400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=74800, episode_reward=-1433.06 +/- 295.88\n",
            "Episode length: 405.60 +/- 59.26\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 406       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 74800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75200, episode_reward=-1380.14 +/- 361.76\n",
            "Episode length: 391.60 +/- 60.04\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 392       |\n",
            "|    mean_reward     | -1.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 75200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=75600, episode_reward=-1522.86 +/- 110.49\n",
            "Episode length: 416.60 +/- 23.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 417       |\n",
            "|    mean_reward     | -1.52e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 75600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76000, episode_reward=-1236.14 +/- 285.46\n",
            "Episode length: 379.40 +/- 50.74\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 379       |\n",
            "|    mean_reward     | -1.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76400, episode_reward=-1527.13 +/- 142.82\n",
            "Episode length: 405.60 +/- 27.59\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 406       |\n",
            "|    mean_reward     | -1.53e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=76800, episode_reward=-1453.25 +/- 224.56\n",
            "Episode length: 402.40 +/- 39.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 402       |\n",
            "|    mean_reward     | -1.45e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 76800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77200, episode_reward=-1483.59 +/- 234.57\n",
            "Episode length: 416.20 +/- 58.83\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 416       |\n",
            "|    mean_reward     | -1.48e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 77200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=77600, episode_reward=-1233.26 +/- 264.05\n",
            "Episode length: 363.60 +/- 22.86\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 364       |\n",
            "|    mean_reward     | -1.23e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 77600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78000, episode_reward=-1426.37 +/- 265.19\n",
            "Episode length: 393.20 +/- 47.13\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 393       |\n",
            "|    mean_reward     | -1.43e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78400, episode_reward=-1129.40 +/- 259.47\n",
            "Episode length: 356.80 +/- 28.91\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 357       |\n",
            "|    mean_reward     | -1.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=78800, episode_reward=-1540.40 +/- 245.79\n",
            "Episode length: 421.00 +/- 52.44\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 421       |\n",
            "|    mean_reward     | -1.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 78800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79200, episode_reward=-1461.88 +/- 221.58\n",
            "Episode length: 403.80 +/- 49.61\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 404       |\n",
            "|    mean_reward     | -1.46e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 79200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=79600, episode_reward=-1241.62 +/- 287.51\n",
            "Episode length: 383.40 +/- 56.82\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 383       |\n",
            "|    mean_reward     | -1.24e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 79600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-1389.98 +/- 181.58\n",
            "Episode length: 383.00 +/- 28.78\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 383       |\n",
            "|    mean_reward     | -1.39e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80400, episode_reward=-1361.74 +/- 197.42\n",
            "Episode length: 392.80 +/- 27.67\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 393       |\n",
            "|    mean_reward     | -1.36e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 80400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80800, episode_reward=-1300.71 +/- 214.99\n",
            "Episode length: 380.00 +/- 19.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 380      |\n",
            "|    mean_reward     | -1.3e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 80800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=81200, episode_reward=-1231.82 +/- 295.99\n",
            "Episode length: 364.20 +/- 41.54\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 364       |\n",
            "|    mean_reward     | -1.23e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 81200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=81600, episode_reward=-1248.09 +/- 102.70\n",
            "Episode length: 357.40 +/- 8.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 357       |\n",
            "|    mean_reward     | -1.25e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 81600     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | -92      |\n",
            "| time/              |          |\n",
            "|    fps             | 78       |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 1046     |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=82000, episode_reward=-1411.30 +/- 402.27\n",
            "Episode length: 671.20 +/- 96.51\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 671         |\n",
            "|    mean_reward          | -1.41e+03   |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 82000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005784146 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.029       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 140         |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00556    |\n",
            "|    value_loss           | 298         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=82400, episode_reward=-1415.23 +/- 255.75\n",
            "Episode length: 663.00 +/- 70.03\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 663       |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 82400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=82800, episode_reward=-1536.12 +/- 441.92\n",
            "Episode length: 697.80 +/- 101.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 698       |\n",
            "|    mean_reward     | -1.54e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 82800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83200, episode_reward=-1363.57 +/- 369.60\n",
            "Episode length: 638.00 +/- 77.66\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 638       |\n",
            "|    mean_reward     | -1.36e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 83200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=83600, episode_reward=-1699.20 +/- 306.16\n",
            "Episode length: 717.60 +/- 86.28\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 718      |\n",
            "|    mean_reward     | -1.7e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 83600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=84000, episode_reward=-1546.96 +/- 582.88\n",
            "Episode length: 697.80 +/- 144.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 698       |\n",
            "|    mean_reward     | -1.55e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 84000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=84400, episode_reward=-1199.77 +/- 258.18\n",
            "Episode length: 613.00 +/- 48.27\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 613      |\n",
            "|    mean_reward     | -1.2e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 84400    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=84800, episode_reward=-1597.90 +/- 298.59\n",
            "Episode length: 708.00 +/- 65.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 708      |\n",
            "|    mean_reward     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 84800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=85200, episode_reward=-1395.97 +/- 227.92\n",
            "Episode length: 656.40 +/- 46.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 656      |\n",
            "|    mean_reward     | -1.4e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 85200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=85600, episode_reward=-1597.24 +/- 192.03\n",
            "Episode length: 702.40 +/- 39.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 702      |\n",
            "|    mean_reward     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 85600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=86000, episode_reward=-1659.54 +/- 189.30\n",
            "Episode length: 703.60 +/- 44.77\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 704       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86400, episode_reward=-1819.74 +/- 204.53\n",
            "Episode length: 756.00 +/- 58.47\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 756       |\n",
            "|    mean_reward     | -1.82e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=86800, episode_reward=-1605.22 +/- 267.23\n",
            "Episode length: 694.00 +/- 52.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 694       |\n",
            "|    mean_reward     | -1.61e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 86800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87200, episode_reward=-1371.93 +/- 379.08\n",
            "Episode length: 661.60 +/- 80.45\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 662       |\n",
            "|    mean_reward     | -1.37e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 87200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=87600, episode_reward=-1643.54 +/- 355.98\n",
            "Episode length: 713.60 +/- 96.84\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 714       |\n",
            "|    mean_reward     | -1.64e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 87600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88000, episode_reward=-1575.54 +/- 459.66\n",
            "Episode length: 697.60 +/- 120.36\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 698       |\n",
            "|    mean_reward     | -1.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88400, episode_reward=-1867.15 +/- 182.16\n",
            "Episode length: 762.40 +/- 51.72\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 762       |\n",
            "|    mean_reward     | -1.87e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=88800, episode_reward=-1423.29 +/- 204.72\n",
            "Episode length: 657.40 +/- 46.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 657       |\n",
            "|    mean_reward     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 88800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89200, episode_reward=-1346.32 +/- 231.94\n",
            "Episode length: 641.00 +/- 53.34\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 641       |\n",
            "|    mean_reward     | -1.35e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 89200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=89600, episode_reward=-1664.15 +/- 296.69\n",
            "Episode length: 720.00 +/- 71.54\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 720       |\n",
            "|    mean_reward     | -1.66e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 89600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-1582.55 +/- 358.45\n",
            "Episode length: 687.40 +/- 81.87\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 687       |\n",
            "|    mean_reward     | -1.58e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90000     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 270      |\n",
            "|    ep_rew_mean     | -83.4    |\n",
            "| time/              |          |\n",
            "|    fps             | 75       |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 1192     |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90400, episode_reward=-1434.98 +/- 171.39\n",
            "Episode length: 957.80 +/- 81.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 958          |\n",
            "|    mean_reward          | -1.43e+03    |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 90400        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067716427 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.17        |\n",
            "|    explained_variance   | -0.0442      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 103          |\n",
            "|    n_updates            | 110          |\n",
            "|    policy_gradient_loss | -0.00368     |\n",
            "|    value_loss           | 246          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=90800, episode_reward=-1181.32 +/- 389.44\n",
            "Episode length: 819.80 +/- 173.98\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 820       |\n",
            "|    mean_reward     | -1.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 90800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91200, episode_reward=-1316.71 +/- 190.79\n",
            "Episode length: 872.60 +/- 75.16\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 873       |\n",
            "|    mean_reward     | -1.32e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 91200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=91600, episode_reward=-1319.78 +/- 239.49\n",
            "Episode length: 867.40 +/- 96.56\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 867       |\n",
            "|    mean_reward     | -1.32e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 91600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92000, episode_reward=-1328.83 +/- 112.57\n",
            "Episode length: 880.20 +/- 41.82\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 880       |\n",
            "|    mean_reward     | -1.33e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92400, episode_reward=-1164.50 +/- 461.61\n",
            "Episode length: 805.60 +/- 224.53\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 806       |\n",
            "|    mean_reward     | -1.16e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=92800, episode_reward=-1151.36 +/- 230.18\n",
            "Episode length: 806.40 +/- 93.46\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 806       |\n",
            "|    mean_reward     | -1.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 92800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93200, episode_reward=-1178.41 +/- 422.32\n",
            "Episode length: 825.00 +/- 209.79\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 825       |\n",
            "|    mean_reward     | -1.18e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 93200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=93600, episode_reward=-1126.08 +/- 404.27\n",
            "Episode length: 786.40 +/- 182.53\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 786       |\n",
            "|    mean_reward     | -1.13e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 93600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94000, episode_reward=-1206.70 +/- 195.79\n",
            "Episode length: 849.80 +/- 82.41\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 850       |\n",
            "|    mean_reward     | -1.21e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94400, episode_reward=-1092.52 +/- 262.39\n",
            "Episode length: 799.40 +/- 120.65\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 799       |\n",
            "|    mean_reward     | -1.09e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=94800, episode_reward=-1221.81 +/- 296.80\n",
            "Episode length: 843.40 +/- 135.97\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 843       |\n",
            "|    mean_reward     | -1.22e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 94800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95200, episode_reward=-1379.72 +/- 133.57\n",
            "Episode length: 924.20 +/- 66.10\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 924       |\n",
            "|    mean_reward     | -1.38e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 95200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=95600, episode_reward=-976.29 +/- 442.60\n",
            "Episode length: 729.20 +/- 196.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 729      |\n",
            "|    mean_reward     | -976     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=-1249.66 +/- 314.23\n",
            "Episode length: 856.20 +/- 119.24\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 856       |\n",
            "|    mean_reward     | -1.25e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96000     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96400, episode_reward=-1159.03 +/- 301.03\n",
            "Episode length: 815.60 +/- 116.14\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 816       |\n",
            "|    mean_reward     | -1.16e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96400     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=96800, episode_reward=-1066.44 +/- 287.07\n",
            "Episode length: 767.00 +/- 123.81\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 767       |\n",
            "|    mean_reward     | -1.07e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 96800     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97200, episode_reward=-1194.84 +/- 301.13\n",
            "Episode length: 830.80 +/- 119.37\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 831       |\n",
            "|    mean_reward     | -1.19e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 97200     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=97600, episode_reward=-1121.09 +/- 394.99\n",
            "Episode length: 779.20 +/- 178.45\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 779       |\n",
            "|    mean_reward     | -1.12e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 97600     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=98000, episode_reward=-1255.23 +/- 378.77\n",
            "Episode length: 833.00 +/- 158.49\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 833       |\n",
            "|    mean_reward     | -1.26e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 98000     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 305      |\n",
            "|    ep_rew_mean     | -67.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 71       |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 1384     |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98400, episode_reward=-802.30 +/- 265.29\n",
            "Episode length: 611.20 +/- 156.09\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 611       |\n",
            "|    mean_reward          | -802      |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 98400     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0089767 |\n",
            "|    clip_fraction        | 0.0688    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.09     |\n",
            "|    explained_variance   | 0.0354    |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | 116       |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -0.00618  |\n",
            "|    value_loss           | 199       |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=98800, episode_reward=-723.64 +/- 340.95\n",
            "Episode length: 548.80 +/- 192.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 549      |\n",
            "|    mean_reward     | -724     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98800    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99200, episode_reward=-951.35 +/- 71.09\n",
            "Episode length: 693.40 +/- 53.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 693      |\n",
            "|    mean_reward     | -951     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99200    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99600, episode_reward=-794.24 +/- 332.50\n",
            "Episode length: 584.60 +/- 191.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 585      |\n",
            "|    mean_reward     | -794     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99600    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-1005.75 +/- 216.38\n",
            "Episode length: 707.20 +/- 112.75\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 707       |\n",
            "|    mean_reward     | -1.01e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100000    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100400, episode_reward=-858.82 +/- 150.78\n",
            "Episode length: 620.60 +/- 81.09\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 621      |\n",
            "|    mean_reward     | -859     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 100400   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100800, episode_reward=-1154.44 +/- 172.24\n",
            "Episode length: 773.00 +/- 84.28\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 773       |\n",
            "|    mean_reward     | -1.15e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 100800    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=101200, episode_reward=-806.12 +/- 198.35\n",
            "Episode length: 590.40 +/- 114.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 590      |\n",
            "|    mean_reward     | -806     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 101200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=101600, episode_reward=-687.15 +/- 332.60\n",
            "Episode length: 529.40 +/- 194.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 529      |\n",
            "|    mean_reward     | -687     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 101600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=102000, episode_reward=-791.80 +/- 227.08\n",
            "Episode length: 603.40 +/- 129.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 603      |\n",
            "|    mean_reward     | -792     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 102000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=102400, episode_reward=-785.66 +/- 197.58\n",
            "Episode length: 592.60 +/- 116.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 593      |\n",
            "|    mean_reward     | -786     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 102400   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=102800, episode_reward=-789.65 +/- 257.48\n",
            "Episode length: 588.20 +/- 136.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 588      |\n",
            "|    mean_reward     | -790     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 102800   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=103200, episode_reward=-989.49 +/- 309.26\n",
            "Episode length: 701.20 +/- 174.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 701      |\n",
            "|    mean_reward     | -989     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 103200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=103600, episode_reward=-676.80 +/- 242.73\n",
            "Episode length: 521.60 +/- 128.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 522      |\n",
            "|    mean_reward     | -677     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 103600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104000, episode_reward=-905.97 +/- 244.70\n",
            "Episode length: 663.80 +/- 141.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 664      |\n",
            "|    mean_reward     | -906     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 104000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=104400, episode_reward=-1036.11 +/- 204.68\n",
            "Episode length: 709.00 +/- 117.69\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 709       |\n",
            "|    mean_reward     | -1.04e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 104400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=104800, episode_reward=-751.47 +/- 383.25\n",
            "Episode length: 582.20 +/- 209.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 582      |\n",
            "|    mean_reward     | -751     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 104800   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=105200, episode_reward=-904.41 +/- 402.71\n",
            "Episode length: 638.20 +/- 230.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 638      |\n",
            "|    mean_reward     | -904     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 105200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=105600, episode_reward=-943.37 +/- 184.60\n",
            "Episode length: 669.60 +/- 102.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 670      |\n",
            "|    mean_reward     | -943     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 105600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=106000, episode_reward=-797.61 +/- 204.31\n",
            "Episode length: 581.60 +/- 115.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 582      |\n",
            "|    mean_reward     | -798     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 106000   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=106400, episode_reward=-992.00 +/- 281.04\n",
            "Episode length: 707.40 +/- 149.22\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 707      |\n",
            "|    mean_reward     | -992     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 106400   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 324      |\n",
            "|    ep_rew_mean     | -59.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 70       |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 1512     |\n",
            "|    total_timesteps | 106496   |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKbJj8a4Obhr",
        "outputId": "011b6734-321f-4b27-d0d3-adfa11e0be83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (4.2.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair) (4.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from altair) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.10/dist-packages (from altair) (2.0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair) (0.12.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair) (0.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.18->altair) (2024.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.18->altair) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt"
      ],
      "metadata": {
        "id": "x8GnkAOtOc9q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs_dqn = np.load('evaluations_dqn.npz', allow_pickle=True)\n",
        "logs_ppo = np.load('evaluations_ppo.npz', allow_pickle=True)"
      ],
      "metadata": {
        "id": "aDoG76tXOrIc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_dqn = np.array(logs_dqn['results'])\n",
        "reward_ppo = np.array(logs_ppo['results'])"
      ],
      "metadata": {
        "id": "pJd7NBuNOzdi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_dqn_flat = reward_dqn.flatten()\n",
        "timesteps_dqn = np.arange(len(reward_dqn_flat))\n",
        "df_dqn = pd.DataFrame({'timesteps': timesteps_dqn, 'rewards': reward_dqn_flat})"
      ],
      "metadata": {
        "id": "s4prgCnaO-7J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_ppo_flat = reward_ppo.flatten()\n",
        "timesteps_ppo = np.arange(len(reward_ppo_flat))\n",
        "df_ppo = pd.DataFrame({'timesteps': timesteps_ppo, 'rewards': reward_ppo_flat})"
      ],
      "metadata": {
        "id": "Vb2kD3n_PXEA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Altair chart for DQN rewards\n",
        "chart_dqn = alt.Chart(df_dqn).mark_line().encode(\n",
        "    x='timesteps',\n",
        "    y='rewards',\n",
        "    tooltip=['timesteps', 'rewards']\n",
        ").properties(\n",
        "    title='Rewards over Timesteps (DQN)'\n",
        ").interactive()\n",
        "\n",
        "# Create Altair chart for PPO rewards\n",
        "chart_ppo = alt.Chart(df_ppo).mark_line().encode(\n",
        "    x='timesteps',\n",
        "    y='rewards',\n",
        "    tooltip=['timesteps', 'rewards']\n",
        ").properties(\n",
        "    title='Rewards over Timesteps (PPO)'\n",
        ").interactive()\n",
        "\n",
        "# Display charts\n",
        "chart_dqn | chart_ppo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "a1yIKihuPdK1",
        "outputId": "4fd28506-3c4f-4ef4-e2b1-91d548deef0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-80ecdb80aa3a43dd832d40320403ea49\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-80ecdb80aa3a43dd832d40320403ea49\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-80ecdb80aa3a43dd832d40320403ea49\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"hconcat\": [{\"data\": {\"name\": \"data-cb182051f3dbfd712097a326b3d60296\"}, \"mark\": \"line\", \"encoding\": {\"tooltip\": [{\"field\": \"timesteps\", \"type\": \"quantitative\"}, {\"field\": \"rewards\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"timesteps\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"rewards\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Rewards over Timesteps (DQN)\"}, {\"data\": {\"name\": \"data-61cb47bb02dfc45b226a66d84b1aefc2\"}, \"mark\": \"line\", \"encoding\": {\"tooltip\": [{\"field\": \"timesteps\", \"type\": \"quantitative\"}, {\"field\": \"rewards\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"timesteps\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"rewards\", \"type\": \"quantitative\"}}, \"selection\": {\"selector002\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Rewards over Timesteps (PPO)\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-cb182051f3dbfd712097a326b3d60296\": [{\"timesteps\": 0, \"rewards\": -315.476646}, {\"timesteps\": 1, \"rewards\": -412.859727}, {\"timesteps\": 2, \"rewards\": -379.438579}, {\"timesteps\": 3, \"rewards\": -481.444647}, {\"timesteps\": 4, \"rewards\": -417.083945}, {\"timesteps\": 5, \"rewards\": -256.919649}, {\"timesteps\": 6, \"rewards\": -313.355101}, {\"timesteps\": 7, \"rewards\": -368.667657}, {\"timesteps\": 8, \"rewards\": -378.231738}, {\"timesteps\": 9, \"rewards\": -327.254184}, {\"timesteps\": 10, \"rewards\": -275.625392}, {\"timesteps\": 11, \"rewards\": -290.939908}, {\"timesteps\": 12, \"rewards\": -282.827517}, {\"timesteps\": 13, \"rewards\": -314.18052}, {\"timesteps\": 14, \"rewards\": -200.6319}, {\"timesteps\": 15, \"rewards\": -244.599378}, {\"timesteps\": 16, \"rewards\": -213.294386}, {\"timesteps\": 17, \"rewards\": -231.505079}, {\"timesteps\": 18, \"rewards\": -249.107222}, {\"timesteps\": 19, \"rewards\": -248.874675}, {\"timesteps\": 20, \"rewards\": -183.490965}, {\"timesteps\": 21, \"rewards\": -203.571167}, {\"timesteps\": 22, \"rewards\": -164.30366}, {\"timesteps\": 23, \"rewards\": 24.12843}, {\"timesteps\": 24, \"rewards\": -207.70964}, {\"timesteps\": 25, \"rewards\": -118.400649}, {\"timesteps\": 26, \"rewards\": -103.501872}, {\"timesteps\": 27, \"rewards\": -194.356668}, {\"timesteps\": 28, \"rewards\": -96.934476}, {\"timesteps\": 29, \"rewards\": -190.99447}, {\"timesteps\": 30, \"rewards\": -96.254905}, {\"timesteps\": 31, \"rewards\": -33.872996}, {\"timesteps\": 32, \"rewards\": -101.052674}, {\"timesteps\": 33, \"rewards\": -100.989091}, {\"timesteps\": 34, \"rewards\": -248.760791}, {\"timesteps\": 35, \"rewards\": -241.665487}, {\"timesteps\": 36, \"rewards\": -12.736158}, {\"timesteps\": 37, \"rewards\": -248.179855}, {\"timesteps\": 38, \"rewards\": -79.300243}, {\"timesteps\": 39, \"rewards\": -102.31486}, {\"timesteps\": 40, \"rewards\": -111.548471}, {\"timesteps\": 41, \"rewards\": -189.156626}, {\"timesteps\": 42, \"rewards\": -127.137853}, {\"timesteps\": 43, \"rewards\": -158.339387}, {\"timesteps\": 44, \"rewards\": -94.541053}, {\"timesteps\": 45, \"rewards\": -311.69926}, {\"timesteps\": 46, \"rewards\": -610.288074}, {\"timesteps\": 47, \"rewards\": -568.80404}, {\"timesteps\": 48, \"rewards\": -598.780463}, {\"timesteps\": 49, \"rewards\": -642.770073}, {\"timesteps\": 50, \"rewards\": -296.61533}, {\"timesteps\": 51, \"rewards\": -382.316228}, {\"timesteps\": 52, \"rewards\": -362.474918}, {\"timesteps\": 53, \"rewards\": -716.359048}, {\"timesteps\": 54, \"rewards\": -881.733541}, {\"timesteps\": 55, \"rewards\": -666.469754}, {\"timesteps\": 56, \"rewards\": -875.614128}, {\"timesteps\": 57, \"rewards\": -748.300304}, {\"timesteps\": 58, \"rewards\": -964.594101}, {\"timesteps\": 59, \"rewards\": -786.629078}, {\"timesteps\": 60, \"rewards\": -188.657238}, {\"timesteps\": 61, \"rewards\": -399.704248}, {\"timesteps\": 62, \"rewards\": -314.49379}, {\"timesteps\": 63, \"rewards\": -536.646842}, {\"timesteps\": 64, \"rewards\": -501.718018}, {\"timesteps\": 65, \"rewards\": -416.144437}, {\"timesteps\": 66, \"rewards\": -336.768692}, {\"timesteps\": 67, \"rewards\": -387.831756}, {\"timesteps\": 68, \"rewards\": -377.897493}, {\"timesteps\": 69, \"rewards\": -337.255115}, {\"timesteps\": 70, \"rewards\": -312.962296}, {\"timesteps\": 71, \"rewards\": -259.987216}, {\"timesteps\": 72, \"rewards\": -299.620456}, {\"timesteps\": 73, \"rewards\": -259.863795}, {\"timesteps\": 74, \"rewards\": -216.392056}, {\"timesteps\": 75, \"rewards\": -549.865509}, {\"timesteps\": 76, \"rewards\": -334.016145}, {\"timesteps\": 77, \"rewards\": -263.734302}, {\"timesteps\": 78, \"rewards\": -419.010743}, {\"timesteps\": 79, \"rewards\": -176.795997}, {\"timesteps\": 80, \"rewards\": -494.223711}, {\"timesteps\": 81, \"rewards\": -184.981587}, {\"timesteps\": 82, \"rewards\": -227.257337}, {\"timesteps\": 83, \"rewards\": -228.151151}, {\"timesteps\": 84, \"rewards\": -462.013348}, {\"timesteps\": 85, \"rewards\": -478.709297}, {\"timesteps\": 86, \"rewards\": -486.738149}, {\"timesteps\": 87, \"rewards\": -133.14596}, {\"timesteps\": 88, \"rewards\": -483.726521}, {\"timesteps\": 89, \"rewards\": -206.112586}, {\"timesteps\": 90, \"rewards\": -419.563573}, {\"timesteps\": 91, \"rewards\": -201.62103}, {\"timesteps\": 92, \"rewards\": -216.445342}, {\"timesteps\": 93, \"rewards\": -420.468551}, {\"timesteps\": 94, \"rewards\": -492.095744}, {\"timesteps\": 95, \"rewards\": -268.274382}, {\"timesteps\": 96, \"rewards\": -207.63958}, {\"timesteps\": 97, \"rewards\": -289.346178}, {\"timesteps\": 98, \"rewards\": -364.733163}, {\"timesteps\": 99, \"rewards\": -371.539666}, {\"timesteps\": 100, \"rewards\": -239.893296}, {\"timesteps\": 101, \"rewards\": -161.507253}, {\"timesteps\": 102, \"rewards\": -163.011976}, {\"timesteps\": 103, \"rewards\": -225.23354}, {\"timesteps\": 104, \"rewards\": -161.891901}, {\"timesteps\": 105, \"rewards\": -315.388972}, {\"timesteps\": 106, \"rewards\": -324.996473}, {\"timesteps\": 107, \"rewards\": -193.485778}, {\"timesteps\": 108, \"rewards\": -300.93049}, {\"timesteps\": 109, \"rewards\": -327.566567}, {\"timesteps\": 110, \"rewards\": -238.00285}, {\"timesteps\": 111, \"rewards\": -227.549219}, {\"timesteps\": 112, \"rewards\": -183.321242}, {\"timesteps\": 113, \"rewards\": -212.987606}, {\"timesteps\": 114, \"rewards\": -105.629884}, {\"timesteps\": 115, \"rewards\": -282.547039}, {\"timesteps\": 116, \"rewards\": -195.22839}, {\"timesteps\": 117, \"rewards\": -192.400473}, {\"timesteps\": 118, \"rewards\": -167.244058}, {\"timesteps\": 119, \"rewards\": -234.433772}, {\"timesteps\": 120, \"rewards\": -349.423114}, {\"timesteps\": 121, \"rewards\": -168.859315}, {\"timesteps\": 122, \"rewards\": -174.906609}, {\"timesteps\": 123, \"rewards\": -192.937072}, {\"timesteps\": 124, \"rewards\": -220.884515}, {\"timesteps\": 125, \"rewards\": -187.067198}, {\"timesteps\": 126, \"rewards\": -171.787653}, {\"timesteps\": 127, \"rewards\": -175.38606}, {\"timesteps\": 128, \"rewards\": -190.373383}, {\"timesteps\": 129, \"rewards\": -108.075098}, {\"timesteps\": 130, \"rewards\": -350.319512}, {\"timesteps\": 131, \"rewards\": -48.482466}, {\"timesteps\": 132, \"rewards\": -44.528682}, {\"timesteps\": 133, \"rewards\": -267.891925}, {\"timesteps\": 134, \"rewards\": -188.073067}, {\"timesteps\": 135, \"rewards\": -282.110962}, {\"timesteps\": 136, \"rewards\": -35.830784}, {\"timesteps\": 137, \"rewards\": -174.71101}, {\"timesteps\": 138, \"rewards\": -195.889581}, {\"timesteps\": 139, \"rewards\": -199.024267}, {\"timesteps\": 140, \"rewards\": -190.738162}, {\"timesteps\": 141, \"rewards\": -70.692479}, {\"timesteps\": 142, \"rewards\": -35.219165}, {\"timesteps\": 143, \"rewards\": -152.483013}, {\"timesteps\": 144, \"rewards\": -233.134782}, {\"timesteps\": 145, \"rewards\": -225.072443}, {\"timesteps\": 146, \"rewards\": -193.60548}, {\"timesteps\": 147, \"rewards\": -200.28868}, {\"timesteps\": 148, \"rewards\": -198.517674}, {\"timesteps\": 149, \"rewards\": -193.965122}, {\"timesteps\": 150, \"rewards\": -203.920306}, {\"timesteps\": 151, \"rewards\": -95.731159}, {\"timesteps\": 152, \"rewards\": -151.77828}, {\"timesteps\": 153, \"rewards\": -83.931769}, {\"timesteps\": 154, \"rewards\": -205.162162}, {\"timesteps\": 155, \"rewards\": -215.519488}, {\"timesteps\": 156, \"rewards\": -290.136401}, {\"timesteps\": 157, \"rewards\": -212.492807}, {\"timesteps\": 158, \"rewards\": -181.785589}, {\"timesteps\": 159, \"rewards\": -194.841973}, {\"timesteps\": 160, \"rewards\": -52.210536}, {\"timesteps\": 161, \"rewards\": -194.65085}, {\"timesteps\": 162, \"rewards\": -91.307566}, {\"timesteps\": 163, \"rewards\": -91.285962}, {\"timesteps\": 164, \"rewards\": -265.021148}, {\"timesteps\": 165, \"rewards\": -33.744255}, {\"timesteps\": 166, \"rewards\": -297.172854}, {\"timesteps\": 167, \"rewards\": -216.153194}, {\"timesteps\": 168, \"rewards\": -204.07557}, {\"timesteps\": 169, \"rewards\": -171.034796}, {\"timesteps\": 170, \"rewards\": -191.770152}, {\"timesteps\": 171, \"rewards\": -353.418634}, {\"timesteps\": 172, \"rewards\": -433.462137}, {\"timesteps\": 173, \"rewards\": -244.806282}, {\"timesteps\": 174, \"rewards\": -328.106556}, {\"timesteps\": 175, \"rewards\": -267.888982}, {\"timesteps\": 176, \"rewards\": -349.854723}, {\"timesteps\": 177, \"rewards\": -142.997354}, {\"timesteps\": 178, \"rewards\": 192.60224}, {\"timesteps\": 179, \"rewards\": -335.528965}, {\"timesteps\": 180, \"rewards\": -462.378863}, {\"timesteps\": 181, \"rewards\": -232.070223}, {\"timesteps\": 182, \"rewards\": -311.544528}, {\"timesteps\": 183, \"rewards\": -277.220255}, {\"timesteps\": 184, \"rewards\": -110.340368}, {\"timesteps\": 185, \"rewards\": -322.125204}, {\"timesteps\": 186, \"rewards\": -211.079849}, {\"timesteps\": 187, \"rewards\": -342.12078}, {\"timesteps\": 188, \"rewards\": -433.569125}, {\"timesteps\": 189, \"rewards\": -171.421136}, {\"timesteps\": 190, \"rewards\": -83.236057}, {\"timesteps\": 191, \"rewards\": -96.33631}, {\"timesteps\": 192, \"rewards\": -255.567374}, {\"timesteps\": 193, \"rewards\": -440.123026}, {\"timesteps\": 194, \"rewards\": -153.303723}, {\"timesteps\": 195, \"rewards\": -191.108657}, {\"timesteps\": 196, \"rewards\": -52.287738}, {\"timesteps\": 197, \"rewards\": -227.801205}, {\"timesteps\": 198, \"rewards\": -225.72561}, {\"timesteps\": 199, \"rewards\": -191.096054}, {\"timesteps\": 200, \"rewards\": -78.641626}, {\"timesteps\": 201, \"rewards\": 18.38764}, {\"timesteps\": 202, \"rewards\": -127.728273}, {\"timesteps\": 203, \"rewards\": -86.848907}, {\"timesteps\": 204, \"rewards\": -321.585028}, {\"timesteps\": 205, \"rewards\": -134.553455}, {\"timesteps\": 206, \"rewards\": -213.052232}, {\"timesteps\": 207, \"rewards\": -59.993464}, {\"timesteps\": 208, \"rewards\": -106.979618}, {\"timesteps\": 209, \"rewards\": -213.129587}, {\"timesteps\": 210, \"rewards\": -399.949978}, {\"timesteps\": 211, \"rewards\": -107.582207}, {\"timesteps\": 212, \"rewards\": -376.06036}, {\"timesteps\": 213, \"rewards\": -192.203686}, {\"timesteps\": 214, \"rewards\": -35.933822}, {\"timesteps\": 215, \"rewards\": -344.033103}, {\"timesteps\": 216, \"rewards\": -188.057806}, {\"timesteps\": 217, \"rewards\": -339.024813}, {\"timesteps\": 218, \"rewards\": -272.218359}, {\"timesteps\": 219, \"rewards\": -254.671171}, {\"timesteps\": 220, \"rewards\": -12.631901}, {\"timesteps\": 221, \"rewards\": -373.083881}, {\"timesteps\": 222, \"rewards\": -302.530816}, {\"timesteps\": 223, \"rewards\": -263.866388}, {\"timesteps\": 224, \"rewards\": -243.096711}, {\"timesteps\": 225, \"rewards\": -310.458002}, {\"timesteps\": 226, \"rewards\": -244.257407}, {\"timesteps\": 227, \"rewards\": -283.61679}, {\"timesteps\": 228, \"rewards\": -283.453551}, {\"timesteps\": 229, \"rewards\": 215.389298}, {\"timesteps\": 230, \"rewards\": -285.969487}, {\"timesteps\": 231, \"rewards\": -466.943034}, {\"timesteps\": 232, \"rewards\": -262.21501}, {\"timesteps\": 233, \"rewards\": -364.579133}, {\"timesteps\": 234, \"rewards\": -190.945741}, {\"timesteps\": 235, \"rewards\": -51.74399}, {\"timesteps\": 236, \"rewards\": -41.274533}, {\"timesteps\": 237, \"rewards\": -265.527578}, {\"timesteps\": 238, \"rewards\": -302.905496}, {\"timesteps\": 239, \"rewards\": -97.950534}, {\"timesteps\": 240, \"rewards\": -73.833329}, {\"timesteps\": 241, \"rewards\": -281.538465}, {\"timesteps\": 242, \"rewards\": -183.146782}, {\"timesteps\": 243, \"rewards\": -70.850415}, {\"timesteps\": 244, \"rewards\": 8.726767}, {\"timesteps\": 245, \"rewards\": -300.243352}, {\"timesteps\": 246, \"rewards\": -113.177616}, {\"timesteps\": 247, \"rewards\": -243.716983}, {\"timesteps\": 248, \"rewards\": -558.988227}, {\"timesteps\": 249, \"rewards\": -204.978243}, {\"timesteps\": 250, \"rewards\": -202.183278}, {\"timesteps\": 251, \"rewards\": -494.492479}, {\"timesteps\": 252, \"rewards\": -363.56068}, {\"timesteps\": 253, \"rewards\": -324.169399}, {\"timesteps\": 254, \"rewards\": -401.850935}, {\"timesteps\": 255, \"rewards\": -126.834502}, {\"timesteps\": 256, \"rewards\": -517.574334}, {\"timesteps\": 257, \"rewards\": -295.392335}, {\"timesteps\": 258, \"rewards\": -345.377527}, {\"timesteps\": 259, \"rewards\": -228.27577}, {\"timesteps\": 260, \"rewards\": -426.5114}, {\"timesteps\": 261, \"rewards\": -133.043703}, {\"timesteps\": 262, \"rewards\": -143.007129}, {\"timesteps\": 263, \"rewards\": -281.203136}, {\"timesteps\": 264, \"rewards\": -300.04448}, {\"timesteps\": 265, \"rewards\": -492.641577}, {\"timesteps\": 266, \"rewards\": -83.382133}, {\"timesteps\": 267, \"rewards\": -307.012276}, {\"timesteps\": 268, \"rewards\": -337.417016}, {\"timesteps\": 269, \"rewards\": -83.87156}, {\"timesteps\": 270, \"rewards\": -523.289498}, {\"timesteps\": 271, \"rewards\": -80.697728}, {\"timesteps\": 272, \"rewards\": -80.42466}, {\"timesteps\": 273, \"rewards\": -212.673432}, {\"timesteps\": 274, \"rewards\": -327.263921}, {\"timesteps\": 275, \"rewards\": -33.112086}, {\"timesteps\": 276, \"rewards\": -81.44028}, {\"timesteps\": 277, \"rewards\": -159.82848}, {\"timesteps\": 278, \"rewards\": -307.569777}, {\"timesteps\": 279, \"rewards\": -288.288912}, {\"timesteps\": 280, \"rewards\": -281.634365}, {\"timesteps\": 281, \"rewards\": -55.556868}, {\"timesteps\": 282, \"rewards\": -195.110462}, {\"timesteps\": 283, \"rewards\": -529.495063}, {\"timesteps\": 284, \"rewards\": -275.723337}, {\"timesteps\": 285, \"rewards\": -95.878353}, {\"timesteps\": 286, \"rewards\": -114.323521}, {\"timesteps\": 287, \"rewards\": -113.390288}, {\"timesteps\": 288, \"rewards\": -480.251198}, {\"timesteps\": 289, \"rewards\": -102.810317}, {\"timesteps\": 290, \"rewards\": -481.840744}, {\"timesteps\": 291, \"rewards\": -224.994275}, {\"timesteps\": 292, \"rewards\": -404.980374}, {\"timesteps\": 293, \"rewards\": -298.813834}, {\"timesteps\": 294, \"rewards\": -354.318611}, {\"timesteps\": 295, \"rewards\": -275.413525}, {\"timesteps\": 296, \"rewards\": -182.46359}, {\"timesteps\": 297, \"rewards\": -373.214256}, {\"timesteps\": 298, \"rewards\": -301.555547}, {\"timesteps\": 299, \"rewards\": -510.114986}, {\"timesteps\": 300, \"rewards\": -143.109834}, {\"timesteps\": 301, \"rewards\": -218.401075}, {\"timesteps\": 302, \"rewards\": -199.145092}, {\"timesteps\": 303, \"rewards\": -438.543209}, {\"timesteps\": 304, \"rewards\": -238.987067}, {\"timesteps\": 305, \"rewards\": -333.706992}, {\"timesteps\": 306, \"rewards\": -157.364131}, {\"timesteps\": 307, \"rewards\": -325.225533}, {\"timesteps\": 308, \"rewards\": -398.589697}, {\"timesteps\": 309, \"rewards\": -219.454726}, {\"timesteps\": 310, \"rewards\": -126.87161}, {\"timesteps\": 311, \"rewards\": -24.842939}, {\"timesteps\": 312, \"rewards\": -297.764533}, {\"timesteps\": 313, \"rewards\": -335.853342}, {\"timesteps\": 314, \"rewards\": -294.319681}, {\"timesteps\": 315, \"rewards\": -286.843601}, {\"timesteps\": 316, \"rewards\": -311.914573}, {\"timesteps\": 317, \"rewards\": -274.849348}, {\"timesteps\": 318, \"rewards\": -303.952637}, {\"timesteps\": 319, \"rewards\": -225.50856}, {\"timesteps\": 320, \"rewards\": -293.865675}, {\"timesteps\": 321, \"rewards\": -368.03647}, {\"timesteps\": 322, \"rewards\": -203.172225}, {\"timesteps\": 323, \"rewards\": -179.309338}, {\"timesteps\": 324, \"rewards\": -448.701465}, {\"timesteps\": 325, \"rewards\": -56.904916}, {\"timesteps\": 326, \"rewards\": -179.482207}, {\"timesteps\": 327, \"rewards\": -274.392175}, {\"timesteps\": 328, \"rewards\": -86.575526}, {\"timesteps\": 329, \"rewards\": -142.557339}, {\"timesteps\": 330, \"rewards\": -444.201932}, {\"timesteps\": 331, \"rewards\": -446.826385}, {\"timesteps\": 332, \"rewards\": -136.908163}, {\"timesteps\": 333, \"rewards\": -365.208362}, {\"timesteps\": 334, \"rewards\": -178.273683}, {\"timesteps\": 335, \"rewards\": -70.087514}, {\"timesteps\": 336, \"rewards\": -150.129714}, {\"timesteps\": 337, \"rewards\": 177.996001}, {\"timesteps\": 338, \"rewards\": -311.811072}, {\"timesteps\": 339, \"rewards\": -111.605571}, {\"timesteps\": 340, \"rewards\": -202.58916}, {\"timesteps\": 341, \"rewards\": -116.590149}, {\"timesteps\": 342, \"rewards\": -250.324459}, {\"timesteps\": 343, \"rewards\": -527.956345}, {\"timesteps\": 344, \"rewards\": -142.942443}, {\"timesteps\": 345, \"rewards\": -169.83201}, {\"timesteps\": 346, \"rewards\": 211.26837}, {\"timesteps\": 347, \"rewards\": -301.860339}, {\"timesteps\": 348, \"rewards\": -62.125036}, {\"timesteps\": 349, \"rewards\": -54.277768}, {\"timesteps\": 350, \"rewards\": -103.778759}, {\"timesteps\": 351, \"rewards\": -105.862152}, {\"timesteps\": 352, \"rewards\": -228.000729}, {\"timesteps\": 353, \"rewards\": -95.727816}, {\"timesteps\": 354, \"rewards\": -133.238296}, {\"timesteps\": 355, \"rewards\": -119.053909}, {\"timesteps\": 356, \"rewards\": -58.604526}, {\"timesteps\": 357, \"rewards\": -9.797868}, {\"timesteps\": 358, \"rewards\": -214.225232}, {\"timesteps\": 359, \"rewards\": -68.965477}, {\"timesteps\": 360, \"rewards\": -87.064023}, {\"timesteps\": 361, \"rewards\": -34.050407}, {\"timesteps\": 362, \"rewards\": -76.975633}, {\"timesteps\": 363, \"rewards\": 216.651293}, {\"timesteps\": 364, \"rewards\": -62.090995}, {\"timesteps\": 365, \"rewards\": -231.04349}, {\"timesteps\": 366, \"rewards\": -183.837953}, {\"timesteps\": 367, \"rewards\": -58.060312}, {\"timesteps\": 368, \"rewards\": -248.627808}, {\"timesteps\": 369, \"rewards\": -193.456808}, {\"timesteps\": 370, \"rewards\": -279.03213}, {\"timesteps\": 371, \"rewards\": -504.954034}, {\"timesteps\": 372, \"rewards\": -266.225029}, {\"timesteps\": 373, \"rewards\": -163.867807}, {\"timesteps\": 374, \"rewards\": -293.461734}, {\"timesteps\": 375, \"rewards\": -182.8631}, {\"timesteps\": 376, \"rewards\": -116.416033}, {\"timesteps\": 377, \"rewards\": -98.620289}, {\"timesteps\": 378, \"rewards\": -295.827017}, {\"timesteps\": 379, \"rewards\": -72.294105}, {\"timesteps\": 380, \"rewards\": -139.263883}, {\"timesteps\": 381, \"rewards\": -184.095731}, {\"timesteps\": 382, \"rewards\": -133.064415}, {\"timesteps\": 383, \"rewards\": -254.928868}, {\"timesteps\": 384, \"rewards\": -307.128805}, {\"timesteps\": 385, \"rewards\": -80.305989}, {\"timesteps\": 386, \"rewards\": -49.704442}, {\"timesteps\": 387, \"rewards\": -335.279015}, {\"timesteps\": 388, \"rewards\": -99.656843}, {\"timesteps\": 389, \"rewards\": -174.916319}, {\"timesteps\": 390, \"rewards\": -195.401351}, {\"timesteps\": 391, \"rewards\": -208.118477}, {\"timesteps\": 392, \"rewards\": -192.624118}, {\"timesteps\": 393, \"rewards\": -56.429538}, {\"timesteps\": 394, \"rewards\": -295.237898}, {\"timesteps\": 395, \"rewards\": -76.008264}, {\"timesteps\": 396, \"rewards\": -45.367085}, {\"timesteps\": 397, \"rewards\": -218.041648}, {\"timesteps\": 398, \"rewards\": -106.182438}, {\"timesteps\": 399, \"rewards\": -435.175906}, {\"timesteps\": 400, \"rewards\": -267.870858}, {\"timesteps\": 401, \"rewards\": -118.280555}, {\"timesteps\": 402, \"rewards\": -199.66374}, {\"timesteps\": 403, \"rewards\": -42.053651}, {\"timesteps\": 404, \"rewards\": -246.885354}, {\"timesteps\": 405, \"rewards\": -231.698384}, {\"timesteps\": 406, \"rewards\": -105.347733}, {\"timesteps\": 407, \"rewards\": -118.850196}, {\"timesteps\": 408, \"rewards\": -506.484139}, {\"timesteps\": 409, \"rewards\": -193.637235}, {\"timesteps\": 410, \"rewards\": -140.963625}, {\"timesteps\": 411, \"rewards\": -339.134517}, {\"timesteps\": 412, \"rewards\": -432.418544}, {\"timesteps\": 413, \"rewards\": -248.938827}, {\"timesteps\": 414, \"rewards\": -436.000111}, {\"timesteps\": 415, \"rewards\": -464.644684}, {\"timesteps\": 416, \"rewards\": -112.230854}, {\"timesteps\": 417, \"rewards\": -164.915215}, {\"timesteps\": 418, \"rewards\": -446.390219}, {\"timesteps\": 419, \"rewards\": -105.957688}, {\"timesteps\": 420, \"rewards\": -486.990484}, {\"timesteps\": 421, \"rewards\": -127.876037}, {\"timesteps\": 422, \"rewards\": -357.38805}, {\"timesteps\": 423, \"rewards\": -244.027514}, {\"timesteps\": 424, \"rewards\": -136.502392}, {\"timesteps\": 425, \"rewards\": -229.971384}, {\"timesteps\": 426, \"rewards\": -83.991732}, {\"timesteps\": 427, \"rewards\": -135.449176}, {\"timesteps\": 428, \"rewards\": -425.600981}, {\"timesteps\": 429, \"rewards\": -238.567839}, {\"timesteps\": 430, \"rewards\": -516.847249}, {\"timesteps\": 431, \"rewards\": -244.525923}, {\"timesteps\": 432, \"rewards\": -255.072034}, {\"timesteps\": 433, \"rewards\": -76.992676}, {\"timesteps\": 434, \"rewards\": -339.676751}, {\"timesteps\": 435, \"rewards\": -83.937449}, {\"timesteps\": 436, \"rewards\": -172.331019}, {\"timesteps\": 437, \"rewards\": -174.105102}, {\"timesteps\": 438, \"rewards\": 238.505249}, {\"timesteps\": 439, \"rewards\": -281.454746}, {\"timesteps\": 440, \"rewards\": -184.532378}, {\"timesteps\": 441, \"rewards\": -116.348341}, {\"timesteps\": 442, \"rewards\": -462.448076}, {\"timesteps\": 443, \"rewards\": -334.891349}, {\"timesteps\": 444, \"rewards\": -329.785314}, {\"timesteps\": 445, \"rewards\": -171.203621}, {\"timesteps\": 446, \"rewards\": -306.432473}, {\"timesteps\": 447, \"rewards\": -153.78117}, {\"timesteps\": 448, \"rewards\": -280.751486}, {\"timesteps\": 449, \"rewards\": -298.121884}, {\"timesteps\": 450, \"rewards\": -233.755134}, {\"timesteps\": 451, \"rewards\": -295.944553}, {\"timesteps\": 452, \"rewards\": -219.058858}, {\"timesteps\": 453, \"rewards\": -230.315829}, {\"timesteps\": 454, \"rewards\": -316.743491}, {\"timesteps\": 455, \"rewards\": -224.914401}, {\"timesteps\": 456, \"rewards\": -257.814082}, {\"timesteps\": 457, \"rewards\": -185.76233}, {\"timesteps\": 458, \"rewards\": -187.700867}, {\"timesteps\": 459, \"rewards\": -176.559412}, {\"timesteps\": 460, \"rewards\": -145.69851}, {\"timesteps\": 461, \"rewards\": -169.494875}, {\"timesteps\": 462, \"rewards\": -445.113673}, {\"timesteps\": 463, \"rewards\": -129.718189}, {\"timesteps\": 464, \"rewards\": -296.297546}, {\"timesteps\": 465, \"rewards\": -153.094698}, {\"timesteps\": 466, \"rewards\": -251.072769}, {\"timesteps\": 467, \"rewards\": -184.462669}, {\"timesteps\": 468, \"rewards\": -203.738615}, {\"timesteps\": 469, \"rewards\": -265.450423}, {\"timesteps\": 470, \"rewards\": -252.291118}, {\"timesteps\": 471, \"rewards\": -373.086448}, {\"timesteps\": 472, \"rewards\": -199.978476}, {\"timesteps\": 473, \"rewards\": -348.690967}, {\"timesteps\": 474, \"rewards\": -97.809607}, {\"timesteps\": 475, \"rewards\": -309.764294}, {\"timesteps\": 476, \"rewards\": -106.637915}, {\"timesteps\": 477, \"rewards\": -144.377697}, {\"timesteps\": 478, \"rewards\": -208.370163}, {\"timesteps\": 479, \"rewards\": 20.165891}, {\"timesteps\": 480, \"rewards\": -260.153689}, {\"timesteps\": 481, \"rewards\": -240.087342}, {\"timesteps\": 482, \"rewards\": -239.829127}, {\"timesteps\": 483, \"rewards\": -268.176765}, {\"timesteps\": 484, \"rewards\": 122.407796}, {\"timesteps\": 485, \"rewards\": -218.542808}, {\"timesteps\": 486, \"rewards\": -205.418272}, {\"timesteps\": 487, \"rewards\": -294.35845}, {\"timesteps\": 488, \"rewards\": -263.376496}, {\"timesteps\": 489, \"rewards\": -316.217536}, {\"timesteps\": 490, \"rewards\": -219.017287}, {\"timesteps\": 491, \"rewards\": -214.771907}, {\"timesteps\": 492, \"rewards\": -244.922347}, {\"timesteps\": 493, \"rewards\": -294.605054}, {\"timesteps\": 494, \"rewards\": -247.909435}, {\"timesteps\": 495, \"rewards\": -257.084166}, {\"timesteps\": 496, \"rewards\": -235.840376}, {\"timesteps\": 497, \"rewards\": -313.911661}, {\"timesteps\": 498, \"rewards\": -156.128805}, {\"timesteps\": 499, \"rewards\": -306.643539}, {\"timesteps\": 500, \"rewards\": -351.618607}, {\"timesteps\": 501, \"rewards\": -286.511257}, {\"timesteps\": 502, \"rewards\": -366.73495}, {\"timesteps\": 503, \"rewards\": -271.111444}, {\"timesteps\": 504, \"rewards\": -244.304698}, {\"timesteps\": 505, \"rewards\": -437.954556}, {\"timesteps\": 506, \"rewards\": -140.956515}, {\"timesteps\": 507, \"rewards\": -249.2526}, {\"timesteps\": 508, \"rewards\": -251.247067}, {\"timesteps\": 509, \"rewards\": -269.583964}, {\"timesteps\": 510, \"rewards\": -466.271686}, {\"timesteps\": 511, \"rewards\": -463.206732}, {\"timesteps\": 512, \"rewards\": -195.672796}, {\"timesteps\": 513, \"rewards\": -418.650285}, {\"timesteps\": 514, \"rewards\": -2.342187}, {\"timesteps\": 515, \"rewards\": -218.042766}, {\"timesteps\": 516, \"rewards\": -226.017994}, {\"timesteps\": 517, \"rewards\": -218.450016}, {\"timesteps\": 518, \"rewards\": -715.686201}, {\"timesteps\": 519, \"rewards\": -345.779693}, {\"timesteps\": 520, \"rewards\": -223.189394}, {\"timesteps\": 521, \"rewards\": -506.490221}, {\"timesteps\": 522, \"rewards\": -348.997231}, {\"timesteps\": 523, \"rewards\": -181.951094}, {\"timesteps\": 524, \"rewards\": -276.323456}, {\"timesteps\": 525, \"rewards\": -536.229603}, {\"timesteps\": 526, \"rewards\": -192.215811}, {\"timesteps\": 527, \"rewards\": -326.417422}, {\"timesteps\": 528, \"rewards\": -284.13503}, {\"timesteps\": 529, \"rewards\": -321.778187}, {\"timesteps\": 530, \"rewards\": -370.715378}, {\"timesteps\": 531, \"rewards\": -75.932681}, {\"timesteps\": 532, \"rewards\": -292.441971}, {\"timesteps\": 533, \"rewards\": -421.094021}, {\"timesteps\": 534, \"rewards\": -465.991263}, {\"timesteps\": 535, \"rewards\": -435.926217}, {\"timesteps\": 536, \"rewards\": -221.326829}, {\"timesteps\": 537, \"rewards\": -159.901249}, {\"timesteps\": 538, \"rewards\": -238.365997}, {\"timesteps\": 539, \"rewards\": -483.080462}, {\"timesteps\": 540, \"rewards\": -393.07056}, {\"timesteps\": 541, \"rewards\": -222.702068}, {\"timesteps\": 542, \"rewards\": -447.120212}, {\"timesteps\": 543, \"rewards\": -443.85447}, {\"timesteps\": 544, \"rewards\": -469.425854}, {\"timesteps\": 545, \"rewards\": -286.40838}, {\"timesteps\": 546, \"rewards\": -148.565572}, {\"timesteps\": 547, \"rewards\": -480.954992}, {\"timesteps\": 548, \"rewards\": -334.479773}, {\"timesteps\": 549, \"rewards\": -275.499275}, {\"timesteps\": 550, \"rewards\": -270.814082}, {\"timesteps\": 551, \"rewards\": -251.775345}, {\"timesteps\": 552, \"rewards\": -540.12076}, {\"timesteps\": 553, \"rewards\": -261.616982}, {\"timesteps\": 554, \"rewards\": -327.371838}, {\"timesteps\": 555, \"rewards\": -313.739551}, {\"timesteps\": 556, \"rewards\": -345.132788}, {\"timesteps\": 557, \"rewards\": -243.78605}, {\"timesteps\": 558, \"rewards\": -47.491882}, {\"timesteps\": 559, \"rewards\": -545.675976}, {\"timesteps\": 560, \"rewards\": -454.067607}, {\"timesteps\": 561, \"rewards\": -431.924356}, {\"timesteps\": 562, \"rewards\": -379.173283}, {\"timesteps\": 563, \"rewards\": -576.023632}, {\"timesteps\": 564, \"rewards\": -358.334038}, {\"timesteps\": 565, \"rewards\": -692.483224}, {\"timesteps\": 566, \"rewards\": -478.511072}, {\"timesteps\": 567, \"rewards\": -269.774325}, {\"timesteps\": 568, \"rewards\": -353.046583}, {\"timesteps\": 569, \"rewards\": -470.739787}, {\"timesteps\": 570, \"rewards\": -111.971202}, {\"timesteps\": 571, \"rewards\": -393.910355}, {\"timesteps\": 572, \"rewards\": -375.217759}, {\"timesteps\": 573, \"rewards\": -421.423534}, {\"timesteps\": 574, \"rewards\": -478.278811}, {\"timesteps\": 575, \"rewards\": -579.243329}, {\"timesteps\": 576, \"rewards\": -196.520243}, {\"timesteps\": 577, \"rewards\": -256.249254}, {\"timesteps\": 578, \"rewards\": -255.73736}, {\"timesteps\": 579, \"rewards\": -392.392226}, {\"timesteps\": 580, \"rewards\": -375.032512}, {\"timesteps\": 581, \"rewards\": -291.746381}, {\"timesteps\": 582, \"rewards\": -473.870283}, {\"timesteps\": 583, \"rewards\": -498.945151}, {\"timesteps\": 584, \"rewards\": -227.306841}, {\"timesteps\": 585, \"rewards\": -341.203809}, {\"timesteps\": 586, \"rewards\": -318.690223}, {\"timesteps\": 587, \"rewards\": -33.848195}, {\"timesteps\": 588, \"rewards\": -307.799123}, {\"timesteps\": 589, \"rewards\": -65.129952}, {\"timesteps\": 590, \"rewards\": -101.023207}, {\"timesteps\": 591, \"rewards\": -200.782651}, {\"timesteps\": 592, \"rewards\": -247.390005}, {\"timesteps\": 593, \"rewards\": -278.933895}, {\"timesteps\": 594, \"rewards\": -351.234292}, {\"timesteps\": 595, \"rewards\": -215.432011}, {\"timesteps\": 596, \"rewards\": -232.30805}, {\"timesteps\": 597, \"rewards\": -227.972748}, {\"timesteps\": 598, \"rewards\": -200.254643}, {\"timesteps\": 599, \"rewards\": -310.439679}, {\"timesteps\": 600, \"rewards\": -111.363413}, {\"timesteps\": 601, \"rewards\": -129.68055}, {\"timesteps\": 602, \"rewards\": -161.336713}, {\"timesteps\": 603, \"rewards\": -195.345299}, {\"timesteps\": 604, \"rewards\": -297.360717}, {\"timesteps\": 605, \"rewards\": -115.803511}, {\"timesteps\": 606, \"rewards\": -152.509174}, {\"timesteps\": 607, \"rewards\": -255.720049}, {\"timesteps\": 608, \"rewards\": -257.290296}, {\"timesteps\": 609, \"rewards\": -93.709127}, {\"timesteps\": 610, \"rewards\": -247.252209}, {\"timesteps\": 611, \"rewards\": -148.992645}, {\"timesteps\": 612, \"rewards\": -31.082338}, {\"timesteps\": 613, \"rewards\": -228.748109}, {\"timesteps\": 614, \"rewards\": -102.640871}, {\"timesteps\": 615, \"rewards\": -253.063596}, {\"timesteps\": 616, \"rewards\": -104.868803}, {\"timesteps\": 617, \"rewards\": 173.461404}, {\"timesteps\": 618, \"rewards\": 5.494159}, {\"timesteps\": 619, \"rewards\": -75.89682}, {\"timesteps\": 620, \"rewards\": -114.917174}, {\"timesteps\": 621, \"rewards\": -141.766428}, {\"timesteps\": 622, \"rewards\": -178.820213}, {\"timesteps\": 623, \"rewards\": -9.14301}, {\"timesteps\": 624, \"rewards\": -191.391559}, {\"timesteps\": 625, \"rewards\": -247.818592}, {\"timesteps\": 626, \"rewards\": -276.832026}, {\"timesteps\": 627, \"rewards\": -13.588171}, {\"timesteps\": 628, \"rewards\": -201.433395}, {\"timesteps\": 629, \"rewards\": -330.799934}, {\"timesteps\": 630, \"rewards\": -162.061047}, {\"timesteps\": 631, \"rewards\": -230.487576}, {\"timesteps\": 632, \"rewards\": -247.626484}, {\"timesteps\": 633, \"rewards\": -131.549475}, {\"timesteps\": 634, \"rewards\": -264.130255}, {\"timesteps\": 635, \"rewards\": -323.652609}, {\"timesteps\": 636, \"rewards\": -29.318006}, {\"timesteps\": 637, \"rewards\": -110.479894}, {\"timesteps\": 638, \"rewards\": -249.829599}, {\"timesteps\": 639, \"rewards\": -111.771644}, {\"timesteps\": 640, \"rewards\": -233.996291}, {\"timesteps\": 641, \"rewards\": -160.412141}, {\"timesteps\": 642, \"rewards\": 0.148813}, {\"timesteps\": 643, \"rewards\": -155.870445}, {\"timesteps\": 644, \"rewards\": -183.867335}, {\"timesteps\": 645, \"rewards\": -143.808579}, {\"timesteps\": 646, \"rewards\": -339.10238}, {\"timesteps\": 647, \"rewards\": -213.88717}, {\"timesteps\": 648, \"rewards\": -10.651478}, {\"timesteps\": 649, \"rewards\": -61.143529}, {\"timesteps\": 650, \"rewards\": -157.581688}, {\"timesteps\": 651, \"rewards\": -246.584656}, {\"timesteps\": 652, \"rewards\": -247.590666}, {\"timesteps\": 653, \"rewards\": -225.179347}, {\"timesteps\": 654, \"rewards\": -259.58426}, {\"timesteps\": 655, \"rewards\": -231.413413}, {\"timesteps\": 656, \"rewards\": -175.461391}, {\"timesteps\": 657, \"rewards\": -195.500032}, {\"timesteps\": 658, \"rewards\": -343.74565}, {\"timesteps\": 659, \"rewards\": -190.273132}, {\"timesteps\": 660, \"rewards\": -160.881725}, {\"timesteps\": 661, \"rewards\": -175.894369}, {\"timesteps\": 662, \"rewards\": -257.253946}, {\"timesteps\": 663, \"rewards\": -225.591087}, {\"timesteps\": 664, \"rewards\": -219.85944}, {\"timesteps\": 665, \"rewards\": -72.048877}, {\"timesteps\": 666, \"rewards\": -177.461083}, {\"timesteps\": 667, \"rewards\": -135.145808}, {\"timesteps\": 668, \"rewards\": -214.288048}, {\"timesteps\": 669, \"rewards\": -22.02168}, {\"timesteps\": 670, \"rewards\": -67.985751}, {\"timesteps\": 671, \"rewards\": -195.433077}, {\"timesteps\": 672, \"rewards\": -158.491231}, {\"timesteps\": 673, \"rewards\": -222.471345}, {\"timesteps\": 674, \"rewards\": -108.800295}, {\"timesteps\": 675, \"rewards\": -194.8561}, {\"timesteps\": 676, \"rewards\": -27.365093}, {\"timesteps\": 677, \"rewards\": -209.345031}, {\"timesteps\": 678, \"rewards\": 164.258072}, {\"timesteps\": 679, \"rewards\": -132.555471}, {\"timesteps\": 680, \"rewards\": -180.336549}, {\"timesteps\": 681, \"rewards\": -70.895813}, {\"timesteps\": 682, \"rewards\": -202.218049}, {\"timesteps\": 683, \"rewards\": -248.015999}, {\"timesteps\": 684, \"rewards\": 149.802534}, {\"timesteps\": 685, \"rewards\": -125.693634}, {\"timesteps\": 686, \"rewards\": -139.779067}, {\"timesteps\": 687, \"rewards\": 253.805138}, {\"timesteps\": 688, \"rewards\": -186.248476}, {\"timesteps\": 689, \"rewards\": -184.021386}, {\"timesteps\": 690, \"rewards\": -62.964619}, {\"timesteps\": 691, \"rewards\": -60.237024}, {\"timesteps\": 692, \"rewards\": -206.64351}, {\"timesteps\": 693, \"rewards\": -34.524727}, {\"timesteps\": 694, \"rewards\": 167.215838}, {\"timesteps\": 695, \"rewards\": -158.856108}, {\"timesteps\": 696, \"rewards\": -71.971446}, {\"timesteps\": 697, \"rewards\": -82.60031}, {\"timesteps\": 698, \"rewards\": -293.075408}, {\"timesteps\": 699, \"rewards\": 6.44919}, {\"timesteps\": 700, \"rewards\": -75.643458}, {\"timesteps\": 701, \"rewards\": -39.275423}, {\"timesteps\": 702, \"rewards\": -210.424751}, {\"timesteps\": 703, \"rewards\": -185.080109}, {\"timesteps\": 704, \"rewards\": -80.156292}, {\"timesteps\": 705, \"rewards\": -171.707352}, {\"timesteps\": 706, \"rewards\": -8.923482}, {\"timesteps\": 707, \"rewards\": 184.184052}, {\"timesteps\": 708, \"rewards\": -178.595614}, {\"timesteps\": 709, \"rewards\": -73.478557}, {\"timesteps\": 710, \"rewards\": 49.310453}, {\"timesteps\": 711, \"rewards\": -55.202767}, {\"timesteps\": 712, \"rewards\": 207.584585}, {\"timesteps\": 713, \"rewards\": -115.819459}, {\"timesteps\": 714, \"rewards\": -108.936689}, {\"timesteps\": 715, \"rewards\": -140.890117}, {\"timesteps\": 716, \"rewards\": -218.090416}, {\"timesteps\": 717, \"rewards\": -55.769802}, {\"timesteps\": 718, \"rewards\": 208.412458}, {\"timesteps\": 719, \"rewards\": -212.716465}, {\"timesteps\": 720, \"rewards\": -178.95725}, {\"timesteps\": 721, \"rewards\": -160.165208}, {\"timesteps\": 722, \"rewards\": 134.851741}, {\"timesteps\": 723, \"rewards\": -74.048636}, {\"timesteps\": 724, \"rewards\": 128.69137}, {\"timesteps\": 725, \"rewards\": -175.967802}, {\"timesteps\": 726, \"rewards\": -277.229081}, {\"timesteps\": 727, \"rewards\": -268.025948}, {\"timesteps\": 728, \"rewards\": -32.867605}, {\"timesteps\": 729, \"rewards\": -30.1046}, {\"timesteps\": 730, \"rewards\": -182.312553}, {\"timesteps\": 731, \"rewards\": -32.376817}, {\"timesteps\": 732, \"rewards\": -253.202515}, {\"timesteps\": 733, \"rewards\": -22.567356}, {\"timesteps\": 734, \"rewards\": 100.981182}, {\"timesteps\": 735, \"rewards\": -196.528259}, {\"timesteps\": 736, \"rewards\": -190.243863}, {\"timesteps\": 737, \"rewards\": 207.969058}, {\"timesteps\": 738, \"rewards\": -215.41402}, {\"timesteps\": 739, \"rewards\": -0.287995}, {\"timesteps\": 740, \"rewards\": -220.858517}, {\"timesteps\": 741, \"rewards\": 143.770044}, {\"timesteps\": 742, \"rewards\": -241.714888}, {\"timesteps\": 743, \"rewards\": -88.912814}, {\"timesteps\": 744, \"rewards\": 191.716136}, {\"timesteps\": 745, \"rewards\": 207.603125}, {\"timesteps\": 746, \"rewards\": -48.157425}, {\"timesteps\": 747, \"rewards\": 175.965979}, {\"timesteps\": 748, \"rewards\": 115.766029}, {\"timesteps\": 749, \"rewards\": -37.76535}, {\"timesteps\": 750, \"rewards\": -10.100858}, {\"timesteps\": 751, \"rewards\": -229.626139}, {\"timesteps\": 752, \"rewards\": -143.844919}, {\"timesteps\": 753, \"rewards\": -40.333867}, {\"timesteps\": 754, \"rewards\": -128.722241}, {\"timesteps\": 755, \"rewards\": -232.51961}, {\"timesteps\": 756, \"rewards\": -43.381307}, {\"timesteps\": 757, \"rewards\": -95.031779}, {\"timesteps\": 758, \"rewards\": -311.005443}, {\"timesteps\": 759, \"rewards\": -263.134044}, {\"timesteps\": 760, \"rewards\": -74.284543}, {\"timesteps\": 761, \"rewards\": -253.126424}, {\"timesteps\": 762, \"rewards\": -87.941269}, {\"timesteps\": 763, \"rewards\": 129.91609}, {\"timesteps\": 764, \"rewards\": -215.291849}, {\"timesteps\": 765, \"rewards\": -111.578457}, {\"timesteps\": 766, \"rewards\": -82.077286}, {\"timesteps\": 767, \"rewards\": -257.21611}, {\"timesteps\": 768, \"rewards\": -286.742602}, {\"timesteps\": 769, \"rewards\": 159.742237}, {\"timesteps\": 770, \"rewards\": -97.814645}, {\"timesteps\": 771, \"rewards\": -268.679389}, {\"timesteps\": 772, \"rewards\": -259.615488}, {\"timesteps\": 773, \"rewards\": -296.379032}, {\"timesteps\": 774, \"rewards\": -311.302861}, {\"timesteps\": 775, \"rewards\": -250.200071}, {\"timesteps\": 776, \"rewards\": -314.9743}, {\"timesteps\": 777, \"rewards\": 138.863632}, {\"timesteps\": 778, \"rewards\": -88.626228}, {\"timesteps\": 779, \"rewards\": -217.019033}, {\"timesteps\": 780, \"rewards\": -179.335379}, {\"timesteps\": 781, \"rewards\": -173.957371}, {\"timesteps\": 782, \"rewards\": -224.889385}, {\"timesteps\": 783, \"rewards\": -11.961422}, {\"timesteps\": 784, \"rewards\": -222.468467}, {\"timesteps\": 785, \"rewards\": -169.001678}, {\"timesteps\": 786, \"rewards\": 163.701846}, {\"timesteps\": 787, \"rewards\": -308.800131}, {\"timesteps\": 788, \"rewards\": -245.908855}, {\"timesteps\": 789, \"rewards\": 75.301764}, {\"timesteps\": 790, \"rewards\": -36.155514}, {\"timesteps\": 791, \"rewards\": -204.280376}, {\"timesteps\": 792, \"rewards\": -269.741549}, {\"timesteps\": 793, \"rewards\": -191.196426}, {\"timesteps\": 794, \"rewards\": -319.991516}, {\"timesteps\": 795, \"rewards\": -185.115968}, {\"timesteps\": 796, \"rewards\": -78.599323}, {\"timesteps\": 797, \"rewards\": -57.421789}, {\"timesteps\": 798, \"rewards\": -339.29297}, {\"timesteps\": 799, \"rewards\": -277.264437}, {\"timesteps\": 800, \"rewards\": -208.821291}, {\"timesteps\": 801, \"rewards\": -14.825092}, {\"timesteps\": 802, \"rewards\": -116.841298}, {\"timesteps\": 803, \"rewards\": -205.027985}, {\"timesteps\": 804, \"rewards\": -138.686133}, {\"timesteps\": 805, \"rewards\": -250.375802}, {\"timesteps\": 806, \"rewards\": -212.429399}, {\"timesteps\": 807, \"rewards\": -59.146074}, {\"timesteps\": 808, \"rewards\": 178.979514}, {\"timesteps\": 809, \"rewards\": 136.841809}, {\"timesteps\": 810, \"rewards\": -123.276258}, {\"timesteps\": 811, \"rewards\": -51.521528}, {\"timesteps\": 812, \"rewards\": 236.561879}, {\"timesteps\": 813, \"rewards\": -399.330729}, {\"timesteps\": 814, \"rewards\": -280.749355}, {\"timesteps\": 815, \"rewards\": -281.13446}, {\"timesteps\": 816, \"rewards\": -290.779614}, {\"timesteps\": 817, \"rewards\": 173.021118}, {\"timesteps\": 818, \"rewards\": -299.180092}, {\"timesteps\": 819, \"rewards\": -111.398961}, {\"timesteps\": 820, \"rewards\": -76.821641}, {\"timesteps\": 821, \"rewards\": 129.6484}, {\"timesteps\": 822, \"rewards\": 138.10855}, {\"timesteps\": 823, \"rewards\": 188.507942}, {\"timesteps\": 824, \"rewards\": 93.166959}, {\"timesteps\": 825, \"rewards\": -179.853769}, {\"timesteps\": 826, \"rewards\": -200.31776}, {\"timesteps\": 827, \"rewards\": 187.121119}, {\"timesteps\": 828, \"rewards\": 110.722412}, {\"timesteps\": 829, \"rewards\": -108.591746}, {\"timesteps\": 830, \"rewards\": -85.483142}, {\"timesteps\": 831, \"rewards\": -91.328525}, {\"timesteps\": 832, \"rewards\": 116.140738}, {\"timesteps\": 833, \"rewards\": 130.161822}, {\"timesteps\": 834, \"rewards\": -71.096452}, {\"timesteps\": 835, \"rewards\": 1.957959}, {\"timesteps\": 836, \"rewards\": -186.123955}, {\"timesteps\": 837, \"rewards\": 5.540189}, {\"timesteps\": 838, \"rewards\": -25.210983}, {\"timesteps\": 839, \"rewards\": -182.914696}, {\"timesteps\": 840, \"rewards\": -129.618792}, {\"timesteps\": 841, \"rewards\": -45.911937}, {\"timesteps\": 842, \"rewards\": -117.953796}, {\"timesteps\": 843, \"rewards\": -196.509358}, {\"timesteps\": 844, \"rewards\": -296.184711}, {\"timesteps\": 845, \"rewards\": -212.429249}, {\"timesteps\": 846, \"rewards\": -204.222372}, {\"timesteps\": 847, \"rewards\": -207.324677}, {\"timesteps\": 848, \"rewards\": 110.298301}, {\"timesteps\": 849, \"rewards\": -221.83363}, {\"timesteps\": 850, \"rewards\": -95.599956}, {\"timesteps\": 851, \"rewards\": -164.199964}, {\"timesteps\": 852, \"rewards\": 120.403657}, {\"timesteps\": 853, \"rewards\": 133.812967}, {\"timesteps\": 854, \"rewards\": -227.608693}, {\"timesteps\": 855, \"rewards\": -29.519851}, {\"timesteps\": 856, \"rewards\": -77.841256}, {\"timesteps\": 857, \"rewards\": -70.061079}, {\"timesteps\": 858, \"rewards\": 159.176129}, {\"timesteps\": 859, \"rewards\": -66.238577}, {\"timesteps\": 860, \"rewards\": -164.2048}, {\"timesteps\": 861, \"rewards\": -167.610182}, {\"timesteps\": 862, \"rewards\": -124.370263}, {\"timesteps\": 863, \"rewards\": -182.775992}, {\"timesteps\": 864, \"rewards\": -151.655697}, {\"timesteps\": 865, \"rewards\": -42.866449}, {\"timesteps\": 866, \"rewards\": -181.977424}, {\"timesteps\": 867, \"rewards\": -95.617618}, {\"timesteps\": 868, \"rewards\": 163.33185}, {\"timesteps\": 869, \"rewards\": -156.309338}, {\"timesteps\": 870, \"rewards\": -6.105524}, {\"timesteps\": 871, \"rewards\": -155.308887}, {\"timesteps\": 872, \"rewards\": -187.542406}, {\"timesteps\": 873, \"rewards\": 173.190879}, {\"timesteps\": 874, \"rewards\": 131.820529}, {\"timesteps\": 875, \"rewards\": 3.879753}, {\"timesteps\": 876, \"rewards\": -333.281118}, {\"timesteps\": 877, \"rewards\": 107.615048}, {\"timesteps\": 878, \"rewards\": -351.41958}, {\"timesteps\": 879, \"rewards\": -276.018719}, {\"timesteps\": 880, \"rewards\": -61.236857}, {\"timesteps\": 881, \"rewards\": -150.527738}, {\"timesteps\": 882, \"rewards\": -229.300707}, {\"timesteps\": 883, \"rewards\": -232.632752}, {\"timesteps\": 884, \"rewards\": -248.682645}, {\"timesteps\": 885, \"rewards\": -206.408419}, {\"timesteps\": 886, \"rewards\": -247.192097}, {\"timesteps\": 887, \"rewards\": -198.274369}, {\"timesteps\": 888, \"rewards\": -289.603321}, {\"timesteps\": 889, \"rewards\": -263.997895}, {\"timesteps\": 890, \"rewards\": -204.354163}, {\"timesteps\": 891, \"rewards\": -193.479342}, {\"timesteps\": 892, \"rewards\": -73.311204}, {\"timesteps\": 893, \"rewards\": -73.042958}, {\"timesteps\": 894, \"rewards\": -264.882892}, {\"timesteps\": 895, \"rewards\": -85.984114}, {\"timesteps\": 896, \"rewards\": -309.789869}, {\"timesteps\": 897, \"rewards\": 166.73178}, {\"timesteps\": 898, \"rewards\": -136.824532}, {\"timesteps\": 899, \"rewards\": -43.934455}, {\"timesteps\": 900, \"rewards\": -184.22693}, {\"timesteps\": 901, \"rewards\": -257.059778}, {\"timesteps\": 902, \"rewards\": -273.038594}, {\"timesteps\": 903, \"rewards\": -234.027935}, {\"timesteps\": 904, \"rewards\": 102.545806}, {\"timesteps\": 905, \"rewards\": -36.825782}, {\"timesteps\": 906, \"rewards\": -31.421285}, {\"timesteps\": 907, \"rewards\": -89.88418}, {\"timesteps\": 908, \"rewards\": -258.06311}, {\"timesteps\": 909, \"rewards\": -231.944392}, {\"timesteps\": 910, \"rewards\": -244.00421}, {\"timesteps\": 911, \"rewards\": -140.76425}, {\"timesteps\": 912, \"rewards\": -43.491116}, {\"timesteps\": 913, \"rewards\": -274.957431}, {\"timesteps\": 914, \"rewards\": -29.672293}, {\"timesteps\": 915, \"rewards\": -45.510836}, {\"timesteps\": 916, \"rewards\": 164.369494}, {\"timesteps\": 917, \"rewards\": -67.296333}, {\"timesteps\": 918, \"rewards\": -46.924441}, {\"timesteps\": 919, \"rewards\": -67.724211}, {\"timesteps\": 920, \"rewards\": -198.846746}, {\"timesteps\": 921, \"rewards\": -222.54677}, {\"timesteps\": 922, \"rewards\": -163.407013}, {\"timesteps\": 923, \"rewards\": -385.694678}, {\"timesteps\": 924, \"rewards\": 271.53682}, {\"timesteps\": 925, \"rewards\": 192.201934}, {\"timesteps\": 926, \"rewards\": -190.008484}, {\"timesteps\": 927, \"rewards\": -332.860434}, {\"timesteps\": 928, \"rewards\": -247.006687}, {\"timesteps\": 929, \"rewards\": 56.922837}, {\"timesteps\": 930, \"rewards\": -55.901478}, {\"timesteps\": 931, \"rewards\": -136.464269}, {\"timesteps\": 932, \"rewards\": 135.987087}, {\"timesteps\": 933, \"rewards\": -249.06514}, {\"timesteps\": 934, \"rewards\": -76.631467}, {\"timesteps\": 935, \"rewards\": -27.903609}, {\"timesteps\": 936, \"rewards\": -215.597445}, {\"timesteps\": 937, \"rewards\": -47.908474}, {\"timesteps\": 938, \"rewards\": 22.895402}, {\"timesteps\": 939, \"rewards\": -224.961995}, {\"timesteps\": 940, \"rewards\": 200.163699}, {\"timesteps\": 941, \"rewards\": -213.030749}, {\"timesteps\": 942, \"rewards\": -65.713631}, {\"timesteps\": 943, \"rewards\": 19.052179}, {\"timesteps\": 944, \"rewards\": -267.664226}, {\"timesteps\": 945, \"rewards\": -67.577122}, {\"timesteps\": 946, \"rewards\": 113.089057}, {\"timesteps\": 947, \"rewards\": -318.240534}, {\"timesteps\": 948, \"rewards\": -54.515172}, {\"timesteps\": 949, \"rewards\": -231.328864}, {\"timesteps\": 950, \"rewards\": -147.09591}, {\"timesteps\": 951, \"rewards\": 111.951017}, {\"timesteps\": 952, \"rewards\": -17.323617}, {\"timesteps\": 953, \"rewards\": -289.000923}, {\"timesteps\": 954, \"rewards\": -194.498437}, {\"timesteps\": 955, \"rewards\": -305.309733}, {\"timesteps\": 956, \"rewards\": -151.606716}, {\"timesteps\": 957, \"rewards\": 165.204064}, {\"timesteps\": 958, \"rewards\": 168.513162}, {\"timesteps\": 959, \"rewards\": -162.379145}, {\"timesteps\": 960, \"rewards\": -178.572074}, {\"timesteps\": 961, \"rewards\": -200.186034}, {\"timesteps\": 962, \"rewards\": -307.532567}, {\"timesteps\": 963, \"rewards\": -197.439312}, {\"timesteps\": 964, \"rewards\": -235.535202}, {\"timesteps\": 965, \"rewards\": -223.802615}, {\"timesteps\": 966, \"rewards\": -2.605354}, {\"timesteps\": 967, \"rewards\": 184.528051}, {\"timesteps\": 968, \"rewards\": -199.932097}, {\"timesteps\": 969, \"rewards\": -88.49657}, {\"timesteps\": 970, \"rewards\": -8.787481}, {\"timesteps\": 971, \"rewards\": -86.130213}, {\"timesteps\": 972, \"rewards\": -151.356496}, {\"timesteps\": 973, \"rewards\": 141.222515}, {\"timesteps\": 974, \"rewards\": 63.453543}, {\"timesteps\": 975, \"rewards\": -140.556041}, {\"timesteps\": 976, \"rewards\": -260.297974}, {\"timesteps\": 977, \"rewards\": -98.693665}, {\"timesteps\": 978, \"rewards\": -179.99329}, {\"timesteps\": 979, \"rewards\": -256.684986}, {\"timesteps\": 980, \"rewards\": -305.88859}, {\"timesteps\": 981, \"rewards\": -18.360724}, {\"timesteps\": 982, \"rewards\": -131.58372}, {\"timesteps\": 983, \"rewards\": 110.645379}, {\"timesteps\": 984, \"rewards\": -259.776561}, {\"timesteps\": 985, \"rewards\": -107.697629}, {\"timesteps\": 986, \"rewards\": -209.698692}, {\"timesteps\": 987, \"rewards\": -130.861473}, {\"timesteps\": 988, \"rewards\": -378.020793}, {\"timesteps\": 989, \"rewards\": -264.507122}, {\"timesteps\": 990, \"rewards\": -212.005493}, {\"timesteps\": 991, \"rewards\": -214.384587}, {\"timesteps\": 992, \"rewards\": -281.429608}, {\"timesteps\": 993, \"rewards\": -132.907558}, {\"timesteps\": 994, \"rewards\": -225.951426}, {\"timesteps\": 995, \"rewards\": 144.723554}, {\"timesteps\": 996, \"rewards\": 106.888863}, {\"timesteps\": 997, \"rewards\": 104.988579}, {\"timesteps\": 998, \"rewards\": -318.63712}, {\"timesteps\": 999, \"rewards\": -90.213302}, {\"timesteps\": 1000, \"rewards\": -200.804846}, {\"timesteps\": 1001, \"rewards\": 159.667421}, {\"timesteps\": 1002, \"rewards\": 232.708441}, {\"timesteps\": 1003, \"rewards\": -245.722785}, {\"timesteps\": 1004, \"rewards\": 9.411393}, {\"timesteps\": 1005, \"rewards\": -34.033276}, {\"timesteps\": 1006, \"rewards\": -180.666791}, {\"timesteps\": 1007, \"rewards\": 146.062864}, {\"timesteps\": 1008, \"rewards\": -46.920385}, {\"timesteps\": 1009, \"rewards\": -56.820321}, {\"timesteps\": 1010, \"rewards\": -205.075381}, {\"timesteps\": 1011, \"rewards\": 66.146911}, {\"timesteps\": 1012, \"rewards\": -154.110332}, {\"timesteps\": 1013, \"rewards\": -137.075383}, {\"timesteps\": 1014, \"rewards\": -160.503453}, {\"timesteps\": 1015, \"rewards\": -189.251963}, {\"timesteps\": 1016, \"rewards\": -203.08392}, {\"timesteps\": 1017, \"rewards\": 185.234949}, {\"timesteps\": 1018, \"rewards\": -62.174295}, {\"timesteps\": 1019, \"rewards\": -108.013921}, {\"timesteps\": 1020, \"rewards\": -150.996257}, {\"timesteps\": 1021, \"rewards\": -182.711886}, {\"timesteps\": 1022, \"rewards\": -230.115566}, {\"timesteps\": 1023, \"rewards\": -156.053362}, {\"timesteps\": 1024, \"rewards\": -5.472379}, {\"timesteps\": 1025, \"rewards\": -97.358846}, {\"timesteps\": 1026, \"rewards\": -156.093069}, {\"timesteps\": 1027, \"rewards\": -214.795715}, {\"timesteps\": 1028, \"rewards\": 158.461402}, {\"timesteps\": 1029, \"rewards\": -331.803618}, {\"timesteps\": 1030, \"rewards\": -172.576598}, {\"timesteps\": 1031, \"rewards\": 107.639397}, {\"timesteps\": 1032, \"rewards\": 88.533478}, {\"timesteps\": 1033, \"rewards\": -232.594671}, {\"timesteps\": 1034, \"rewards\": 120.2424}, {\"timesteps\": 1035, \"rewards\": -240.087976}, {\"timesteps\": 1036, \"rewards\": -227.836593}, {\"timesteps\": 1037, \"rewards\": 168.902879}, {\"timesteps\": 1038, \"rewards\": -145.269032}, {\"timesteps\": 1039, \"rewards\": -342.453746}, {\"timesteps\": 1040, \"rewards\": -154.93573}, {\"timesteps\": 1041, \"rewards\": -206.516494}, {\"timesteps\": 1042, \"rewards\": -291.811505}, {\"timesteps\": 1043, \"rewards\": -208.338645}, {\"timesteps\": 1044, \"rewards\": 183.62964}, {\"timesteps\": 1045, \"rewards\": -110.606441}, {\"timesteps\": 1046, \"rewards\": -75.664744}, {\"timesteps\": 1047, \"rewards\": -177.106044}, {\"timesteps\": 1048, \"rewards\": -96.447965}, {\"timesteps\": 1049, \"rewards\": -298.309618}, {\"timesteps\": 1050, \"rewards\": -92.479742}, {\"timesteps\": 1051, \"rewards\": -254.711748}, {\"timesteps\": 1052, \"rewards\": 129.871138}, {\"timesteps\": 1053, \"rewards\": -184.373832}, {\"timesteps\": 1054, \"rewards\": 27.94089}, {\"timesteps\": 1055, \"rewards\": -132.66902}, {\"timesteps\": 1056, \"rewards\": -22.898402}, {\"timesteps\": 1057, \"rewards\": 192.137991}, {\"timesteps\": 1058, \"rewards\": -216.127142}, {\"timesteps\": 1059, \"rewards\": -282.969676}, {\"timesteps\": 1060, \"rewards\": -210.579578}, {\"timesteps\": 1061, \"rewards\": -54.451804}, {\"timesteps\": 1062, \"rewards\": -334.416765}, {\"timesteps\": 1063, \"rewards\": 143.947731}, {\"timesteps\": 1064, \"rewards\": -242.174671}, {\"timesteps\": 1065, \"rewards\": -189.395755}, {\"timesteps\": 1066, \"rewards\": 35.4769}, {\"timesteps\": 1067, \"rewards\": -73.890542}, {\"timesteps\": 1068, \"rewards\": 58.929104}, {\"timesteps\": 1069, \"rewards\": 198.226352}, {\"timesteps\": 1070, \"rewards\": 176.827535}, {\"timesteps\": 1071, \"rewards\": -260.72863}, {\"timesteps\": 1072, \"rewards\": -174.254868}, {\"timesteps\": 1073, \"rewards\": -186.150092}, {\"timesteps\": 1074, \"rewards\": -262.373767}, {\"timesteps\": 1075, \"rewards\": -85.350346}, {\"timesteps\": 1076, \"rewards\": -247.550389}, {\"timesteps\": 1077, \"rewards\": -276.046011}, {\"timesteps\": 1078, \"rewards\": 171.189946}, {\"timesteps\": 1079, \"rewards\": -266.844264}, {\"timesteps\": 1080, \"rewards\": 148.773318}, {\"timesteps\": 1081, \"rewards\": 147.839317}, {\"timesteps\": 1082, \"rewards\": 80.585684}, {\"timesteps\": 1083, \"rewards\": -16.453145}, {\"timesteps\": 1084, \"rewards\": 174.341067}, {\"timesteps\": 1085, \"rewards\": 107.941276}, {\"timesteps\": 1086, \"rewards\": -95.778591}, {\"timesteps\": 1087, \"rewards\": -63.83889}, {\"timesteps\": 1088, \"rewards\": 4.954163}, {\"timesteps\": 1089, \"rewards\": 137.044726}, {\"timesteps\": 1090, \"rewards\": -325.713413}, {\"timesteps\": 1091, \"rewards\": 124.456623}, {\"timesteps\": 1092, \"rewards\": 51.379324}, {\"timesteps\": 1093, \"rewards\": -225.887939}, {\"timesteps\": 1094, \"rewards\": -78.803857}, {\"timesteps\": 1095, \"rewards\": -72.637129}, {\"timesteps\": 1096, \"rewards\": -37.087922}, {\"timesteps\": 1097, \"rewards\": -23.258838}, {\"timesteps\": 1098, \"rewards\": -302.570323}, {\"timesteps\": 1099, \"rewards\": -120.681575}, {\"timesteps\": 1100, \"rewards\": 199.721946}, {\"timesteps\": 1101, \"rewards\": -109.730679}, {\"timesteps\": 1102, \"rewards\": -275.599978}, {\"timesteps\": 1103, \"rewards\": -249.73995}, {\"timesteps\": 1104, \"rewards\": -225.455375}, {\"timesteps\": 1105, \"rewards\": -38.417094}, {\"timesteps\": 1106, \"rewards\": -154.785697}, {\"timesteps\": 1107, \"rewards\": -152.33068}, {\"timesteps\": 1108, \"rewards\": 11.846115}, {\"timesteps\": 1109, \"rewards\": -157.282431}, {\"timesteps\": 1110, \"rewards\": 128.666406}, {\"timesteps\": 1111, \"rewards\": -339.576258}, {\"timesteps\": 1112, \"rewards\": -338.321556}, {\"timesteps\": 1113, \"rewards\": -125.261134}, {\"timesteps\": 1114, \"rewards\": -10.106461}, {\"timesteps\": 1115, \"rewards\": -38.45824}, {\"timesteps\": 1116, \"rewards\": -53.958495}, {\"timesteps\": 1117, \"rewards\": -100.63392}, {\"timesteps\": 1118, \"rewards\": -225.506147}, {\"timesteps\": 1119, \"rewards\": 72.807325}, {\"timesteps\": 1120, \"rewards\": -256.379396}, {\"timesteps\": 1121, \"rewards\": -55.753101}, {\"timesteps\": 1122, \"rewards\": -42.813702}, {\"timesteps\": 1123, \"rewards\": 19.361212}, {\"timesteps\": 1124, \"rewards\": -79.070735}, {\"timesteps\": 1125, \"rewards\": -13.954187}, {\"timesteps\": 1126, \"rewards\": -91.058939}, {\"timesteps\": 1127, \"rewards\": 102.570696}, {\"timesteps\": 1128, \"rewards\": -31.424852}, {\"timesteps\": 1129, \"rewards\": -126.244194}, {\"timesteps\": 1130, \"rewards\": -184.666697}, {\"timesteps\": 1131, \"rewards\": -227.148069}, {\"timesteps\": 1132, \"rewards\": -306.866974}, {\"timesteps\": 1133, \"rewards\": -72.639313}, {\"timesteps\": 1134, \"rewards\": -20.838888}, {\"timesteps\": 1135, \"rewards\": 195.091633}, {\"timesteps\": 1136, \"rewards\": 129.25341}, {\"timesteps\": 1137, \"rewards\": -185.868923}, {\"timesteps\": 1138, \"rewards\": -60.190332}, {\"timesteps\": 1139, \"rewards\": -183.56778}, {\"timesteps\": 1140, \"rewards\": -15.658764}, {\"timesteps\": 1141, \"rewards\": 194.190634}, {\"timesteps\": 1142, \"rewards\": 132.831131}, {\"timesteps\": 1143, \"rewards\": -305.259627}, {\"timesteps\": 1144, \"rewards\": -304.154551}, {\"timesteps\": 1145, \"rewards\": -86.168615}, {\"timesteps\": 1146, \"rewards\": 202.624962}, {\"timesteps\": 1147, \"rewards\": -184.821655}, {\"timesteps\": 1148, \"rewards\": -158.956457}, {\"timesteps\": 1149, \"rewards\": 147.197799}, {\"timesteps\": 1150, \"rewards\": 54.412242}, {\"timesteps\": 1151, \"rewards\": -191.888429}, {\"timesteps\": 1152, \"rewards\": -52.612705}, {\"timesteps\": 1153, \"rewards\": 134.295366}, {\"timesteps\": 1154, \"rewards\": -225.606584}, {\"timesteps\": 1155, \"rewards\": -45.758569}, {\"timesteps\": 1156, \"rewards\": -114.346108}, {\"timesteps\": 1157, \"rewards\": -36.107879}, {\"timesteps\": 1158, \"rewards\": -220.368364}, {\"timesteps\": 1159, \"rewards\": -18.294135}, {\"timesteps\": 1160, \"rewards\": -236.51479}, {\"timesteps\": 1161, \"rewards\": -76.867336}, {\"timesteps\": 1162, \"rewards\": 38.051369}, {\"timesteps\": 1163, \"rewards\": 29.137294}, {\"timesteps\": 1164, \"rewards\": -45.834213}, {\"timesteps\": 1165, \"rewards\": -218.42557}, {\"timesteps\": 1166, \"rewards\": -84.766576}, {\"timesteps\": 1167, \"rewards\": 92.324196}, {\"timesteps\": 1168, \"rewards\": 34.472886}, {\"timesteps\": 1169, \"rewards\": -209.27553}, {\"timesteps\": 1170, \"rewards\": -184.116731}, {\"timesteps\": 1171, \"rewards\": -147.580334}, {\"timesteps\": 1172, \"rewards\": -100.544373}, {\"timesteps\": 1173, \"rewards\": -252.659525}, {\"timesteps\": 1174, \"rewards\": -101.06191}, {\"timesteps\": 1175, \"rewards\": -216.122325}, {\"timesteps\": 1176, \"rewards\": -234.237317}, {\"timesteps\": 1177, \"rewards\": -228.866073}, {\"timesteps\": 1178, \"rewards\": -294.727982}, {\"timesteps\": 1179, \"rewards\": -71.775945}, {\"timesteps\": 1180, \"rewards\": -127.120701}, {\"timesteps\": 1181, \"rewards\": 7.611367}, {\"timesteps\": 1182, \"rewards\": 164.230216}, {\"timesteps\": 1183, \"rewards\": -8.886685}, {\"timesteps\": 1184, \"rewards\": 121.21802}, {\"timesteps\": 1185, \"rewards\": 196.703156}, {\"timesteps\": 1186, \"rewards\": 100.713219}, {\"timesteps\": 1187, \"rewards\": -260.342289}, {\"timesteps\": 1188, \"rewards\": 72.939801}, {\"timesteps\": 1189, \"rewards\": 29.528232}, {\"timesteps\": 1190, \"rewards\": -209.858538}, {\"timesteps\": 1191, \"rewards\": -63.612071}, {\"timesteps\": 1192, \"rewards\": 144.613272}, {\"timesteps\": 1193, \"rewards\": 178.692692}, {\"timesteps\": 1194, \"rewards\": -279.172405}, {\"timesteps\": 1195, \"rewards\": -53.825286}, {\"timesteps\": 1196, \"rewards\": -121.217483}, {\"timesteps\": 1197, \"rewards\": -106.846929}, {\"timesteps\": 1198, \"rewards\": 51.28477}, {\"timesteps\": 1199, \"rewards\": 173.876913}, {\"timesteps\": 1200, \"rewards\": -262.642923}, {\"timesteps\": 1201, \"rewards\": 175.518783}, {\"timesteps\": 1202, \"rewards\": -49.87268}, {\"timesteps\": 1203, \"rewards\": 46.565572}, {\"timesteps\": 1204, \"rewards\": 211.510462}, {\"timesteps\": 1205, \"rewards\": 196.881201}, {\"timesteps\": 1206, \"rewards\": 105.436934}, {\"timesteps\": 1207, \"rewards\": 62.773673}, {\"timesteps\": 1208, \"rewards\": 102.127514}, {\"timesteps\": 1209, \"rewards\": -247.154212}, {\"timesteps\": 1210, \"rewards\": -212.932278}, {\"timesteps\": 1211, \"rewards\": 202.282177}, {\"timesteps\": 1212, \"rewards\": -60.650705}, {\"timesteps\": 1213, \"rewards\": -240.839729}, {\"timesteps\": 1214, \"rewards\": 64.744248}, {\"timesteps\": 1215, \"rewards\": -26.799041}, {\"timesteps\": 1216, \"rewards\": 153.482914}, {\"timesteps\": 1217, \"rewards\": -146.372994}, {\"timesteps\": 1218, \"rewards\": 112.824057}, {\"timesteps\": 1219, \"rewards\": 100.506591}, {\"timesteps\": 1220, \"rewards\": -28.559393}, {\"timesteps\": 1221, \"rewards\": 159.633069}, {\"timesteps\": 1222, \"rewards\": 6.934616}, {\"timesteps\": 1223, \"rewards\": -320.50362}, {\"timesteps\": 1224, \"rewards\": 102.500172}, {\"timesteps\": 1225, \"rewards\": -53.952121}, {\"timesteps\": 1226, \"rewards\": -41.064001}, {\"timesteps\": 1227, \"rewards\": 239.764033}, {\"timesteps\": 1228, \"rewards\": 130.279621}, {\"timesteps\": 1229, \"rewards\": 110.708909}, {\"timesteps\": 1230, \"rewards\": -223.494404}, {\"timesteps\": 1231, \"rewards\": 253.105179}, {\"timesteps\": 1232, \"rewards\": -66.305825}, {\"timesteps\": 1233, \"rewards\": -178.186292}, {\"timesteps\": 1234, \"rewards\": 156.366886}, {\"timesteps\": 1235, \"rewards\": -230.543023}, {\"timesteps\": 1236, \"rewards\": 206.904755}, {\"timesteps\": 1237, \"rewards\": -214.243035}, {\"timesteps\": 1238, \"rewards\": 94.391833}, {\"timesteps\": 1239, \"rewards\": -254.419205}, {\"timesteps\": 1240, \"rewards\": 211.607376}, {\"timesteps\": 1241, \"rewards\": 222.033853}, {\"timesteps\": 1242, \"rewards\": 107.970794}, {\"timesteps\": 1243, \"rewards\": 110.554548}, {\"timesteps\": 1244, \"rewards\": -170.751556}, {\"timesteps\": 1245, \"rewards\": -60.838899}, {\"timesteps\": 1246, \"rewards\": 146.812143}, {\"timesteps\": 1247, \"rewards\": 163.685249}, {\"timesteps\": 1248, \"rewards\": -88.048477}, {\"timesteps\": 1249, \"rewards\": 100.485414}, {\"timesteps\": 1250, \"rewards\": -321.11185}, {\"timesteps\": 1251, \"rewards\": 123.721543}, {\"timesteps\": 1252, \"rewards\": 62.317417}, {\"timesteps\": 1253, \"rewards\": 134.621015}, {\"timesteps\": 1254, \"rewards\": 111.857453}, {\"timesteps\": 1255, \"rewards\": -52.403397}, {\"timesteps\": 1256, \"rewards\": 184.221087}, {\"timesteps\": 1257, \"rewards\": -396.467004}, {\"timesteps\": 1258, \"rewards\": 36.952513}, {\"timesteps\": 1259, \"rewards\": 249.788044}, {\"timesteps\": 1260, \"rewards\": -35.202546}, {\"timesteps\": 1261, \"rewards\": -3.121647}, {\"timesteps\": 1262, \"rewards\": -59.913089}, {\"timesteps\": 1263, \"rewards\": 258.268375}, {\"timesteps\": 1264, \"rewards\": -22.791847}, {\"timesteps\": 1265, \"rewards\": 205.137964}, {\"timesteps\": 1266, \"rewards\": -121.270395}, {\"timesteps\": 1267, \"rewards\": 97.943446}, {\"timesteps\": 1268, \"rewards\": -45.65424}, {\"timesteps\": 1269, \"rewards\": -244.907994}, {\"timesteps\": 1270, \"rewards\": -268.951686}, {\"timesteps\": 1271, \"rewards\": -132.754352}, {\"timesteps\": 1272, \"rewards\": -30.28637}, {\"timesteps\": 1273, \"rewards\": -107.067836}, {\"timesteps\": 1274, \"rewards\": -259.348324}, {\"timesteps\": 1275, \"rewards\": 220.055191}, {\"timesteps\": 1276, \"rewards\": -96.993146}, {\"timesteps\": 1277, \"rewards\": 169.368841}, {\"timesteps\": 1278, \"rewards\": 8.425115}, {\"timesteps\": 1279, \"rewards\": 186.403813}, {\"timesteps\": 1280, \"rewards\": -59.236247}, {\"timesteps\": 1281, \"rewards\": -70.456552}, {\"timesteps\": 1282, \"rewards\": 187.163176}, {\"timesteps\": 1283, \"rewards\": -217.523633}, {\"timesteps\": 1284, \"rewards\": 233.635967}, {\"timesteps\": 1285, \"rewards\": -85.29335}, {\"timesteps\": 1286, \"rewards\": 190.556271}, {\"timesteps\": 1287, \"rewards\": 157.666273}, {\"timesteps\": 1288, \"rewards\": -216.365739}, {\"timesteps\": 1289, \"rewards\": 60.855319}, {\"timesteps\": 1290, \"rewards\": -82.139594}, {\"timesteps\": 1291, \"rewards\": -62.534851}, {\"timesteps\": 1292, \"rewards\": -96.613209}, {\"timesteps\": 1293, \"rewards\": -280.662969}, {\"timesteps\": 1294, \"rewards\": -69.638762}, {\"timesteps\": 1295, \"rewards\": -215.213258}, {\"timesteps\": 1296, \"rewards\": 169.033054}, {\"timesteps\": 1297, \"rewards\": 143.944057}, {\"timesteps\": 1298, \"rewards\": -127.705378}, {\"timesteps\": 1299, \"rewards\": 92.109942}, {\"timesteps\": 1300, \"rewards\": -14.203068}, {\"timesteps\": 1301, \"rewards\": -229.727114}, {\"timesteps\": 1302, \"rewards\": -141.702451}, {\"timesteps\": 1303, \"rewards\": -83.443162}, {\"timesteps\": 1304, \"rewards\": -66.575537}, {\"timesteps\": 1305, \"rewards\": 49.761715}, {\"timesteps\": 1306, \"rewards\": -81.684228}, {\"timesteps\": 1307, \"rewards\": 166.018843}, {\"timesteps\": 1308, \"rewards\": -4.517845}, {\"timesteps\": 1309, \"rewards\": 117.946277}, {\"timesteps\": 1310, \"rewards\": 58.764946}, {\"timesteps\": 1311, \"rewards\": -225.099951}, {\"timesteps\": 1312, \"rewards\": -324.464332}, {\"timesteps\": 1313, \"rewards\": 213.426918}, {\"timesteps\": 1314, \"rewards\": 104.185585}, {\"timesteps\": 1315, \"rewards\": -41.413543}, {\"timesteps\": 1316, \"rewards\": 125.603747}, {\"timesteps\": 1317, \"rewards\": 118.487201}, {\"timesteps\": 1318, \"rewards\": 35.948717}, {\"timesteps\": 1319, \"rewards\": 29.927344}, {\"timesteps\": 1320, \"rewards\": -5.26565}, {\"timesteps\": 1321, \"rewards\": -69.485516}, {\"timesteps\": 1322, \"rewards\": 168.438565}, {\"timesteps\": 1323, \"rewards\": 170.476374}, {\"timesteps\": 1324, \"rewards\": 204.645966}, {\"timesteps\": 1325, \"rewards\": -160.222105}, {\"timesteps\": 1326, \"rewards\": 124.897314}, {\"timesteps\": 1327, \"rewards\": 211.705302}, {\"timesteps\": 1328, \"rewards\": 93.021662}, {\"timesteps\": 1329, \"rewards\": 206.654175}, {\"timesteps\": 1330, \"rewards\": 28.410538}, {\"timesteps\": 1331, \"rewards\": -246.699459}, {\"timesteps\": 1332, \"rewards\": 135.201286}, {\"timesteps\": 1333, \"rewards\": 92.567244}, {\"timesteps\": 1334, \"rewards\": 209.258396}, {\"timesteps\": 1335, \"rewards\": -38.13948}, {\"timesteps\": 1336, \"rewards\": -24.949868}, {\"timesteps\": 1337, \"rewards\": -9.219347}, {\"timesteps\": 1338, \"rewards\": 139.153449}, {\"timesteps\": 1339, \"rewards\": -194.450746}, {\"timesteps\": 1340, \"rewards\": -209.963285}, {\"timesteps\": 1341, \"rewards\": -84.520106}, {\"timesteps\": 1342, \"rewards\": 108.39675}, {\"timesteps\": 1343, \"rewards\": 40.615304}, {\"timesteps\": 1344, \"rewards\": 130.798157}, {\"timesteps\": 1345, \"rewards\": -54.500754}, {\"timesteps\": 1346, \"rewards\": 122.198218}, {\"timesteps\": 1347, \"rewards\": 145.952539}, {\"timesteps\": 1348, \"rewards\": 94.060104}, {\"timesteps\": 1349, \"rewards\": -27.024114}, {\"timesteps\": 1350, \"rewards\": -1.528048}, {\"timesteps\": 1351, \"rewards\": 57.882853}, {\"timesteps\": 1352, \"rewards\": 10.462542}, {\"timesteps\": 1353, \"rewards\": 149.864659}, {\"timesteps\": 1354, \"rewards\": 220.266719}, {\"timesteps\": 1355, \"rewards\": 41.240265}, {\"timesteps\": 1356, \"rewards\": -112.37355}, {\"timesteps\": 1357, \"rewards\": -86.120743}, {\"timesteps\": 1358, \"rewards\": -58.711408}, {\"timesteps\": 1359, \"rewards\": -45.124438}, {\"timesteps\": 1360, \"rewards\": -67.44988}, {\"timesteps\": 1361, \"rewards\": 232.606066}, {\"timesteps\": 1362, \"rewards\": -202.980063}, {\"timesteps\": 1363, \"rewards\": 98.195287}, {\"timesteps\": 1364, \"rewards\": 135.083744}, {\"timesteps\": 1365, \"rewards\": -41.986334}, {\"timesteps\": 1366, \"rewards\": -56.004661}, {\"timesteps\": 1367, \"rewards\": 140.626475}, {\"timesteps\": 1368, \"rewards\": 126.748818}, {\"timesteps\": 1369, \"rewards\": -209.729118}, {\"timesteps\": 1370, \"rewards\": -224.042486}, {\"timesteps\": 1371, \"rewards\": -187.170533}, {\"timesteps\": 1372, \"rewards\": 228.452809}, {\"timesteps\": 1373, \"rewards\": 87.761815}, {\"timesteps\": 1374, \"rewards\": -252.724515}, {\"timesteps\": 1375, \"rewards\": -268.982673}, {\"timesteps\": 1376, \"rewards\": -53.443338}, {\"timesteps\": 1377, \"rewards\": 125.402653}, {\"timesteps\": 1378, \"rewards\": 32.93645}, {\"timesteps\": 1379, \"rewards\": -194.368622}, {\"timesteps\": 1380, \"rewards\": -46.485447}, {\"timesteps\": 1381, \"rewards\": 217.051341}, {\"timesteps\": 1382, \"rewards\": 55.783581}, {\"timesteps\": 1383, \"rewards\": 95.400773}, {\"timesteps\": 1384, \"rewards\": -48.762794}, {\"timesteps\": 1385, \"rewards\": 2.335999}, {\"timesteps\": 1386, \"rewards\": -193.241346}, {\"timesteps\": 1387, \"rewards\": -177.402451}, {\"timesteps\": 1388, \"rewards\": -26.861638}, {\"timesteps\": 1389, \"rewards\": 187.031354}, {\"timesteps\": 1390, \"rewards\": -88.639615}, {\"timesteps\": 1391, \"rewards\": -61.204993}, {\"timesteps\": 1392, \"rewards\": 81.632361}, {\"timesteps\": 1393, \"rewards\": 108.925633}, {\"timesteps\": 1394, \"rewards\": 43.746763}, {\"timesteps\": 1395, \"rewards\": -23.104715}, {\"timesteps\": 1396, \"rewards\": -202.356559}, {\"timesteps\": 1397, \"rewards\": -27.990048}, {\"timesteps\": 1398, \"rewards\": 237.464969}, {\"timesteps\": 1399, \"rewards\": 42.340236}, {\"timesteps\": 1400, \"rewards\": -69.113147}, {\"timesteps\": 1401, \"rewards\": 219.180008}, {\"timesteps\": 1402, \"rewards\": 203.602333}, {\"timesteps\": 1403, \"rewards\": 106.495832}, {\"timesteps\": 1404, \"rewards\": 93.498835}, {\"timesteps\": 1405, \"rewards\": -29.744736}, {\"timesteps\": 1406, \"rewards\": -251.719705}, {\"timesteps\": 1407, \"rewards\": 132.31667}, {\"timesteps\": 1408, \"rewards\": 123.075637}, {\"timesteps\": 1409, \"rewards\": -166.213077}, {\"timesteps\": 1410, \"rewards\": 209.513969}, {\"timesteps\": 1411, \"rewards\": 204.744316}, {\"timesteps\": 1412, \"rewards\": 100.72344}, {\"timesteps\": 1413, \"rewards\": -206.695085}, {\"timesteps\": 1414, \"rewards\": 158.493611}, {\"timesteps\": 1415, \"rewards\": -183.852579}, {\"timesteps\": 1416, \"rewards\": 16.595662}, {\"timesteps\": 1417, \"rewards\": -30.214797}, {\"timesteps\": 1418, \"rewards\": -61.943413}, {\"timesteps\": 1419, \"rewards\": 229.81376}, {\"timesteps\": 1420, \"rewards\": -24.86191}, {\"timesteps\": 1421, \"rewards\": 144.924223}, {\"timesteps\": 1422, \"rewards\": 134.878026}, {\"timesteps\": 1423, \"rewards\": 53.123997}, {\"timesteps\": 1424, \"rewards\": 43.286867}, {\"timesteps\": 1425, \"rewards\": 251.644576}, {\"timesteps\": 1426, \"rewards\": -201.161905}, {\"timesteps\": 1427, \"rewards\": 134.373621}, {\"timesteps\": 1428, \"rewards\": 75.073662}, {\"timesteps\": 1429, \"rewards\": 183.581666}, {\"timesteps\": 1430, \"rewards\": -36.633971}, {\"timesteps\": 1431, \"rewards\": 170.877152}, {\"timesteps\": 1432, \"rewards\": -269.835318}, {\"timesteps\": 1433, \"rewards\": 235.746369}, {\"timesteps\": 1434, \"rewards\": 144.596064}, {\"timesteps\": 1435, \"rewards\": -134.390129}, {\"timesteps\": 1436, \"rewards\": 144.278066}, {\"timesteps\": 1437, \"rewards\": 131.860007}, {\"timesteps\": 1438, \"rewards\": -45.136844}, {\"timesteps\": 1439, \"rewards\": 29.639747}, {\"timesteps\": 1440, \"rewards\": -62.107788}, {\"timesteps\": 1441, \"rewards\": -288.439317}, {\"timesteps\": 1442, \"rewards\": 154.899472}, {\"timesteps\": 1443, \"rewards\": 198.215047}, {\"timesteps\": 1444, \"rewards\": 1.953922}, {\"timesteps\": 1445, \"rewards\": -20.512484}, {\"timesteps\": 1446, \"rewards\": 175.218029}, {\"timesteps\": 1447, \"rewards\": 161.039869}, {\"timesteps\": 1448, \"rewards\": -42.700692}, {\"timesteps\": 1449, \"rewards\": 112.624668}, {\"timesteps\": 1450, \"rewards\": -201.406847}, {\"timesteps\": 1451, \"rewards\": 169.27135}, {\"timesteps\": 1452, \"rewards\": 109.325271}, {\"timesteps\": 1453, \"rewards\": 119.209631}, {\"timesteps\": 1454, \"rewards\": 36.952683}, {\"timesteps\": 1455, \"rewards\": -193.177146}, {\"timesteps\": 1456, \"rewards\": 209.289275}, {\"timesteps\": 1457, \"rewards\": -233.944403}, {\"timesteps\": 1458, \"rewards\": 190.45209}, {\"timesteps\": 1459, \"rewards\": 203.074959}, {\"timesteps\": 1460, \"rewards\": 241.563937}, {\"timesteps\": 1461, \"rewards\": 225.187127}, {\"timesteps\": 1462, \"rewards\": 79.605453}, {\"timesteps\": 1463, \"rewards\": 104.125086}, {\"timesteps\": 1464, \"rewards\": -87.598447}, {\"timesteps\": 1465, \"rewards\": -23.259081}, {\"timesteps\": 1466, \"rewards\": -3.893679}, {\"timesteps\": 1467, \"rewards\": -62.319814}, {\"timesteps\": 1468, \"rewards\": 160.301345}, {\"timesteps\": 1469, \"rewards\": -39.128393}, {\"timesteps\": 1470, \"rewards\": 168.287698}, {\"timesteps\": 1471, \"rewards\": -21.22557}, {\"timesteps\": 1472, \"rewards\": 167.558494}, {\"timesteps\": 1473, \"rewards\": 109.577847}, {\"timesteps\": 1474, \"rewards\": 186.045924}, {\"timesteps\": 1475, \"rewards\": -241.693431}, {\"timesteps\": 1476, \"rewards\": 132.529472}, {\"timesteps\": 1477, \"rewards\": 115.542157}, {\"timesteps\": 1478, \"rewards\": 55.602565}, {\"timesteps\": 1479, \"rewards\": 146.407609}, {\"timesteps\": 1480, \"rewards\": -72.347244}, {\"timesteps\": 1481, \"rewards\": 173.451585}, {\"timesteps\": 1482, \"rewards\": 142.66953}, {\"timesteps\": 1483, \"rewards\": -39.58213}, {\"timesteps\": 1484, \"rewards\": 133.697726}, {\"timesteps\": 1485, \"rewards\": 243.2542}, {\"timesteps\": 1486, \"rewards\": 142.738495}, {\"timesteps\": 1487, \"rewards\": 190.749002}, {\"timesteps\": 1488, \"rewards\": 61.339308}, {\"timesteps\": 1489, \"rewards\": 120.744624}, {\"timesteps\": 1490, \"rewards\": 211.527299}, {\"timesteps\": 1491, \"rewards\": -158.688949}, {\"timesteps\": 1492, \"rewards\": 247.673277}, {\"timesteps\": 1493, \"rewards\": 13.963637}, {\"timesteps\": 1494, \"rewards\": -190.103429}, {\"timesteps\": 1495, \"rewards\": 2.821581}, {\"timesteps\": 1496, \"rewards\": -291.833531}, {\"timesteps\": 1497, \"rewards\": 233.058998}, {\"timesteps\": 1498, \"rewards\": 163.733928}, {\"timesteps\": 1499, \"rewards\": 26.301165}, {\"timesteps\": 1500, \"rewards\": -37.769953}, {\"timesteps\": 1501, \"rewards\": 171.61563}, {\"timesteps\": 1502, \"rewards\": -317.042004}, {\"timesteps\": 1503, \"rewards\": -95.252148}, {\"timesteps\": 1504, \"rewards\": 114.821107}, {\"timesteps\": 1505, \"rewards\": -234.603561}, {\"timesteps\": 1506, \"rewards\": 182.057316}, {\"timesteps\": 1507, \"rewards\": -49.124358}, {\"timesteps\": 1508, \"rewards\": 194.16119}, {\"timesteps\": 1509, \"rewards\": 107.663073}, {\"timesteps\": 1510, \"rewards\": -163.978467}, {\"timesteps\": 1511, \"rewards\": -57.783151}, {\"timesteps\": 1512, \"rewards\": -150.772677}, {\"timesteps\": 1513, \"rewards\": 5.443453}, {\"timesteps\": 1514, \"rewards\": 51.57075}, {\"timesteps\": 1515, \"rewards\": 52.926361}, {\"timesteps\": 1516, \"rewards\": 132.811911}, {\"timesteps\": 1517, \"rewards\": 125.190106}, {\"timesteps\": 1518, \"rewards\": 64.562136}, {\"timesteps\": 1519, \"rewards\": 96.100493}, {\"timesteps\": 1520, \"rewards\": 170.355452}, {\"timesteps\": 1521, \"rewards\": 63.579665}, {\"timesteps\": 1522, \"rewards\": 19.470435}, {\"timesteps\": 1523, \"rewards\": -128.773475}, {\"timesteps\": 1524, \"rewards\": 157.91215}, {\"timesteps\": 1525, \"rewards\": -250.828588}, {\"timesteps\": 1526, \"rewards\": 98.019624}, {\"timesteps\": 1527, \"rewards\": 77.295999}, {\"timesteps\": 1528, \"rewards\": 102.584151}, {\"timesteps\": 1529, \"rewards\": 118.043952}, {\"timesteps\": 1530, \"rewards\": -46.977135}, {\"timesteps\": 1531, \"rewards\": -62.625376}, {\"timesteps\": 1532, \"rewards\": 101.287044}, {\"timesteps\": 1533, \"rewards\": 89.962179}, {\"timesteps\": 1534, \"rewards\": -145.042283}, {\"timesteps\": 1535, \"rewards\": 49.678795}, {\"timesteps\": 1536, \"rewards\": 190.741177}, {\"timesteps\": 1537, \"rewards\": -92.647578}, {\"timesteps\": 1538, \"rewards\": 118.369069}, {\"timesteps\": 1539, \"rewards\": 155.521489}, {\"timesteps\": 1540, \"rewards\": -55.088421}, {\"timesteps\": 1541, \"rewards\": -37.310777}, {\"timesteps\": 1542, \"rewards\": 183.694883}, {\"timesteps\": 1543, \"rewards\": -123.686869}, {\"timesteps\": 1544, \"rewards\": 44.14965}, {\"timesteps\": 1545, \"rewards\": 25.556204}, {\"timesteps\": 1546, \"rewards\": -69.780986}, {\"timesteps\": 1547, \"rewards\": -242.065186}, {\"timesteps\": 1548, \"rewards\": 199.798764}, {\"timesteps\": 1549, \"rewards\": -29.698104}, {\"timesteps\": 1550, \"rewards\": -32.398961}, {\"timesteps\": 1551, \"rewards\": -69.349288}, {\"timesteps\": 1552, \"rewards\": 177.456259}, {\"timesteps\": 1553, \"rewards\": -31.194133}, {\"timesteps\": 1554, \"rewards\": -229.697382}, {\"timesteps\": 1555, \"rewards\": 224.755561}, {\"timesteps\": 1556, \"rewards\": -207.508649}, {\"timesteps\": 1557, \"rewards\": 107.745871}, {\"timesteps\": 1558, \"rewards\": 139.977028}, {\"timesteps\": 1559, \"rewards\": 119.354053}, {\"timesteps\": 1560, \"rewards\": -103.312285}, {\"timesteps\": 1561, \"rewards\": -113.52389}, {\"timesteps\": 1562, \"rewards\": -74.517763}, {\"timesteps\": 1563, \"rewards\": 169.866315}, {\"timesteps\": 1564, \"rewards\": 69.398768}, {\"timesteps\": 1565, \"rewards\": 204.979514}, {\"timesteps\": 1566, \"rewards\": 189.05377}, {\"timesteps\": 1567, \"rewards\": 102.120882}, {\"timesteps\": 1568, \"rewards\": 53.508733}, {\"timesteps\": 1569, \"rewards\": -35.375696}, {\"timesteps\": 1570, \"rewards\": 145.248328}, {\"timesteps\": 1571, \"rewards\": 112.249052}, {\"timesteps\": 1572, \"rewards\": 86.75017}, {\"timesteps\": 1573, \"rewards\": 80.216266}, {\"timesteps\": 1574, \"rewards\": 154.431814}, {\"timesteps\": 1575, \"rewards\": 164.120124}, {\"timesteps\": 1576, \"rewards\": 106.974901}, {\"timesteps\": 1577, \"rewards\": 51.516969}, {\"timesteps\": 1578, \"rewards\": -135.846358}, {\"timesteps\": 1579, \"rewards\": -143.188252}, {\"timesteps\": 1580, \"rewards\": 85.173981}, {\"timesteps\": 1581, \"rewards\": 33.067463}, {\"timesteps\": 1582, \"rewards\": 82.257118}, {\"timesteps\": 1583, \"rewards\": -183.996008}, {\"timesteps\": 1584, \"rewards\": -34.689687}, {\"timesteps\": 1585, \"rewards\": -49.864573}, {\"timesteps\": 1586, \"rewards\": 172.094249}, {\"timesteps\": 1587, \"rewards\": 186.937337}, {\"timesteps\": 1588, \"rewards\": 78.30432}, {\"timesteps\": 1589, \"rewards\": -158.145122}, {\"timesteps\": 1590, \"rewards\": 211.140717}, {\"timesteps\": 1591, \"rewards\": 34.063332}, {\"timesteps\": 1592, \"rewards\": 96.40671}, {\"timesteps\": 1593, \"rewards\": 41.923526}, {\"timesteps\": 1594, \"rewards\": 96.30077}, {\"timesteps\": 1595, \"rewards\": 38.597529}, {\"timesteps\": 1596, \"rewards\": -53.773008}, {\"timesteps\": 1597, \"rewards\": -51.104846}, {\"timesteps\": 1598, \"rewards\": -45.392516}, {\"timesteps\": 1599, \"rewards\": -164.592395}, {\"timesteps\": 1600, \"rewards\": -38.063473}, {\"timesteps\": 1601, \"rewards\": -50.598073}, {\"timesteps\": 1602, \"rewards\": 112.381647}, {\"timesteps\": 1603, \"rewards\": 222.488678}, {\"timesteps\": 1604, \"rewards\": 47.039175}, {\"timesteps\": 1605, \"rewards\": -89.069025}, {\"timesteps\": 1606, \"rewards\": 201.013353}, {\"timesteps\": 1607, \"rewards\": 44.73587}, {\"timesteps\": 1608, \"rewards\": 68.85883}, {\"timesteps\": 1609, \"rewards\": 181.041471}, {\"timesteps\": 1610, \"rewards\": 159.975091}, {\"timesteps\": 1611, \"rewards\": 144.351267}, {\"timesteps\": 1612, \"rewards\": 120.121444}, {\"timesteps\": 1613, \"rewards\": -148.54851}, {\"timesteps\": 1614, \"rewards\": 223.760912}, {\"timesteps\": 1615, \"rewards\": 188.902787}, {\"timesteps\": 1616, \"rewards\": 36.180208}, {\"timesteps\": 1617, \"rewards\": 64.58186}, {\"timesteps\": 1618, \"rewards\": 59.411316}, {\"timesteps\": 1619, \"rewards\": 92.440175}, {\"timesteps\": 1620, \"rewards\": 175.214921}, {\"timesteps\": 1621, \"rewards\": -199.95633}, {\"timesteps\": 1622, \"rewards\": 110.555659}, {\"timesteps\": 1623, \"rewards\": -91.84998}, {\"timesteps\": 1624, \"rewards\": 81.730958}, {\"timesteps\": 1625, \"rewards\": -82.58649}, {\"timesteps\": 1626, \"rewards\": 148.981929}, {\"timesteps\": 1627, \"rewards\": 129.230287}, {\"timesteps\": 1628, \"rewards\": 61.128815}, {\"timesteps\": 1629, \"rewards\": 142.866735}, {\"timesteps\": 1630, \"rewards\": -121.253409}, {\"timesteps\": 1631, \"rewards\": -135.541337}, {\"timesteps\": 1632, \"rewards\": -111.525884}, {\"timesteps\": 1633, \"rewards\": 13.978537}, {\"timesteps\": 1634, \"rewards\": 157.613376}, {\"timesteps\": 1635, \"rewards\": -112.368693}, {\"timesteps\": 1636, \"rewards\": 158.942648}, {\"timesteps\": 1637, \"rewards\": 58.950173}, {\"timesteps\": 1638, \"rewards\": 78.833311}, {\"timesteps\": 1639, \"rewards\": -151.643232}, {\"timesteps\": 1640, \"rewards\": -122.581161}, {\"timesteps\": 1641, \"rewards\": 191.385821}, {\"timesteps\": 1642, \"rewards\": 122.358346}, {\"timesteps\": 1643, \"rewards\": -183.682253}, {\"timesteps\": 1644, \"rewards\": 113.164943}, {\"timesteps\": 1645, \"rewards\": 93.290324}, {\"timesteps\": 1646, \"rewards\": 94.463879}, {\"timesteps\": 1647, \"rewards\": -127.762465}, {\"timesteps\": 1648, \"rewards\": -104.384482}, {\"timesteps\": 1649, \"rewards\": -166.655012}, {\"timesteps\": 1650, \"rewards\": 4.017808}, {\"timesteps\": 1651, \"rewards\": -146.050223}, {\"timesteps\": 1652, \"rewards\": -114.412344}, {\"timesteps\": 1653, \"rewards\": 72.221609}, {\"timesteps\": 1654, \"rewards\": -99.424303}, {\"timesteps\": 1655, \"rewards\": -187.679443}, {\"timesteps\": 1656, \"rewards\": -110.925194}, {\"timesteps\": 1657, \"rewards\": -149.104136}, {\"timesteps\": 1658, \"rewards\": -150.304331}, {\"timesteps\": 1659, \"rewards\": -157.514929}, {\"timesteps\": 1660, \"rewards\": -104.864361}, {\"timesteps\": 1661, \"rewards\": -207.659306}, {\"timesteps\": 1662, \"rewards\": -84.923089}, {\"timesteps\": 1663, \"rewards\": -129.140751}, {\"timesteps\": 1664, \"rewards\": 39.674165}, {\"timesteps\": 1665, \"rewards\": -145.61201}, {\"timesteps\": 1666, \"rewards\": 153.291057}, {\"timesteps\": 1667, \"rewards\": -132.0746}, {\"timesteps\": 1668, \"rewards\": -169.698434}, {\"timesteps\": 1669, \"rewards\": -193.008224}, {\"timesteps\": 1670, \"rewards\": -126.186364}, {\"timesteps\": 1671, \"rewards\": 94.571873}, {\"timesteps\": 1672, \"rewards\": -172.488887}, {\"timesteps\": 1673, \"rewards\": 85.381404}, {\"timesteps\": 1674, \"rewards\": -121.981667}, {\"timesteps\": 1675, \"rewards\": -250.701397}, {\"timesteps\": 1676, \"rewards\": -19.805363}, {\"timesteps\": 1677, \"rewards\": -7.950504}, {\"timesteps\": 1678, \"rewards\": -187.478914}, {\"timesteps\": 1679, \"rewards\": 146.121843}, {\"timesteps\": 1680, \"rewards\": -50.537925}, {\"timesteps\": 1681, \"rewards\": 288.013035}, {\"timesteps\": 1682, \"rewards\": 4.45419}, {\"timesteps\": 1683, \"rewards\": 28.977863}, {\"timesteps\": 1684, \"rewards\": -138.684024}, {\"timesteps\": 1685, \"rewards\": -118.224573}, {\"timesteps\": 1686, \"rewards\": -143.674855}, {\"timesteps\": 1687, \"rewards\": -112.342677}, {\"timesteps\": 1688, \"rewards\": -139.573641}, {\"timesteps\": 1689, \"rewards\": 96.104684}, {\"timesteps\": 1690, \"rewards\": 209.680982}, {\"timesteps\": 1691, \"rewards\": -201.737318}, {\"timesteps\": 1692, \"rewards\": -208.121461}, {\"timesteps\": 1693, \"rewards\": -130.21496}, {\"timesteps\": 1694, \"rewards\": -180.509384}, {\"timesteps\": 1695, \"rewards\": 99.131428}, {\"timesteps\": 1696, \"rewards\": 41.550622}, {\"timesteps\": 1697, \"rewards\": -43.804527}, {\"timesteps\": 1698, \"rewards\": -131.859865}, {\"timesteps\": 1699, \"rewards\": -191.346105}, {\"timesteps\": 1700, \"rewards\": 21.166114}, {\"timesteps\": 1701, \"rewards\": -118.559264}, {\"timesteps\": 1702, \"rewards\": -115.363415}, {\"timesteps\": 1703, \"rewards\": -122.267841}, {\"timesteps\": 1704, \"rewards\": -174.232102}, {\"timesteps\": 1705, \"rewards\": -163.665123}, {\"timesteps\": 1706, \"rewards\": -212.283334}, {\"timesteps\": 1707, \"rewards\": 167.436222}, {\"timesteps\": 1708, \"rewards\": -61.769435}, {\"timesteps\": 1709, \"rewards\": -206.062613}, {\"timesteps\": 1710, \"rewards\": -109.499509}, {\"timesteps\": 1711, \"rewards\": -127.016321}, {\"timesteps\": 1712, \"rewards\": 87.088639}, {\"timesteps\": 1713, \"rewards\": -94.931285}, {\"timesteps\": 1714, \"rewards\": 143.520314}, {\"timesteps\": 1715, \"rewards\": 118.330582}, {\"timesteps\": 1716, \"rewards\": -109.118509}, {\"timesteps\": 1717, \"rewards\": 54.320729}, {\"timesteps\": 1718, \"rewards\": 76.300827}, {\"timesteps\": 1719, \"rewards\": -100.48022}, {\"timesteps\": 1720, \"rewards\": 110.351806}, {\"timesteps\": 1721, \"rewards\": 107.224953}, {\"timesteps\": 1722, \"rewards\": 92.219284}, {\"timesteps\": 1723, \"rewards\": -116.482541}, {\"timesteps\": 1724, \"rewards\": -100.322257}, {\"timesteps\": 1725, \"rewards\": 155.791966}, {\"timesteps\": 1726, \"rewards\": -90.759816}, {\"timesteps\": 1727, \"rewards\": 93.62118}, {\"timesteps\": 1728, \"rewards\": 51.370294}, {\"timesteps\": 1729, \"rewards\": -119.748698}, {\"timesteps\": 1730, \"rewards\": -85.100531}, {\"timesteps\": 1731, \"rewards\": -42.424638}, {\"timesteps\": 1732, \"rewards\": 146.444443}, {\"timesteps\": 1733, \"rewards\": -76.888712}, {\"timesteps\": 1734, \"rewards\": -110.707714}, {\"timesteps\": 1735, \"rewards\": 171.47425}, {\"timesteps\": 1736, \"rewards\": 153.551096}, {\"timesteps\": 1737, \"rewards\": 39.576009}, {\"timesteps\": 1738, \"rewards\": 143.730866}, {\"timesteps\": 1739, \"rewards\": 118.543064}, {\"timesteps\": 1740, \"rewards\": -229.15645}, {\"timesteps\": 1741, \"rewards\": -190.189419}, {\"timesteps\": 1742, \"rewards\": -119.297021}, {\"timesteps\": 1743, \"rewards\": 32.252723}, {\"timesteps\": 1744, \"rewards\": 10.728262}, {\"timesteps\": 1745, \"rewards\": -29.057334}, {\"timesteps\": 1746, \"rewards\": -203.305022}, {\"timesteps\": 1747, \"rewards\": 244.118182}, {\"timesteps\": 1748, \"rewards\": -142.131514}, {\"timesteps\": 1749, \"rewards\": 5.37845}, {\"timesteps\": 1750, \"rewards\": -127.636795}, {\"timesteps\": 1751, \"rewards\": 153.828174}, {\"timesteps\": 1752, \"rewards\": -90.471107}, {\"timesteps\": 1753, \"rewards\": 187.981778}, {\"timesteps\": 1754, \"rewards\": 125.051772}, {\"timesteps\": 1755, \"rewards\": -208.690817}, {\"timesteps\": 1756, \"rewards\": -141.024923}, {\"timesteps\": 1757, \"rewards\": 33.507485}, {\"timesteps\": 1758, \"rewards\": -90.825505}, {\"timesteps\": 1759, \"rewards\": -139.284144}, {\"timesteps\": 1760, \"rewards\": 270.600475}, {\"timesteps\": 1761, \"rewards\": -63.851104}, {\"timesteps\": 1762, \"rewards\": 166.746695}, {\"timesteps\": 1763, \"rewards\": -111.585959}, {\"timesteps\": 1764, \"rewards\": -137.335458}, {\"timesteps\": 1765, \"rewards\": 206.698902}, {\"timesteps\": 1766, \"rewards\": -194.621221}, {\"timesteps\": 1767, \"rewards\": -104.380615}, {\"timesteps\": 1768, \"rewards\": -83.711368}, {\"timesteps\": 1769, \"rewards\": -118.963644}, {\"timesteps\": 1770, \"rewards\": 168.422996}, {\"timesteps\": 1771, \"rewards\": -105.105174}, {\"timesteps\": 1772, \"rewards\": -191.58715}, {\"timesteps\": 1773, \"rewards\": -77.465686}, {\"timesteps\": 1774, \"rewards\": -217.383913}, {\"timesteps\": 1775, \"rewards\": -32.553077}, {\"timesteps\": 1776, \"rewards\": 158.31599}, {\"timesteps\": 1777, \"rewards\": 136.189972}, {\"timesteps\": 1778, \"rewards\": -196.201492}, {\"timesteps\": 1779, \"rewards\": -114.112643}, {\"timesteps\": 1780, \"rewards\": 240.346551}, {\"timesteps\": 1781, \"rewards\": 156.587839}, {\"timesteps\": 1782, \"rewards\": 198.272193}, {\"timesteps\": 1783, \"rewards\": -138.40301}, {\"timesteps\": 1784, \"rewards\": -159.719857}, {\"timesteps\": 1785, \"rewards\": -132.89652}, {\"timesteps\": 1786, \"rewards\": -220.30225}, {\"timesteps\": 1787, \"rewards\": -138.64446}, {\"timesteps\": 1788, \"rewards\": -132.334011}, {\"timesteps\": 1789, \"rewards\": 154.969995}, {\"timesteps\": 1790, \"rewards\": -207.334091}, {\"timesteps\": 1791, \"rewards\": -120.174185}, {\"timesteps\": 1792, \"rewards\": -128.916679}, {\"timesteps\": 1793, \"rewards\": -109.443316}, {\"timesteps\": 1794, \"rewards\": 101.194731}, {\"timesteps\": 1795, \"rewards\": -75.890396}, {\"timesteps\": 1796, \"rewards\": 202.347294}, {\"timesteps\": 1797, \"rewards\": -171.646151}, {\"timesteps\": 1798, \"rewards\": 157.633847}, {\"timesteps\": 1799, \"rewards\": 84.407966}, {\"timesteps\": 1800, \"rewards\": -9.403863}, {\"timesteps\": 1801, \"rewards\": 163.51936}, {\"timesteps\": 1802, \"rewards\": -158.682934}, {\"timesteps\": 1803, \"rewards\": -142.788852}, {\"timesteps\": 1804, \"rewards\": -104.536367}, {\"timesteps\": 1805, \"rewards\": 275.826654}, {\"timesteps\": 1806, \"rewards\": -100.17083}, {\"timesteps\": 1807, \"rewards\": -107.278194}, {\"timesteps\": 1808, \"rewards\": -132.551767}, {\"timesteps\": 1809, \"rewards\": -67.891109}, {\"timesteps\": 1810, \"rewards\": -119.848891}, {\"timesteps\": 1811, \"rewards\": -118.047177}, {\"timesteps\": 1812, \"rewards\": -101.61643}, {\"timesteps\": 1813, \"rewards\": -97.929327}, {\"timesteps\": 1814, \"rewards\": -134.343598}, {\"timesteps\": 1815, \"rewards\": -130.372549}, {\"timesteps\": 1816, \"rewards\": -226.099687}, {\"timesteps\": 1817, \"rewards\": -114.295191}, {\"timesteps\": 1818, \"rewards\": -95.705946}, {\"timesteps\": 1819, \"rewards\": -86.895038}, {\"timesteps\": 1820, \"rewards\": -202.460834}, {\"timesteps\": 1821, \"rewards\": -87.294826}, {\"timesteps\": 1822, \"rewards\": -128.067236}, {\"timesteps\": 1823, \"rewards\": -122.063569}, {\"timesteps\": 1824, \"rewards\": -197.122914}, {\"timesteps\": 1825, \"rewards\": -141.245776}, {\"timesteps\": 1826, \"rewards\": -91.85498}, {\"timesteps\": 1827, \"rewards\": -123.598826}, {\"timesteps\": 1828, \"rewards\": -91.618216}, {\"timesteps\": 1829, \"rewards\": -227.861997}, {\"timesteps\": 1830, \"rewards\": -102.964058}, {\"timesteps\": 1831, \"rewards\": -173.57019}, {\"timesteps\": 1832, \"rewards\": -73.700816}, {\"timesteps\": 1833, \"rewards\": -112.358619}, {\"timesteps\": 1834, \"rewards\": 138.762826}, {\"timesteps\": 1835, \"rewards\": -101.265021}, {\"timesteps\": 1836, \"rewards\": -95.412747}, {\"timesteps\": 1837, \"rewards\": -130.024105}, {\"timesteps\": 1838, \"rewards\": -44.825274}, {\"timesteps\": 1839, \"rewards\": 63.057074}, {\"timesteps\": 1840, \"rewards\": -96.568395}, {\"timesteps\": 1841, \"rewards\": -181.750939}, {\"timesteps\": 1842, \"rewards\": -208.630796}, {\"timesteps\": 1843, \"rewards\": -81.123737}, {\"timesteps\": 1844, \"rewards\": 158.399427}, {\"timesteps\": 1845, \"rewards\": -111.891671}, {\"timesteps\": 1846, \"rewards\": -100.045649}, {\"timesteps\": 1847, \"rewards\": -79.566797}, {\"timesteps\": 1848, \"rewards\": -81.052623}, {\"timesteps\": 1849, \"rewards\": -243.779541}, {\"timesteps\": 1850, \"rewards\": -112.624309}, {\"timesteps\": 1851, \"rewards\": -116.056389}, {\"timesteps\": 1852, \"rewards\": 79.412489}, {\"timesteps\": 1853, \"rewards\": -103.86415}, {\"timesteps\": 1854, \"rewards\": -110.640541}, {\"timesteps\": 1855, \"rewards\": -155.452098}, {\"timesteps\": 1856, \"rewards\": -118.846392}, {\"timesteps\": 1857, \"rewards\": -130.419416}, {\"timesteps\": 1858, \"rewards\": -139.856591}, {\"timesteps\": 1859, \"rewards\": -169.224613}, {\"timesteps\": 1860, \"rewards\": -158.255441}, {\"timesteps\": 1861, \"rewards\": -219.64634}, {\"timesteps\": 1862, \"rewards\": -181.902456}, {\"timesteps\": 1863, \"rewards\": -124.545897}, {\"timesteps\": 1864, \"rewards\": -205.835904}, {\"timesteps\": 1865, \"rewards\": -208.986173}, {\"timesteps\": 1866, \"rewards\": -144.591897}, {\"timesteps\": 1867, \"rewards\": -112.879396}, {\"timesteps\": 1868, \"rewards\": 101.786081}, {\"timesteps\": 1869, \"rewards\": -206.025052}, {\"timesteps\": 1870, \"rewards\": -119.94439}, {\"timesteps\": 1871, \"rewards\": -98.337712}, {\"timesteps\": 1872, \"rewards\": -126.392099}, {\"timesteps\": 1873, \"rewards\": -89.073029}, {\"timesteps\": 1874, \"rewards\": -92.977491}, {\"timesteps\": 1875, \"rewards\": -177.390134}, {\"timesteps\": 1876, \"rewards\": 59.354221}, {\"timesteps\": 1877, \"rewards\": -91.415106}, {\"timesteps\": 1878, \"rewards\": -107.610373}, {\"timesteps\": 1879, \"rewards\": -141.010835}, {\"timesteps\": 1880, \"rewards\": 158.964306}, {\"timesteps\": 1881, \"rewards\": 145.562327}, {\"timesteps\": 1882, \"rewards\": 132.53082}, {\"timesteps\": 1883, \"rewards\": -108.137195}, {\"timesteps\": 1884, \"rewards\": -198.939738}, {\"timesteps\": 1885, \"rewards\": 213.867338}, {\"timesteps\": 1886, \"rewards\": 34.077355}, {\"timesteps\": 1887, \"rewards\": -100.804086}, {\"timesteps\": 1888, \"rewards\": -100.932456}, {\"timesteps\": 1889, \"rewards\": -117.229903}, {\"timesteps\": 1890, \"rewards\": -163.61652}, {\"timesteps\": 1891, \"rewards\": -133.560003}, {\"timesteps\": 1892, \"rewards\": -222.392119}, {\"timesteps\": 1893, \"rewards\": -109.607552}, {\"timesteps\": 1894, \"rewards\": -98.298626}, {\"timesteps\": 1895, \"rewards\": -112.577728}, {\"timesteps\": 1896, \"rewards\": -80.488302}, {\"timesteps\": 1897, \"rewards\": -117.485947}, {\"timesteps\": 1898, \"rewards\": -95.520635}, {\"timesteps\": 1899, \"rewards\": -69.539062}, {\"timesteps\": 1900, \"rewards\": -134.91708}, {\"timesteps\": 1901, \"rewards\": -103.576746}, {\"timesteps\": 1902, \"rewards\": -134.530545}, {\"timesteps\": 1903, \"rewards\": -110.74199}, {\"timesteps\": 1904, \"rewards\": -211.929556}, {\"timesteps\": 1905, \"rewards\": -144.79986}, {\"timesteps\": 1906, \"rewards\": -98.503175}, {\"timesteps\": 1907, \"rewards\": -90.827493}, {\"timesteps\": 1908, \"rewards\": -107.33694}, {\"timesteps\": 1909, \"rewards\": -59.276606}, {\"timesteps\": 1910, \"rewards\": 136.531597}, {\"timesteps\": 1911, \"rewards\": -94.297592}, {\"timesteps\": 1912, \"rewards\": -87.73227}, {\"timesteps\": 1913, \"rewards\": -110.66427}, {\"timesteps\": 1914, \"rewards\": -72.556519}, {\"timesteps\": 1915, \"rewards\": -47.7848}, {\"timesteps\": 1916, \"rewards\": -123.835072}, {\"timesteps\": 1917, \"rewards\": -155.586216}, {\"timesteps\": 1918, \"rewards\": -110.5942}, {\"timesteps\": 1919, \"rewards\": -64.026035}, {\"timesteps\": 1920, \"rewards\": -8.763499}, {\"timesteps\": 1921, \"rewards\": -135.88336}, {\"timesteps\": 1922, \"rewards\": -96.204916}, {\"timesteps\": 1923, \"rewards\": 54.904091}, {\"timesteps\": 1924, \"rewards\": -123.901323}, {\"timesteps\": 1925, \"rewards\": -193.419079}, {\"timesteps\": 1926, \"rewards\": -102.076546}, {\"timesteps\": 1927, \"rewards\": -89.552903}, {\"timesteps\": 1928, \"rewards\": -55.165118}, {\"timesteps\": 1929, \"rewards\": -133.941832}, {\"timesteps\": 1930, \"rewards\": -162.504869}, {\"timesteps\": 1931, \"rewards\": -200.2695}, {\"timesteps\": 1932, \"rewards\": -113.120783}, {\"timesteps\": 1933, \"rewards\": -59.267287}, {\"timesteps\": 1934, \"rewards\": -88.599038}, {\"timesteps\": 1935, \"rewards\": -121.391604}, {\"timesteps\": 1936, \"rewards\": -74.682506}, {\"timesteps\": 1937, \"rewards\": -103.601449}, {\"timesteps\": 1938, \"rewards\": -131.375596}, {\"timesteps\": 1939, \"rewards\": -110.240398}, {\"timesteps\": 1940, \"rewards\": -105.026209}, {\"timesteps\": 1941, \"rewards\": -67.659895}, {\"timesteps\": 1942, \"rewards\": -77.281581}, {\"timesteps\": 1943, \"rewards\": -104.430605}, {\"timesteps\": 1944, \"rewards\": -146.970391}, {\"timesteps\": 1945, \"rewards\": -112.954191}, {\"timesteps\": 1946, \"rewards\": -95.518275}, {\"timesteps\": 1947, \"rewards\": 40.682543}, {\"timesteps\": 1948, \"rewards\": -123.645996}, {\"timesteps\": 1949, \"rewards\": -113.340888}, {\"timesteps\": 1950, \"rewards\": -89.392881}, {\"timesteps\": 1951, \"rewards\": 172.532117}, {\"timesteps\": 1952, \"rewards\": -190.763563}, {\"timesteps\": 1953, \"rewards\": -248.605631}, {\"timesteps\": 1954, \"rewards\": -141.279135}, {\"timesteps\": 1955, \"rewards\": 155.399376}, {\"timesteps\": 1956, \"rewards\": -135.781387}, {\"timesteps\": 1957, \"rewards\": -123.614025}, {\"timesteps\": 1958, \"rewards\": -96.50802}, {\"timesteps\": 1959, \"rewards\": -104.85465}, {\"timesteps\": 1960, \"rewards\": -134.722887}, {\"timesteps\": 1961, \"rewards\": -86.105823}, {\"timesteps\": 1962, \"rewards\": -104.302744}, {\"timesteps\": 1963, \"rewards\": -100.834333}, {\"timesteps\": 1964, \"rewards\": -128.546088}, {\"timesteps\": 1965, \"rewards\": -21.796579}, {\"timesteps\": 1966, \"rewards\": -130.953945}, {\"timesteps\": 1967, \"rewards\": -193.77307}, {\"timesteps\": 1968, \"rewards\": -92.355199}, {\"timesteps\": 1969, \"rewards\": -101.28062}, {\"timesteps\": 1970, \"rewards\": -106.056505}, {\"timesteps\": 1971, \"rewards\": -86.099839}, {\"timesteps\": 1972, \"rewards\": -109.287926}, {\"timesteps\": 1973, \"rewards\": -116.139084}, {\"timesteps\": 1974, \"rewards\": -113.936654}, {\"timesteps\": 1975, \"rewards\": -93.930968}, {\"timesteps\": 1976, \"rewards\": -103.698193}, {\"timesteps\": 1977, \"rewards\": -82.912993}, {\"timesteps\": 1978, \"rewards\": -128.872843}, {\"timesteps\": 1979, \"rewards\": -210.507662}, {\"timesteps\": 1980, \"rewards\": -93.33852}, {\"timesteps\": 1981, \"rewards\": -139.250732}, {\"timesteps\": 1982, \"rewards\": -30.977839}, {\"timesteps\": 1983, \"rewards\": -68.835449}, {\"timesteps\": 1984, \"rewards\": -77.389877}, {\"timesteps\": 1985, \"rewards\": -145.683548}, {\"timesteps\": 1986, \"rewards\": -75.272383}, {\"timesteps\": 1987, \"rewards\": -128.348369}, {\"timesteps\": 1988, \"rewards\": -75.784828}, {\"timesteps\": 1989, \"rewards\": -117.714031}, {\"timesteps\": 1990, \"rewards\": -127.165296}, {\"timesteps\": 1991, \"rewards\": -165.8208}, {\"timesteps\": 1992, \"rewards\": -167.027096}, {\"timesteps\": 1993, \"rewards\": 135.701863}, {\"timesteps\": 1994, \"rewards\": -82.312158}, {\"timesteps\": 1995, \"rewards\": 67.127368}, {\"timesteps\": 1996, \"rewards\": 103.20412}, {\"timesteps\": 1997, \"rewards\": -86.569737}, {\"timesteps\": 1998, \"rewards\": -69.634995}, {\"timesteps\": 1999, \"rewards\": 67.042411}, {\"timesteps\": 2000, \"rewards\": 215.637214}, {\"timesteps\": 2001, \"rewards\": -100.071827}, {\"timesteps\": 2002, \"rewards\": -203.445823}, {\"timesteps\": 2003, \"rewards\": -159.94477}, {\"timesteps\": 2004, \"rewards\": -149.898799}, {\"timesteps\": 2005, \"rewards\": -93.791583}, {\"timesteps\": 2006, \"rewards\": -140.246987}, {\"timesteps\": 2007, \"rewards\": -60.776198}, {\"timesteps\": 2008, \"rewards\": -97.26543}, {\"timesteps\": 2009, \"rewards\": -82.760315}, {\"timesteps\": 2010, \"rewards\": -111.184704}, {\"timesteps\": 2011, \"rewards\": -133.226756}, {\"timesteps\": 2012, \"rewards\": -101.294162}, {\"timesteps\": 2013, \"rewards\": -65.966204}, {\"timesteps\": 2014, \"rewards\": -78.212611}, {\"timesteps\": 2015, \"rewards\": -124.968212}, {\"timesteps\": 2016, \"rewards\": -98.831416}, {\"timesteps\": 2017, \"rewards\": -60.021511}, {\"timesteps\": 2018, \"rewards\": -91.132602}, {\"timesteps\": 2019, \"rewards\": -90.533046}, {\"timesteps\": 2020, \"rewards\": 178.698849}, {\"timesteps\": 2021, \"rewards\": -174.842053}, {\"timesteps\": 2022, \"rewards\": -183.145073}, {\"timesteps\": 2023, \"rewards\": -80.022472}, {\"timesteps\": 2024, \"rewards\": -103.659583}, {\"timesteps\": 2025, \"rewards\": -92.178073}, {\"timesteps\": 2026, \"rewards\": -93.745913}, {\"timesteps\": 2027, \"rewards\": -92.358275}, {\"timesteps\": 2028, \"rewards\": -80.127579}, {\"timesteps\": 2029, \"rewards\": -39.687962}, {\"timesteps\": 2030, \"rewards\": -119.798864}, {\"timesteps\": 2031, \"rewards\": -85.319182}, {\"timesteps\": 2032, \"rewards\": -61.76278}, {\"timesteps\": 2033, \"rewards\": -107.040241}, {\"timesteps\": 2034, \"rewards\": -98.993938}, {\"timesteps\": 2035, \"rewards\": -198.910378}, {\"timesteps\": 2036, \"rewards\": -136.721755}, {\"timesteps\": 2037, \"rewards\": -109.629851}, {\"timesteps\": 2038, \"rewards\": -118.525427}, {\"timesteps\": 2039, \"rewards\": -66.154378}, {\"timesteps\": 2040, \"rewards\": -148.668657}, {\"timesteps\": 2041, \"rewards\": -81.534306}, {\"timesteps\": 2042, \"rewards\": -80.990093}, {\"timesteps\": 2043, \"rewards\": -105.032232}, {\"timesteps\": 2044, \"rewards\": -97.949219}, {\"timesteps\": 2045, \"rewards\": -104.609078}, {\"timesteps\": 2046, \"rewards\": -105.628347}, {\"timesteps\": 2047, \"rewards\": -84.015504}, {\"timesteps\": 2048, \"rewards\": -56.789748}, {\"timesteps\": 2049, \"rewards\": -72.31389}, {\"timesteps\": 2050, \"rewards\": -172.036546}, {\"timesteps\": 2051, \"rewards\": -211.932899}, {\"timesteps\": 2052, \"rewards\": -90.137187}, {\"timesteps\": 2053, \"rewards\": -68.72423}, {\"timesteps\": 2054, \"rewards\": -58.863037}, {\"timesteps\": 2055, \"rewards\": -85.275296}, {\"timesteps\": 2056, \"rewards\": -82.545149}, {\"timesteps\": 2057, \"rewards\": -99.0942}, {\"timesteps\": 2058, \"rewards\": -68.162965}, {\"timesteps\": 2059, \"rewards\": -69.629357}, {\"timesteps\": 2060, \"rewards\": -180.490145}, {\"timesteps\": 2061, \"rewards\": -87.266559}, {\"timesteps\": 2062, \"rewards\": -85.891215}, {\"timesteps\": 2063, \"rewards\": -96.493872}, {\"timesteps\": 2064, \"rewards\": -111.811374}, {\"timesteps\": 2065, \"rewards\": -216.382052}, {\"timesteps\": 2066, \"rewards\": -88.460013}, {\"timesteps\": 2067, \"rewards\": -82.006588}, {\"timesteps\": 2068, \"rewards\": -86.812686}, {\"timesteps\": 2069, \"rewards\": -134.914677}, {\"timesteps\": 2070, \"rewards\": -153.305837}, {\"timesteps\": 2071, \"rewards\": -160.227579}, {\"timesteps\": 2072, \"rewards\": -82.027821}, {\"timesteps\": 2073, \"rewards\": -64.117195}, {\"timesteps\": 2074, \"rewards\": -104.672412}, {\"timesteps\": 2075, \"rewards\": -154.099224}, {\"timesteps\": 2076, \"rewards\": -117.742376}, {\"timesteps\": 2077, \"rewards\": -70.162676}, {\"timesteps\": 2078, \"rewards\": -80.135349}, {\"timesteps\": 2079, \"rewards\": -100.646334}, {\"timesteps\": 2080, \"rewards\": -89.843174}, {\"timesteps\": 2081, \"rewards\": -110.871302}, {\"timesteps\": 2082, \"rewards\": -115.638688}, {\"timesteps\": 2083, \"rewards\": -83.56938}, {\"timesteps\": 2084, \"rewards\": -164.549205}, {\"timesteps\": 2085, \"rewards\": -102.860968}, {\"timesteps\": 2086, \"rewards\": -75.015453}, {\"timesteps\": 2087, \"rewards\": -94.493697}, {\"timesteps\": 2088, \"rewards\": -70.406172}, {\"timesteps\": 2089, \"rewards\": -145.980184}, {\"timesteps\": 2090, \"rewards\": -88.719053}, {\"timesteps\": 2091, \"rewards\": -133.498833}, {\"timesteps\": 2092, \"rewards\": -98.299817}, {\"timesteps\": 2093, \"rewards\": -109.415699}, {\"timesteps\": 2094, \"rewards\": -111.376654}, {\"timesteps\": 2095, \"rewards\": -101.195934}, {\"timesteps\": 2096, \"rewards\": -104.375448}, {\"timesteps\": 2097, \"rewards\": -89.074427}, {\"timesteps\": 2098, \"rewards\": -127.996668}, {\"timesteps\": 2099, \"rewards\": -91.970921}, {\"timesteps\": 2100, \"rewards\": -207.607809}, {\"timesteps\": 2101, \"rewards\": -95.734596}, {\"timesteps\": 2102, \"rewards\": -124.691784}, {\"timesteps\": 2103, \"rewards\": -110.594}, {\"timesteps\": 2104, \"rewards\": -102.006072}, {\"timesteps\": 2105, \"rewards\": 209.942373}, {\"timesteps\": 2106, \"rewards\": -88.77795}, {\"timesteps\": 2107, \"rewards\": -64.002049}, {\"timesteps\": 2108, \"rewards\": -71.321986}, {\"timesteps\": 2109, \"rewards\": -87.069287}, {\"timesteps\": 2110, \"rewards\": -152.211003}, {\"timesteps\": 2111, \"rewards\": -172.650341}, {\"timesteps\": 2112, \"rewards\": -85.842265}, {\"timesteps\": 2113, \"rewards\": -77.685693}, {\"timesteps\": 2114, \"rewards\": -123.626386}, {\"timesteps\": 2115, \"rewards\": -152.933267}, {\"timesteps\": 2116, \"rewards\": -87.880447}, {\"timesteps\": 2117, \"rewards\": -89.40622}, {\"timesteps\": 2118, \"rewards\": -63.363385}, {\"timesteps\": 2119, \"rewards\": -112.512417}, {\"timesteps\": 2120, \"rewards\": -97.810333}, {\"timesteps\": 2121, \"rewards\": -104.305409}, {\"timesteps\": 2122, \"rewards\": -75.972273}, {\"timesteps\": 2123, \"rewards\": -43.26365}, {\"timesteps\": 2124, \"rewards\": -78.601147}, {\"timesteps\": 2125, \"rewards\": -180.907183}, {\"timesteps\": 2126, \"rewards\": -98.897971}, {\"timesteps\": 2127, \"rewards\": -61.236995}, {\"timesteps\": 2128, \"rewards\": -81.133888}, {\"timesteps\": 2129, \"rewards\": -59.91715}, {\"timesteps\": 2130, \"rewards\": -126.386059}, {\"timesteps\": 2131, \"rewards\": -154.870619}, {\"timesteps\": 2132, \"rewards\": -162.708112}, {\"timesteps\": 2133, \"rewards\": -66.921191}, {\"timesteps\": 2134, \"rewards\": -82.611555}, {\"timesteps\": 2135, \"rewards\": -148.342896}, {\"timesteps\": 2136, \"rewards\": -220.606199}, {\"timesteps\": 2137, \"rewards\": -62.319399}, {\"timesteps\": 2138, \"rewards\": -84.313938}, {\"timesteps\": 2139, \"rewards\": -70.655193}, {\"timesteps\": 2140, \"rewards\": -138.678615}, {\"timesteps\": 2141, \"rewards\": -87.648071}, {\"timesteps\": 2142, \"rewards\": -96.065516}, {\"timesteps\": 2143, \"rewards\": -54.086574}, {\"timesteps\": 2144, \"rewards\": -76.95841}, {\"timesteps\": 2145, \"rewards\": -189.455213}, {\"timesteps\": 2146, \"rewards\": -85.768295}, {\"timesteps\": 2147, \"rewards\": -124.109313}, {\"timesteps\": 2148, \"rewards\": -61.827528}, {\"timesteps\": 2149, \"rewards\": -116.852932}, {\"timesteps\": 2150, \"rewards\": -140.491211}, {\"timesteps\": 2151, \"rewards\": -89.323386}, {\"timesteps\": 2152, \"rewards\": -55.017018}, {\"timesteps\": 2153, \"rewards\": -103.193796}, {\"timesteps\": 2154, \"rewards\": -80.733473}, {\"timesteps\": 2155, \"rewards\": -186.80105}, {\"timesteps\": 2156, \"rewards\": -64.934396}, {\"timesteps\": 2157, \"rewards\": -108.410325}, {\"timesteps\": 2158, \"rewards\": -92.588389}, {\"timesteps\": 2159, \"rewards\": -104.534394}, {\"timesteps\": 2160, \"rewards\": -165.772087}, {\"timesteps\": 2161, \"rewards\": -114.301146}, {\"timesteps\": 2162, \"rewards\": -198.256671}, {\"timesteps\": 2163, \"rewards\": -109.999648}, {\"timesteps\": 2164, \"rewards\": -39.632243}, {\"timesteps\": 2165, \"rewards\": -133.710913}, {\"timesteps\": 2166, \"rewards\": -161.689742}, {\"timesteps\": 2167, \"rewards\": -74.364286}, {\"timesteps\": 2168, \"rewards\": -90.357228}, {\"timesteps\": 2169, \"rewards\": -64.267902}, {\"timesteps\": 2170, \"rewards\": -178.332099}, {\"timesteps\": 2171, \"rewards\": -118.904672}, {\"timesteps\": 2172, \"rewards\": -87.813986}, {\"timesteps\": 2173, \"rewards\": -73.907826}, {\"timesteps\": 2174, \"rewards\": -95.821216}, {\"timesteps\": 2175, \"rewards\": -153.990701}, {\"timesteps\": 2176, \"rewards\": -76.354615}, {\"timesteps\": 2177, \"rewards\": -54.338342}, {\"timesteps\": 2178, \"rewards\": -75.154917}, {\"timesteps\": 2179, \"rewards\": -110.527629}, {\"timesteps\": 2180, \"rewards\": -140.705304}, {\"timesteps\": 2181, \"rewards\": -105.076836}, {\"timesteps\": 2182, \"rewards\": -88.009316}, {\"timesteps\": 2183, \"rewards\": -111.074391}, {\"timesteps\": 2184, \"rewards\": -112.719513}, {\"timesteps\": 2185, \"rewards\": -111.021167}, {\"timesteps\": 2186, \"rewards\": -110.982275}, {\"timesteps\": 2187, \"rewards\": -74.901428}, {\"timesteps\": 2188, \"rewards\": -60.024166}, {\"timesteps\": 2189, \"rewards\": -85.686298}, {\"timesteps\": 2190, \"rewards\": -196.147206}, {\"timesteps\": 2191, \"rewards\": -93.258666}, {\"timesteps\": 2192, \"rewards\": -88.245013}, {\"timesteps\": 2193, \"rewards\": -79.61841}, {\"timesteps\": 2194, \"rewards\": -73.449589}, {\"timesteps\": 2195, \"rewards\": -160.941015}, {\"timesteps\": 2196, \"rewards\": -79.156415}, {\"timesteps\": 2197, \"rewards\": -70.283461}, {\"timesteps\": 2198, \"rewards\": -71.33068}, {\"timesteps\": 2199, \"rewards\": -112.989871}, {\"timesteps\": 2200, \"rewards\": -84.813554}, {\"timesteps\": 2201, \"rewards\": -112.035331}, {\"timesteps\": 2202, \"rewards\": 91.181417}, {\"timesteps\": 2203, \"rewards\": -125.555956}, {\"timesteps\": 2204, \"rewards\": -128.361656}, {\"timesteps\": 2205, \"rewards\": -117.328484}, {\"timesteps\": 2206, \"rewards\": -206.375872}, {\"timesteps\": 2207, \"rewards\": -190.863415}, {\"timesteps\": 2208, \"rewards\": -58.506105}, {\"timesteps\": 2209, \"rewards\": -59.547389}, {\"timesteps\": 2210, \"rewards\": -29.225854}, {\"timesteps\": 2211, \"rewards\": -123.761786}, {\"timesteps\": 2212, \"rewards\": -74.049896}, {\"timesteps\": 2213, \"rewards\": -105.383749}, {\"timesteps\": 2214, \"rewards\": -101.616089}, {\"timesteps\": 2215, \"rewards\": -178.342283}, {\"timesteps\": 2216, \"rewards\": -81.395298}, {\"timesteps\": 2217, \"rewards\": -51.792855}, {\"timesteps\": 2218, \"rewards\": -57.924581}, {\"timesteps\": 2219, \"rewards\": 168.876888}, {\"timesteps\": 2220, \"rewards\": -64.410951}, {\"timesteps\": 2221, \"rewards\": -79.144063}, {\"timesteps\": 2222, \"rewards\": -70.432139}, {\"timesteps\": 2223, \"rewards\": -62.772809}, {\"timesteps\": 2224, \"rewards\": -38.998125}, {\"timesteps\": 2225, \"rewards\": -54.950231}, {\"timesteps\": 2226, \"rewards\": -79.14251}, {\"timesteps\": 2227, \"rewards\": -43.445501}, {\"timesteps\": 2228, \"rewards\": -43.121974}, {\"timesteps\": 2229, \"rewards\": -101.446317}, {\"timesteps\": 2230, \"rewards\": -95.595632}, {\"timesteps\": 2231, \"rewards\": -66.447884}, {\"timesteps\": 2232, \"rewards\": -113.658067}, {\"timesteps\": 2233, \"rewards\": -100.186921}, {\"timesteps\": 2234, \"rewards\": -64.227186}, {\"timesteps\": 2235, \"rewards\": -191.157588}, {\"timesteps\": 2236, \"rewards\": -79.355163}, {\"timesteps\": 2237, \"rewards\": -84.941903}, {\"timesteps\": 2238, \"rewards\": -120.755619}, {\"timesteps\": 2239, \"rewards\": -82.218917}, {\"timesteps\": 2240, \"rewards\": -73.052038}, {\"timesteps\": 2241, \"rewards\": -39.942749}, {\"timesteps\": 2242, \"rewards\": -112.175254}, {\"timesteps\": 2243, \"rewards\": -97.759908}, {\"timesteps\": 2244, \"rewards\": -170.79568}, {\"timesteps\": 2245, \"rewards\": -89.195306}, {\"timesteps\": 2246, \"rewards\": -104.527363}, {\"timesteps\": 2247, \"rewards\": -78.973873}, {\"timesteps\": 2248, \"rewards\": -50.08431}, {\"timesteps\": 2249, \"rewards\": -66.54792}, {\"timesteps\": 2250, \"rewards\": -179.925245}, {\"timesteps\": 2251, \"rewards\": -99.961184}, {\"timesteps\": 2252, \"rewards\": -85.165632}, {\"timesteps\": 2253, \"rewards\": -59.416715}, {\"timesteps\": 2254, \"rewards\": -60.895054}, {\"timesteps\": 2255, \"rewards\": -73.641956}, {\"timesteps\": 2256, \"rewards\": -111.39411}, {\"timesteps\": 2257, \"rewards\": -79.36237}, {\"timesteps\": 2258, \"rewards\": -88.455212}, {\"timesteps\": 2259, \"rewards\": -113.842667}, {\"timesteps\": 2260, \"rewards\": -141.07871}, {\"timesteps\": 2261, \"rewards\": -27.371994}, {\"timesteps\": 2262, \"rewards\": -75.337048}, {\"timesteps\": 2263, \"rewards\": -34.336823}, {\"timesteps\": 2264, \"rewards\": -171.71133}, {\"timesteps\": 2265, \"rewards\": -83.102508}, {\"timesteps\": 2266, \"rewards\": -26.488607}, {\"timesteps\": 2267, \"rewards\": -93.354035}, {\"timesteps\": 2268, \"rewards\": -38.726213}, {\"timesteps\": 2269, \"rewards\": -96.422725}, {\"timesteps\": 2270, \"rewards\": -89.162859}, {\"timesteps\": 2271, \"rewards\": -110.701715}, {\"timesteps\": 2272, \"rewards\": -91.122257}, {\"timesteps\": 2273, \"rewards\": -93.16634}, {\"timesteps\": 2274, \"rewards\": -33.658183}, {\"timesteps\": 2275, \"rewards\": -84.735136}, {\"timesteps\": 2276, \"rewards\": -83.252193}, {\"timesteps\": 2277, \"rewards\": -96.291798}, {\"timesteps\": 2278, \"rewards\": -91.618049}, {\"timesteps\": 2279, \"rewards\": -106.165861}, {\"timesteps\": 2280, \"rewards\": -99.036798}, {\"timesteps\": 2281, \"rewards\": -87.992345}, {\"timesteps\": 2282, \"rewards\": -115.077504}, {\"timesteps\": 2283, \"rewards\": -112.267599}, {\"timesteps\": 2284, \"rewards\": -62.961728}, {\"timesteps\": 2285, \"rewards\": -80.076505}, {\"timesteps\": 2286, \"rewards\": -99.705209}, {\"timesteps\": 2287, \"rewards\": -77.739016}, {\"timesteps\": 2288, \"rewards\": -86.517218}, {\"timesteps\": 2289, \"rewards\": -59.593268}, {\"timesteps\": 2290, \"rewards\": -97.353216}, {\"timesteps\": 2291, \"rewards\": -63.229757}, {\"timesteps\": 2292, \"rewards\": -106.382021}, {\"timesteps\": 2293, \"rewards\": -82.267885}, {\"timesteps\": 2294, \"rewards\": -32.495612}, {\"timesteps\": 2295, \"rewards\": -81.005156}, {\"timesteps\": 2296, \"rewards\": -73.985447}, {\"timesteps\": 2297, \"rewards\": -101.192321}, {\"timesteps\": 2298, \"rewards\": -103.572389}, {\"timesteps\": 2299, \"rewards\": -77.893647}, {\"timesteps\": 2300, \"rewards\": -181.699731}, {\"timesteps\": 2301, \"rewards\": -192.754498}, {\"timesteps\": 2302, \"rewards\": -104.347963}, {\"timesteps\": 2303, \"rewards\": -65.558147}, {\"timesteps\": 2304, \"rewards\": -84.894471}, {\"timesteps\": 2305, \"rewards\": -95.620402}, {\"timesteps\": 2306, \"rewards\": -73.593415}, {\"timesteps\": 2307, \"rewards\": -69.618052}, {\"timesteps\": 2308, \"rewards\": -46.193659}, {\"timesteps\": 2309, \"rewards\": -82.632881}, {\"timesteps\": 2310, \"rewards\": -174.106479}, {\"timesteps\": 2311, \"rewards\": -62.909473}, {\"timesteps\": 2312, \"rewards\": -104.018419}, {\"timesteps\": 2313, \"rewards\": -101.066189}, {\"timesteps\": 2314, \"rewards\": -68.498107}, {\"timesteps\": 2315, \"rewards\": -46.336501}, {\"timesteps\": 2316, \"rewards\": -75.411131}, {\"timesteps\": 2317, \"rewards\": -84.944201}, {\"timesteps\": 2318, \"rewards\": -114.708585}, {\"timesteps\": 2319, \"rewards\": -116.326462}, {\"timesteps\": 2320, \"rewards\": -125.064628}, {\"timesteps\": 2321, \"rewards\": -73.985787}, {\"timesteps\": 2322, \"rewards\": -66.449035}, {\"timesteps\": 2323, \"rewards\": -57.408003}, {\"timesteps\": 2324, \"rewards\": -207.062972}, {\"timesteps\": 2325, \"rewards\": -159.191628}, {\"timesteps\": 2326, \"rewards\": -182.992908}, {\"timesteps\": 2327, \"rewards\": -86.649164}, {\"timesteps\": 2328, \"rewards\": -72.731881}, {\"timesteps\": 2329, \"rewards\": -83.668058}, {\"timesteps\": 2330, \"rewards\": -115.529557}, {\"timesteps\": 2331, \"rewards\": -99.700646}, {\"timesteps\": 2332, \"rewards\": -85.866938}, {\"timesteps\": 2333, \"rewards\": -114.37481}, {\"timesteps\": 2334, \"rewards\": -67.465677}, {\"timesteps\": 2335, \"rewards\": -66.010043}, {\"timesteps\": 2336, \"rewards\": -66.671716}, {\"timesteps\": 2337, \"rewards\": -76.716872}, {\"timesteps\": 2338, \"rewards\": -70.88818}, {\"timesteps\": 2339, \"rewards\": -115.970867}, {\"timesteps\": 2340, \"rewards\": -142.477907}, {\"timesteps\": 2341, \"rewards\": -112.357734}, {\"timesteps\": 2342, \"rewards\": -99.169959}, {\"timesteps\": 2343, \"rewards\": -138.954636}, {\"timesteps\": 2344, \"rewards\": -76.858954}, {\"timesteps\": 2345, \"rewards\": -102.149506}, {\"timesteps\": 2346, \"rewards\": -139.404479}, {\"timesteps\": 2347, \"rewards\": -111.537172}, {\"timesteps\": 2348, \"rewards\": -63.400803}, {\"timesteps\": 2349, \"rewards\": -82.700121}, {\"timesteps\": 2350, \"rewards\": -156.41749}, {\"timesteps\": 2351, \"rewards\": -83.99573}, {\"timesteps\": 2352, \"rewards\": -64.278901}, {\"timesteps\": 2353, \"rewards\": -105.405788}, {\"timesteps\": 2354, \"rewards\": -103.768309}, {\"timesteps\": 2355, \"rewards\": -122.641375}, {\"timesteps\": 2356, \"rewards\": -74.31502}, {\"timesteps\": 2357, \"rewards\": -84.330265}, {\"timesteps\": 2358, \"rewards\": -34.870159}, {\"timesteps\": 2359, \"rewards\": -105.326882}, {\"timesteps\": 2360, \"rewards\": -71.691095}, {\"timesteps\": 2361, \"rewards\": -116.161533}, {\"timesteps\": 2362, \"rewards\": -59.664216}, {\"timesteps\": 2363, \"rewards\": -85.278223}, {\"timesteps\": 2364, \"rewards\": -87.374327}, {\"timesteps\": 2365, \"rewards\": -106.371081}, {\"timesteps\": 2366, \"rewards\": -29.449643}, {\"timesteps\": 2367, \"rewards\": -26.10459}, {\"timesteps\": 2368, \"rewards\": -99.806613}, {\"timesteps\": 2369, \"rewards\": -77.531768}, {\"timesteps\": 2370, \"rewards\": -217.491571}, {\"timesteps\": 2371, \"rewards\": -59.194701}, {\"timesteps\": 2372, \"rewards\": -111.056607}, {\"timesteps\": 2373, \"rewards\": -114.826865}, {\"timesteps\": 2374, \"rewards\": -45.68192}, {\"timesteps\": 2375, \"rewards\": -161.055612}, {\"timesteps\": 2376, \"rewards\": -147.341326}, {\"timesteps\": 2377, \"rewards\": -175.976519}, {\"timesteps\": 2378, \"rewards\": -83.298603}, {\"timesteps\": 2379, \"rewards\": -79.897303}, {\"timesteps\": 2380, \"rewards\": -205.959127}, {\"timesteps\": 2381, \"rewards\": -59.686149}, {\"timesteps\": 2382, \"rewards\": -62.124856}, {\"timesteps\": 2383, \"rewards\": -6.250097}, {\"timesteps\": 2384, \"rewards\": -87.642838}, {\"timesteps\": 2385, \"rewards\": -69.743492}, {\"timesteps\": 2386, \"rewards\": -31.230577}, {\"timesteps\": 2387, \"rewards\": -51.045191}, {\"timesteps\": 2388, \"rewards\": -44.400405}, {\"timesteps\": 2389, \"rewards\": -114.148167}, {\"timesteps\": 2390, \"rewards\": -35.95096}, {\"timesteps\": 2391, \"rewards\": -80.771932}, {\"timesteps\": 2392, \"rewards\": -92.983186}, {\"timesteps\": 2393, \"rewards\": -24.909517}, {\"timesteps\": 2394, \"rewards\": -32.171342}, {\"timesteps\": 2395, \"rewards\": -198.614768}, {\"timesteps\": 2396, \"rewards\": -62.019537}, {\"timesteps\": 2397, \"rewards\": -70.624589}, {\"timesteps\": 2398, \"rewards\": -102.363778}, {\"timesteps\": 2399, \"rewards\": -87.640729}, {\"timesteps\": 2400, \"rewards\": -154.471316}, {\"timesteps\": 2401, \"rewards\": -81.446809}, {\"timesteps\": 2402, \"rewards\": -52.581947}, {\"timesteps\": 2403, \"rewards\": -53.63414}, {\"timesteps\": 2404, \"rewards\": -66.650068}, {\"timesteps\": 2405, \"rewards\": -41.421338}, {\"timesteps\": 2406, \"rewards\": -69.092601}, {\"timesteps\": 2407, \"rewards\": -65.882961}, {\"timesteps\": 2408, \"rewards\": -86.029141}, {\"timesteps\": 2409, \"rewards\": -60.48143}, {\"timesteps\": 2410, \"rewards\": -79.707155}, {\"timesteps\": 2411, \"rewards\": -55.784875}, {\"timesteps\": 2412, \"rewards\": -81.916884}, {\"timesteps\": 2413, \"rewards\": -96.978635}, {\"timesteps\": 2414, \"rewards\": -96.471374}, {\"timesteps\": 2415, \"rewards\": -71.476286}, {\"timesteps\": 2416, \"rewards\": -105.269322}, {\"timesteps\": 2417, \"rewards\": -67.96276}, {\"timesteps\": 2418, \"rewards\": -113.444431}, {\"timesteps\": 2419, \"rewards\": -66.302916}, {\"timesteps\": 2420, \"rewards\": -86.907578}, {\"timesteps\": 2421, \"rewards\": -121.130604}, {\"timesteps\": 2422, \"rewards\": -73.896649}, {\"timesteps\": 2423, \"rewards\": -52.138873}, {\"timesteps\": 2424, \"rewards\": -94.817066}, {\"timesteps\": 2425, \"rewards\": -62.309156}, {\"timesteps\": 2426, \"rewards\": -70.718759}, {\"timesteps\": 2427, \"rewards\": -114.776584}, {\"timesteps\": 2428, \"rewards\": -92.923281}, {\"timesteps\": 2429, \"rewards\": -78.756312}, {\"timesteps\": 2430, \"rewards\": -96.822254}, {\"timesteps\": 2431, \"rewards\": -82.881776}, {\"timesteps\": 2432, \"rewards\": -102.786552}, {\"timesteps\": 2433, \"rewards\": -81.702035}, {\"timesteps\": 2434, \"rewards\": -84.306123}, {\"timesteps\": 2435, \"rewards\": -97.38839}, {\"timesteps\": 2436, \"rewards\": -119.18886}, {\"timesteps\": 2437, \"rewards\": -138.960069}, {\"timesteps\": 2438, \"rewards\": -97.779047}, {\"timesteps\": 2439, \"rewards\": -59.766178}, {\"timesteps\": 2440, \"rewards\": -172.553033}, {\"timesteps\": 2441, \"rewards\": -101.527628}, {\"timesteps\": 2442, \"rewards\": -72.300055}, {\"timesteps\": 2443, \"rewards\": -53.114747}, {\"timesteps\": 2444, \"rewards\": -98.651392}, {\"timesteps\": 2445, \"rewards\": -145.333134}, {\"timesteps\": 2446, \"rewards\": -85.703234}, {\"timesteps\": 2447, \"rewards\": -107.109763}, {\"timesteps\": 2448, \"rewards\": -63.923997}, {\"timesteps\": 2449, \"rewards\": -74.380994}, {\"timesteps\": 2450, \"rewards\": -59.416686}, {\"timesteps\": 2451, \"rewards\": -66.427359}, {\"timesteps\": 2452, \"rewards\": -51.951568}, {\"timesteps\": 2453, \"rewards\": -83.73178}, {\"timesteps\": 2454, \"rewards\": -63.856119}, {\"timesteps\": 2455, \"rewards\": -64.784632}, {\"timesteps\": 2456, \"rewards\": -54.059021}, {\"timesteps\": 2457, \"rewards\": -88.850165}, {\"timesteps\": 2458, \"rewards\": -81.195879}, {\"timesteps\": 2459, \"rewards\": -84.801723}, {\"timesteps\": 2460, \"rewards\": -88.482485}, {\"timesteps\": 2461, \"rewards\": -122.384852}, {\"timesteps\": 2462, \"rewards\": -56.220135}, {\"timesteps\": 2463, \"rewards\": -82.362693}, {\"timesteps\": 2464, \"rewards\": -58.917036}, {\"timesteps\": 2465, \"rewards\": -80.38968}, {\"timesteps\": 2466, \"rewards\": -67.474588}, {\"timesteps\": 2467, \"rewards\": -87.338304}, {\"timesteps\": 2468, \"rewards\": -49.78076}, {\"timesteps\": 2469, \"rewards\": -112.387717}, {\"timesteps\": 2470, \"rewards\": -49.474855}, {\"timesteps\": 2471, \"rewards\": -71.452067}, {\"timesteps\": 2472, \"rewards\": -76.609185}, {\"timesteps\": 2473, \"rewards\": -94.99659}, {\"timesteps\": 2474, \"rewards\": -104.916323}, {\"timesteps\": 2475, \"rewards\": -51.53862}, {\"timesteps\": 2476, \"rewards\": -149.526645}, {\"timesteps\": 2477, \"rewards\": -56.956437}, {\"timesteps\": 2478, \"rewards\": -99.703202}, {\"timesteps\": 2479, \"rewards\": -51.961143}, {\"timesteps\": 2480, \"rewards\": -110.263796}, {\"timesteps\": 2481, \"rewards\": -93.414846}, {\"timesteps\": 2482, \"rewards\": -86.271563}, {\"timesteps\": 2483, \"rewards\": -57.695148}, {\"timesteps\": 2484, \"rewards\": -75.73842}, {\"timesteps\": 2485, \"rewards\": -83.892192}, {\"timesteps\": 2486, \"rewards\": -118.162114}, {\"timesteps\": 2487, \"rewards\": -113.191862}, {\"timesteps\": 2488, \"rewards\": -81.141425}, {\"timesteps\": 2489, \"rewards\": -68.289255}, {\"timesteps\": 2490, \"rewards\": -81.266094}, {\"timesteps\": 2491, \"rewards\": -79.377628}, {\"timesteps\": 2492, \"rewards\": -80.657285}, {\"timesteps\": 2493, \"rewards\": -64.263164}, {\"timesteps\": 2494, \"rewards\": -87.497959}, {\"timesteps\": 2495, \"rewards\": -58.820108}, {\"timesteps\": 2496, \"rewards\": -127.771307}, {\"timesteps\": 2497, \"rewards\": -95.883653}, {\"timesteps\": 2498, \"rewards\": -118.420742}, {\"timesteps\": 2499, \"rewards\": -101.273783}, {\"timesteps\": 2500, \"rewards\": -159.260718}, {\"timesteps\": 2501, \"rewards\": -75.937174}, {\"timesteps\": 2502, \"rewards\": -75.110424}, {\"timesteps\": 2503, \"rewards\": -106.556476}, {\"timesteps\": 2504, \"rewards\": -57.218612}, {\"timesteps\": 2505, \"rewards\": -65.047658}, {\"timesteps\": 2506, \"rewards\": -88.289604}, {\"timesteps\": 2507, \"rewards\": -96.517407}, {\"timesteps\": 2508, \"rewards\": -76.953933}, {\"timesteps\": 2509, \"rewards\": -75.435311}, {\"timesteps\": 2510, \"rewards\": -84.92735}, {\"timesteps\": 2511, \"rewards\": -99.52762}, {\"timesteps\": 2512, \"rewards\": -64.50417}, {\"timesteps\": 2513, \"rewards\": -60.717626}, {\"timesteps\": 2514, \"rewards\": -66.630665}], \"data-61cb47bb02dfc45b226a66d84b1aefc2\": [{\"timesteps\": 0, \"rewards\": -315.476646}, {\"timesteps\": 1, \"rewards\": -412.859727}, {\"timesteps\": 2, \"rewards\": -379.438579}, {\"timesteps\": 3, \"rewards\": -481.444647}, {\"timesteps\": 4, \"rewards\": -417.083945}, {\"timesteps\": 5, \"rewards\": -256.919649}, {\"timesteps\": 6, \"rewards\": -313.355101}, {\"timesteps\": 7, \"rewards\": -368.667657}, {\"timesteps\": 8, \"rewards\": -378.231738}, {\"timesteps\": 9, \"rewards\": -327.254184}, {\"timesteps\": 10, \"rewards\": -275.625392}, {\"timesteps\": 11, \"rewards\": -290.939908}, {\"timesteps\": 12, \"rewards\": -282.827517}, {\"timesteps\": 13, \"rewards\": -314.18052}, {\"timesteps\": 14, \"rewards\": -200.6319}, {\"timesteps\": 15, \"rewards\": -244.599378}, {\"timesteps\": 16, \"rewards\": -213.294386}, {\"timesteps\": 17, \"rewards\": -231.505079}, {\"timesteps\": 18, \"rewards\": -249.107222}, {\"timesteps\": 19, \"rewards\": -248.874675}, {\"timesteps\": 20, \"rewards\": -183.490965}, {\"timesteps\": 21, \"rewards\": -203.571167}, {\"timesteps\": 22, \"rewards\": -164.30366}, {\"timesteps\": 23, \"rewards\": 24.12843}, {\"timesteps\": 24, \"rewards\": -207.70964}, {\"timesteps\": 25, \"rewards\": -118.400649}, {\"timesteps\": 26, \"rewards\": -103.501872}, {\"timesteps\": 27, \"rewards\": -194.356668}, {\"timesteps\": 28, \"rewards\": -96.934476}, {\"timesteps\": 29, \"rewards\": -190.99447}, {\"timesteps\": 30, \"rewards\": -96.254905}, {\"timesteps\": 31, \"rewards\": -33.872996}, {\"timesteps\": 32, \"rewards\": -101.052674}, {\"timesteps\": 33, \"rewards\": -100.989091}, {\"timesteps\": 34, \"rewards\": -248.760791}, {\"timesteps\": 35, \"rewards\": -241.665487}, {\"timesteps\": 36, \"rewards\": -12.736158}, {\"timesteps\": 37, \"rewards\": -248.179855}, {\"timesteps\": 38, \"rewards\": -79.300243}, {\"timesteps\": 39, \"rewards\": -102.31486}, {\"timesteps\": 40, \"rewards\": -111.548471}, {\"timesteps\": 41, \"rewards\": -189.156626}, {\"timesteps\": 42, \"rewards\": -127.137853}, {\"timesteps\": 43, \"rewards\": -158.339387}, {\"timesteps\": 44, \"rewards\": -94.541053}, {\"timesteps\": 45, \"rewards\": -311.69926}, {\"timesteps\": 46, \"rewards\": -610.288074}, {\"timesteps\": 47, \"rewards\": -568.80404}, {\"timesteps\": 48, \"rewards\": -598.780463}, {\"timesteps\": 49, \"rewards\": -642.770073}, {\"timesteps\": 50, \"rewards\": -296.61533}, {\"timesteps\": 51, \"rewards\": -382.316228}, {\"timesteps\": 52, \"rewards\": -362.474918}, {\"timesteps\": 53, \"rewards\": -716.359048}, {\"timesteps\": 54, \"rewards\": -881.733541}, {\"timesteps\": 55, \"rewards\": -666.469754}, {\"timesteps\": 56, \"rewards\": -875.614128}, {\"timesteps\": 57, \"rewards\": -748.300304}, {\"timesteps\": 58, \"rewards\": -964.594101}, {\"timesteps\": 59, \"rewards\": -786.629078}, {\"timesteps\": 60, \"rewards\": -188.657238}, {\"timesteps\": 61, \"rewards\": -399.704248}, {\"timesteps\": 62, \"rewards\": -314.49379}, {\"timesteps\": 63, \"rewards\": -536.646842}, {\"timesteps\": 64, \"rewards\": -501.718018}, {\"timesteps\": 65, \"rewards\": -416.144437}, {\"timesteps\": 66, \"rewards\": -336.768692}, {\"timesteps\": 67, \"rewards\": -387.831756}, {\"timesteps\": 68, \"rewards\": -377.897493}, {\"timesteps\": 69, \"rewards\": -337.255115}, {\"timesteps\": 70, \"rewards\": -312.962296}, {\"timesteps\": 71, \"rewards\": -259.987216}, {\"timesteps\": 72, \"rewards\": -299.620456}, {\"timesteps\": 73, \"rewards\": -259.863795}, {\"timesteps\": 74, \"rewards\": -216.392056}, {\"timesteps\": 75, \"rewards\": -549.865509}, {\"timesteps\": 76, \"rewards\": -334.016145}, {\"timesteps\": 77, \"rewards\": -263.734302}, {\"timesteps\": 78, \"rewards\": -419.010743}, {\"timesteps\": 79, \"rewards\": -176.795997}, {\"timesteps\": 80, \"rewards\": -494.223711}, {\"timesteps\": 81, \"rewards\": -184.981587}, {\"timesteps\": 82, \"rewards\": -227.257337}, {\"timesteps\": 83, \"rewards\": -228.151151}, {\"timesteps\": 84, \"rewards\": -462.013348}, {\"timesteps\": 85, \"rewards\": -478.709297}, {\"timesteps\": 86, \"rewards\": -486.738149}, {\"timesteps\": 87, \"rewards\": -133.14596}, {\"timesteps\": 88, \"rewards\": -483.726521}, {\"timesteps\": 89, \"rewards\": -206.112586}, {\"timesteps\": 90, \"rewards\": -419.563573}, {\"timesteps\": 91, \"rewards\": -201.62103}, {\"timesteps\": 92, \"rewards\": -216.445342}, {\"timesteps\": 93, \"rewards\": -420.468551}, {\"timesteps\": 94, \"rewards\": -492.095744}, {\"timesteps\": 95, \"rewards\": -268.274382}, {\"timesteps\": 96, \"rewards\": -207.63958}, {\"timesteps\": 97, \"rewards\": -289.346178}, {\"timesteps\": 98, \"rewards\": -364.733163}, {\"timesteps\": 99, \"rewards\": -371.539666}, {\"timesteps\": 100, \"rewards\": -239.893296}, {\"timesteps\": 101, \"rewards\": -161.507253}, {\"timesteps\": 102, \"rewards\": -163.011976}, {\"timesteps\": 103, \"rewards\": -225.23354}, {\"timesteps\": 104, \"rewards\": -161.891901}, {\"timesteps\": 105, \"rewards\": -315.388972}, {\"timesteps\": 106, \"rewards\": -324.996473}, {\"timesteps\": 107, \"rewards\": -193.485778}, {\"timesteps\": 108, \"rewards\": -300.93049}, {\"timesteps\": 109, \"rewards\": -327.566567}, {\"timesteps\": 110, \"rewards\": -238.00285}, {\"timesteps\": 111, \"rewards\": -227.549219}, {\"timesteps\": 112, \"rewards\": -183.321242}, {\"timesteps\": 113, \"rewards\": -212.987606}, {\"timesteps\": 114, \"rewards\": -105.629884}, {\"timesteps\": 115, \"rewards\": -282.547039}, {\"timesteps\": 116, \"rewards\": -195.22839}, {\"timesteps\": 117, \"rewards\": -192.400473}, {\"timesteps\": 118, \"rewards\": -167.244058}, {\"timesteps\": 119, \"rewards\": -234.433772}, {\"timesteps\": 120, \"rewards\": -349.423114}, {\"timesteps\": 121, \"rewards\": -168.859315}, {\"timesteps\": 122, \"rewards\": -174.906609}, {\"timesteps\": 123, \"rewards\": -192.937072}, {\"timesteps\": 124, \"rewards\": -220.884515}, {\"timesteps\": 125, \"rewards\": -187.067198}, {\"timesteps\": 126, \"rewards\": -171.787653}, {\"timesteps\": 127, \"rewards\": -175.38606}, {\"timesteps\": 128, \"rewards\": -190.373383}, {\"timesteps\": 129, \"rewards\": -108.075098}, {\"timesteps\": 130, \"rewards\": -350.319512}, {\"timesteps\": 131, \"rewards\": -48.482466}, {\"timesteps\": 132, \"rewards\": -44.528682}, {\"timesteps\": 133, \"rewards\": -267.891925}, {\"timesteps\": 134, \"rewards\": -188.073067}, {\"timesteps\": 135, \"rewards\": -282.110962}, {\"timesteps\": 136, \"rewards\": -35.830784}, {\"timesteps\": 137, \"rewards\": -174.71101}, {\"timesteps\": 138, \"rewards\": -195.889581}, {\"timesteps\": 139, \"rewards\": -199.024267}, {\"timesteps\": 140, \"rewards\": -190.738162}, {\"timesteps\": 141, \"rewards\": -70.692479}, {\"timesteps\": 142, \"rewards\": -35.219165}, {\"timesteps\": 143, \"rewards\": -152.483013}, {\"timesteps\": 144, \"rewards\": -233.134782}, {\"timesteps\": 145, \"rewards\": -225.072443}, {\"timesteps\": 146, \"rewards\": -193.60548}, {\"timesteps\": 147, \"rewards\": -200.28868}, {\"timesteps\": 148, \"rewards\": -198.517674}, {\"timesteps\": 149, \"rewards\": -193.965122}, {\"timesteps\": 150, \"rewards\": -203.920306}, {\"timesteps\": 151, \"rewards\": -95.731159}, {\"timesteps\": 152, \"rewards\": -151.77828}, {\"timesteps\": 153, \"rewards\": -83.931769}, {\"timesteps\": 154, \"rewards\": -205.162162}, {\"timesteps\": 155, \"rewards\": -215.519488}, {\"timesteps\": 156, \"rewards\": -290.136401}, {\"timesteps\": 157, \"rewards\": -212.492807}, {\"timesteps\": 158, \"rewards\": -181.785589}, {\"timesteps\": 159, \"rewards\": -194.841973}, {\"timesteps\": 160, \"rewards\": -52.210536}, {\"timesteps\": 161, \"rewards\": -194.65085}, {\"timesteps\": 162, \"rewards\": -91.307566}, {\"timesteps\": 163, \"rewards\": -91.285962}, {\"timesteps\": 164, \"rewards\": -265.021148}, {\"timesteps\": 165, \"rewards\": -33.744255}, {\"timesteps\": 166, \"rewards\": -297.172854}, {\"timesteps\": 167, \"rewards\": -216.153194}, {\"timesteps\": 168, \"rewards\": -204.07557}, {\"timesteps\": 169, \"rewards\": -171.034796}, {\"timesteps\": 170, \"rewards\": -191.770152}, {\"timesteps\": 171, \"rewards\": -353.418634}, {\"timesteps\": 172, \"rewards\": -433.462137}, {\"timesteps\": 173, \"rewards\": -244.806282}, {\"timesteps\": 174, \"rewards\": -328.106556}, {\"timesteps\": 175, \"rewards\": -267.888982}, {\"timesteps\": 176, \"rewards\": -349.854723}, {\"timesteps\": 177, \"rewards\": -142.997354}, {\"timesteps\": 178, \"rewards\": 192.60224}, {\"timesteps\": 179, \"rewards\": -335.528965}, {\"timesteps\": 180, \"rewards\": -462.378863}, {\"timesteps\": 181, \"rewards\": -232.070223}, {\"timesteps\": 182, \"rewards\": -311.544528}, {\"timesteps\": 183, \"rewards\": -277.220255}, {\"timesteps\": 184, \"rewards\": -110.340368}, {\"timesteps\": 185, \"rewards\": -322.125204}, {\"timesteps\": 186, \"rewards\": -211.079849}, {\"timesteps\": 187, \"rewards\": -342.12078}, {\"timesteps\": 188, \"rewards\": -433.569125}, {\"timesteps\": 189, \"rewards\": -171.421136}, {\"timesteps\": 190, \"rewards\": -83.236057}, {\"timesteps\": 191, \"rewards\": -96.33631}, {\"timesteps\": 192, \"rewards\": -255.567374}, {\"timesteps\": 193, \"rewards\": -440.123026}, {\"timesteps\": 194, \"rewards\": -153.303723}, {\"timesteps\": 195, \"rewards\": -191.108657}, {\"timesteps\": 196, \"rewards\": -52.287738}, {\"timesteps\": 197, \"rewards\": -227.801205}, {\"timesteps\": 198, \"rewards\": -225.72561}, {\"timesteps\": 199, \"rewards\": -191.096054}, {\"timesteps\": 200, \"rewards\": -78.641626}, {\"timesteps\": 201, \"rewards\": 18.38764}, {\"timesteps\": 202, \"rewards\": -127.728273}, {\"timesteps\": 203, \"rewards\": -86.848907}, {\"timesteps\": 204, \"rewards\": -321.585028}, {\"timesteps\": 205, \"rewards\": -134.553455}, {\"timesteps\": 206, \"rewards\": -213.052232}, {\"timesteps\": 207, \"rewards\": -59.993464}, {\"timesteps\": 208, \"rewards\": -106.979618}, {\"timesteps\": 209, \"rewards\": -213.129587}, {\"timesteps\": 210, \"rewards\": -399.949978}, {\"timesteps\": 211, \"rewards\": -107.582207}, {\"timesteps\": 212, \"rewards\": -376.06036}, {\"timesteps\": 213, \"rewards\": -192.203686}, {\"timesteps\": 214, \"rewards\": -35.933822}, {\"timesteps\": 215, \"rewards\": -344.033103}, {\"timesteps\": 216, \"rewards\": -188.057806}, {\"timesteps\": 217, \"rewards\": -339.024813}, {\"timesteps\": 218, \"rewards\": -272.218359}, {\"timesteps\": 219, \"rewards\": -254.671171}, {\"timesteps\": 220, \"rewards\": -12.631901}, {\"timesteps\": 221, \"rewards\": -373.083881}, {\"timesteps\": 222, \"rewards\": -302.530816}, {\"timesteps\": 223, \"rewards\": -263.866388}, {\"timesteps\": 224, \"rewards\": -243.096711}, {\"timesteps\": 225, \"rewards\": -310.458002}, {\"timesteps\": 226, \"rewards\": -244.257407}, {\"timesteps\": 227, \"rewards\": -283.61679}, {\"timesteps\": 228, \"rewards\": -283.453551}, {\"timesteps\": 229, \"rewards\": 215.389298}, {\"timesteps\": 230, \"rewards\": -285.969487}, {\"timesteps\": 231, \"rewards\": -466.943034}, {\"timesteps\": 232, \"rewards\": -262.21501}, {\"timesteps\": 233, \"rewards\": -364.579133}, {\"timesteps\": 234, \"rewards\": -190.945741}, {\"timesteps\": 235, \"rewards\": -51.74399}, {\"timesteps\": 236, \"rewards\": -41.274533}, {\"timesteps\": 237, \"rewards\": -265.527578}, {\"timesteps\": 238, \"rewards\": -302.905496}, {\"timesteps\": 239, \"rewards\": -97.950534}, {\"timesteps\": 240, \"rewards\": -73.833329}, {\"timesteps\": 241, \"rewards\": -281.538465}, {\"timesteps\": 242, \"rewards\": -183.146782}, {\"timesteps\": 243, \"rewards\": -70.850415}, {\"timesteps\": 244, \"rewards\": 8.726767}, {\"timesteps\": 245, \"rewards\": -300.243352}, {\"timesteps\": 246, \"rewards\": -113.177616}, {\"timesteps\": 247, \"rewards\": -243.716983}, {\"timesteps\": 248, \"rewards\": -558.988227}, {\"timesteps\": 249, \"rewards\": -204.978243}, {\"timesteps\": 250, \"rewards\": -202.183278}, {\"timesteps\": 251, \"rewards\": -494.492479}, {\"timesteps\": 252, \"rewards\": -363.56068}, {\"timesteps\": 253, \"rewards\": -324.169399}, {\"timesteps\": 254, \"rewards\": -401.850935}, {\"timesteps\": 255, \"rewards\": -126.834502}, {\"timesteps\": 256, \"rewards\": -517.574334}, {\"timesteps\": 257, \"rewards\": -295.392335}, {\"timesteps\": 258, \"rewards\": -345.377527}, {\"timesteps\": 259, \"rewards\": -228.27577}, {\"timesteps\": 260, \"rewards\": -426.5114}, {\"timesteps\": 261, \"rewards\": -133.043703}, {\"timesteps\": 262, \"rewards\": -143.007129}, {\"timesteps\": 263, \"rewards\": -281.203136}, {\"timesteps\": 264, \"rewards\": -300.04448}, {\"timesteps\": 265, \"rewards\": -492.641577}, {\"timesteps\": 266, \"rewards\": -83.382133}, {\"timesteps\": 267, \"rewards\": -307.012276}, {\"timesteps\": 268, \"rewards\": -337.417016}, {\"timesteps\": 269, \"rewards\": -83.87156}, {\"timesteps\": 270, \"rewards\": -523.289498}, {\"timesteps\": 271, \"rewards\": -80.697728}, {\"timesteps\": 272, \"rewards\": -80.42466}, {\"timesteps\": 273, \"rewards\": -212.673432}, {\"timesteps\": 274, \"rewards\": -327.263921}, {\"timesteps\": 275, \"rewards\": -33.112086}, {\"timesteps\": 276, \"rewards\": -81.44028}, {\"timesteps\": 277, \"rewards\": -159.82848}, {\"timesteps\": 278, \"rewards\": -307.569777}, {\"timesteps\": 279, \"rewards\": -288.288912}, {\"timesteps\": 280, \"rewards\": -281.634365}, {\"timesteps\": 281, \"rewards\": -55.556868}, {\"timesteps\": 282, \"rewards\": -195.110462}, {\"timesteps\": 283, \"rewards\": -529.495063}, {\"timesteps\": 284, \"rewards\": -275.723337}, {\"timesteps\": 285, \"rewards\": -95.878353}, {\"timesteps\": 286, \"rewards\": -114.323521}, {\"timesteps\": 287, \"rewards\": -113.390288}, {\"timesteps\": 288, \"rewards\": -480.251198}, {\"timesteps\": 289, \"rewards\": -102.810317}, {\"timesteps\": 290, \"rewards\": -481.840744}, {\"timesteps\": 291, \"rewards\": -224.994275}, {\"timesteps\": 292, \"rewards\": -404.980374}, {\"timesteps\": 293, \"rewards\": -298.813834}, {\"timesteps\": 294, \"rewards\": -354.318611}, {\"timesteps\": 295, \"rewards\": -275.413525}, {\"timesteps\": 296, \"rewards\": -182.46359}, {\"timesteps\": 297, \"rewards\": -373.214256}, {\"timesteps\": 298, \"rewards\": -301.555547}, {\"timesteps\": 299, \"rewards\": -510.114986}, {\"timesteps\": 300, \"rewards\": -143.109834}, {\"timesteps\": 301, \"rewards\": -218.401075}, {\"timesteps\": 302, \"rewards\": -199.145092}, {\"timesteps\": 303, \"rewards\": -438.543209}, {\"timesteps\": 304, \"rewards\": -238.987067}, {\"timesteps\": 305, \"rewards\": -333.706992}, {\"timesteps\": 306, \"rewards\": -157.364131}, {\"timesteps\": 307, \"rewards\": -325.225533}, {\"timesteps\": 308, \"rewards\": -398.589697}, {\"timesteps\": 309, \"rewards\": -219.454726}, {\"timesteps\": 310, \"rewards\": -126.87161}, {\"timesteps\": 311, \"rewards\": -24.842939}, {\"timesteps\": 312, \"rewards\": -297.764533}, {\"timesteps\": 313, \"rewards\": -335.853342}, {\"timesteps\": 314, \"rewards\": -294.319681}, {\"timesteps\": 315, \"rewards\": -286.843601}, {\"timesteps\": 316, \"rewards\": -311.914573}, {\"timesteps\": 317, \"rewards\": -274.849348}, {\"timesteps\": 318, \"rewards\": -303.952637}, {\"timesteps\": 319, \"rewards\": -225.50856}, {\"timesteps\": 320, \"rewards\": -293.865675}, {\"timesteps\": 321, \"rewards\": -368.03647}, {\"timesteps\": 322, \"rewards\": -203.172225}, {\"timesteps\": 323, \"rewards\": -179.309338}, {\"timesteps\": 324, \"rewards\": -448.701465}, {\"timesteps\": 325, \"rewards\": -56.904916}, {\"timesteps\": 326, \"rewards\": -179.482207}, {\"timesteps\": 327, \"rewards\": -274.392175}, {\"timesteps\": 328, \"rewards\": -86.575526}, {\"timesteps\": 329, \"rewards\": -142.557339}, {\"timesteps\": 330, \"rewards\": -444.201932}, {\"timesteps\": 331, \"rewards\": -446.826385}, {\"timesteps\": 332, \"rewards\": -136.908163}, {\"timesteps\": 333, \"rewards\": -365.208362}, {\"timesteps\": 334, \"rewards\": -178.273683}, {\"timesteps\": 335, \"rewards\": -70.087514}, {\"timesteps\": 336, \"rewards\": -150.129714}, {\"timesteps\": 337, \"rewards\": 177.996001}, {\"timesteps\": 338, \"rewards\": -311.811072}, {\"timesteps\": 339, \"rewards\": -111.605571}, {\"timesteps\": 340, \"rewards\": -202.58916}, {\"timesteps\": 341, \"rewards\": -116.590149}, {\"timesteps\": 342, \"rewards\": -250.324459}, {\"timesteps\": 343, \"rewards\": -527.956345}, {\"timesteps\": 344, \"rewards\": -142.942443}, {\"timesteps\": 345, \"rewards\": -169.83201}, {\"timesteps\": 346, \"rewards\": 211.26837}, {\"timesteps\": 347, \"rewards\": -301.860339}, {\"timesteps\": 348, \"rewards\": -62.125036}, {\"timesteps\": 349, \"rewards\": -54.277768}, {\"timesteps\": 350, \"rewards\": -103.778759}, {\"timesteps\": 351, \"rewards\": -105.862152}, {\"timesteps\": 352, \"rewards\": -228.000729}, {\"timesteps\": 353, \"rewards\": -95.727816}, {\"timesteps\": 354, \"rewards\": -133.238296}, {\"timesteps\": 355, \"rewards\": -119.053909}, {\"timesteps\": 356, \"rewards\": -58.604526}, {\"timesteps\": 357, \"rewards\": -9.797868}, {\"timesteps\": 358, \"rewards\": -214.225232}, {\"timesteps\": 359, \"rewards\": -68.965477}, {\"timesteps\": 360, \"rewards\": -87.064023}, {\"timesteps\": 361, \"rewards\": -34.050407}, {\"timesteps\": 362, \"rewards\": -76.975633}, {\"timesteps\": 363, \"rewards\": 216.651293}, {\"timesteps\": 364, \"rewards\": -62.090995}, {\"timesteps\": 365, \"rewards\": -231.04349}, {\"timesteps\": 366, \"rewards\": -183.837953}, {\"timesteps\": 367, \"rewards\": -58.060312}, {\"timesteps\": 368, \"rewards\": -248.627808}, {\"timesteps\": 369, \"rewards\": -193.456808}, {\"timesteps\": 370, \"rewards\": -279.03213}, {\"timesteps\": 371, \"rewards\": -504.954034}, {\"timesteps\": 372, \"rewards\": -266.225029}, {\"timesteps\": 373, \"rewards\": -163.867807}, {\"timesteps\": 374, \"rewards\": -293.461734}, {\"timesteps\": 375, \"rewards\": -182.8631}, {\"timesteps\": 376, \"rewards\": -116.416033}, {\"timesteps\": 377, \"rewards\": -98.620289}, {\"timesteps\": 378, \"rewards\": -295.827017}, {\"timesteps\": 379, \"rewards\": -72.294105}, {\"timesteps\": 380, \"rewards\": -139.263883}, {\"timesteps\": 381, \"rewards\": -184.095731}, {\"timesteps\": 382, \"rewards\": -133.064415}, {\"timesteps\": 383, \"rewards\": -254.928868}, {\"timesteps\": 384, \"rewards\": -307.128805}, {\"timesteps\": 385, \"rewards\": -80.305989}, {\"timesteps\": 386, \"rewards\": -49.704442}, {\"timesteps\": 387, \"rewards\": -335.279015}, {\"timesteps\": 388, \"rewards\": -99.656843}, {\"timesteps\": 389, \"rewards\": -174.916319}, {\"timesteps\": 390, \"rewards\": -195.401351}, {\"timesteps\": 391, \"rewards\": -208.118477}, {\"timesteps\": 392, \"rewards\": -192.624118}, {\"timesteps\": 393, \"rewards\": -56.429538}, {\"timesteps\": 394, \"rewards\": -295.237898}, {\"timesteps\": 395, \"rewards\": -76.008264}, {\"timesteps\": 396, \"rewards\": -45.367085}, {\"timesteps\": 397, \"rewards\": -218.041648}, {\"timesteps\": 398, \"rewards\": -106.182438}, {\"timesteps\": 399, \"rewards\": -435.175906}, {\"timesteps\": 400, \"rewards\": -267.870858}, {\"timesteps\": 401, \"rewards\": -118.280555}, {\"timesteps\": 402, \"rewards\": -199.66374}, {\"timesteps\": 403, \"rewards\": -42.053651}, {\"timesteps\": 404, \"rewards\": -246.885354}, {\"timesteps\": 405, \"rewards\": -231.698384}, {\"timesteps\": 406, \"rewards\": -105.347733}, {\"timesteps\": 407, \"rewards\": -118.850196}, {\"timesteps\": 408, \"rewards\": -506.484139}, {\"timesteps\": 409, \"rewards\": -193.637235}, {\"timesteps\": 410, \"rewards\": -140.963625}, {\"timesteps\": 411, \"rewards\": -339.134517}, {\"timesteps\": 412, \"rewards\": -432.418544}, {\"timesteps\": 413, \"rewards\": -248.938827}, {\"timesteps\": 414, \"rewards\": -436.000111}, {\"timesteps\": 415, \"rewards\": -464.644684}, {\"timesteps\": 416, \"rewards\": -112.230854}, {\"timesteps\": 417, \"rewards\": -164.915215}, {\"timesteps\": 418, \"rewards\": -446.390219}, {\"timesteps\": 419, \"rewards\": -105.957688}, {\"timesteps\": 420, \"rewards\": -486.990484}, {\"timesteps\": 421, \"rewards\": -127.876037}, {\"timesteps\": 422, \"rewards\": -357.38805}, {\"timesteps\": 423, \"rewards\": -244.027514}, {\"timesteps\": 424, \"rewards\": -136.502392}, {\"timesteps\": 425, \"rewards\": -229.971384}, {\"timesteps\": 426, \"rewards\": -83.991732}, {\"timesteps\": 427, \"rewards\": -135.449176}, {\"timesteps\": 428, \"rewards\": -425.600981}, {\"timesteps\": 429, \"rewards\": -238.567839}, {\"timesteps\": 430, \"rewards\": -516.847249}, {\"timesteps\": 431, \"rewards\": -244.525923}, {\"timesteps\": 432, \"rewards\": -255.072034}, {\"timesteps\": 433, \"rewards\": -76.992676}, {\"timesteps\": 434, \"rewards\": -339.676751}, {\"timesteps\": 435, \"rewards\": -83.937449}, {\"timesteps\": 436, \"rewards\": -172.331019}, {\"timesteps\": 437, \"rewards\": -174.105102}, {\"timesteps\": 438, \"rewards\": 238.505249}, {\"timesteps\": 439, \"rewards\": -281.454746}, {\"timesteps\": 440, \"rewards\": -184.532378}, {\"timesteps\": 441, \"rewards\": -116.348341}, {\"timesteps\": 442, \"rewards\": -462.448076}, {\"timesteps\": 443, \"rewards\": -334.891349}, {\"timesteps\": 444, \"rewards\": -329.785314}, {\"timesteps\": 445, \"rewards\": -171.203621}, {\"timesteps\": 446, \"rewards\": -306.432473}, {\"timesteps\": 447, \"rewards\": -153.78117}, {\"timesteps\": 448, \"rewards\": -280.751486}, {\"timesteps\": 449, \"rewards\": -298.121884}, {\"timesteps\": 450, \"rewards\": -233.755134}, {\"timesteps\": 451, \"rewards\": -295.944553}, {\"timesteps\": 452, \"rewards\": -219.058858}, {\"timesteps\": 453, \"rewards\": -230.315829}, {\"timesteps\": 454, \"rewards\": -316.743491}, {\"timesteps\": 455, \"rewards\": -224.914401}, {\"timesteps\": 456, \"rewards\": -257.814082}, {\"timesteps\": 457, \"rewards\": -185.76233}, {\"timesteps\": 458, \"rewards\": -187.700867}, {\"timesteps\": 459, \"rewards\": -176.559412}, {\"timesteps\": 460, \"rewards\": -145.69851}, {\"timesteps\": 461, \"rewards\": -169.494875}, {\"timesteps\": 462, \"rewards\": -445.113673}, {\"timesteps\": 463, \"rewards\": -129.718189}, {\"timesteps\": 464, \"rewards\": -296.297546}, {\"timesteps\": 465, \"rewards\": -153.094698}, {\"timesteps\": 466, \"rewards\": -251.072769}, {\"timesteps\": 467, \"rewards\": -184.462669}, {\"timesteps\": 468, \"rewards\": -203.738615}, {\"timesteps\": 469, \"rewards\": -265.450423}, {\"timesteps\": 470, \"rewards\": -252.291118}, {\"timesteps\": 471, \"rewards\": -373.086448}, {\"timesteps\": 472, \"rewards\": -199.978476}, {\"timesteps\": 473, \"rewards\": -348.690967}, {\"timesteps\": 474, \"rewards\": -97.809607}, {\"timesteps\": 475, \"rewards\": -309.764294}, {\"timesteps\": 476, \"rewards\": -106.637915}, {\"timesteps\": 477, \"rewards\": -144.377697}, {\"timesteps\": 478, \"rewards\": -208.370163}, {\"timesteps\": 479, \"rewards\": 20.165891}, {\"timesteps\": 480, \"rewards\": -260.153689}, {\"timesteps\": 481, \"rewards\": -240.087342}, {\"timesteps\": 482, \"rewards\": -239.829127}, {\"timesteps\": 483, \"rewards\": -268.176765}, {\"timesteps\": 484, \"rewards\": 122.407796}, {\"timesteps\": 485, \"rewards\": -218.542808}, {\"timesteps\": 486, \"rewards\": -205.418272}, {\"timesteps\": 487, \"rewards\": -294.35845}, {\"timesteps\": 488, \"rewards\": -263.376496}, {\"timesteps\": 489, \"rewards\": -316.217536}, {\"timesteps\": 490, \"rewards\": -219.017287}, {\"timesteps\": 491, \"rewards\": -214.771907}, {\"timesteps\": 492, \"rewards\": -244.922347}, {\"timesteps\": 493, \"rewards\": -294.605054}, {\"timesteps\": 494, \"rewards\": -247.909435}, {\"timesteps\": 495, \"rewards\": -257.084166}, {\"timesteps\": 496, \"rewards\": -235.840376}, {\"timesteps\": 497, \"rewards\": -313.911661}, {\"timesteps\": 498, \"rewards\": -156.128805}, {\"timesteps\": 499, \"rewards\": -306.643539}, {\"timesteps\": 500, \"rewards\": -351.618607}, {\"timesteps\": 501, \"rewards\": -286.511257}, {\"timesteps\": 502, \"rewards\": -366.73495}, {\"timesteps\": 503, \"rewards\": -271.111444}, {\"timesteps\": 504, \"rewards\": -244.304698}, {\"timesteps\": 505, \"rewards\": -437.954556}, {\"timesteps\": 506, \"rewards\": -140.956515}, {\"timesteps\": 507, \"rewards\": -249.2526}, {\"timesteps\": 508, \"rewards\": -251.247067}, {\"timesteps\": 509, \"rewards\": -269.583964}, {\"timesteps\": 510, \"rewards\": -466.271686}, {\"timesteps\": 511, \"rewards\": -463.206732}, {\"timesteps\": 512, \"rewards\": -195.672796}, {\"timesteps\": 513, \"rewards\": -418.650285}, {\"timesteps\": 514, \"rewards\": -2.342187}, {\"timesteps\": 515, \"rewards\": -218.042766}, {\"timesteps\": 516, \"rewards\": -226.017994}, {\"timesteps\": 517, \"rewards\": -218.450016}, {\"timesteps\": 518, \"rewards\": -715.686201}, {\"timesteps\": 519, \"rewards\": -345.779693}, {\"timesteps\": 520, \"rewards\": -223.189394}, {\"timesteps\": 521, \"rewards\": -506.490221}, {\"timesteps\": 522, \"rewards\": -348.997231}, {\"timesteps\": 523, \"rewards\": -181.951094}, {\"timesteps\": 524, \"rewards\": -276.323456}, {\"timesteps\": 525, \"rewards\": -536.229603}, {\"timesteps\": 526, \"rewards\": -192.215811}, {\"timesteps\": 527, \"rewards\": -326.417422}, {\"timesteps\": 528, \"rewards\": -284.13503}, {\"timesteps\": 529, \"rewards\": -321.778187}, {\"timesteps\": 530, \"rewards\": -370.715378}, {\"timesteps\": 531, \"rewards\": -75.932681}, {\"timesteps\": 532, \"rewards\": -292.441971}, {\"timesteps\": 533, \"rewards\": -421.094021}, {\"timesteps\": 534, \"rewards\": -465.991263}, {\"timesteps\": 535, \"rewards\": -435.926217}, {\"timesteps\": 536, \"rewards\": -221.326829}, {\"timesteps\": 537, \"rewards\": -159.901249}, {\"timesteps\": 538, \"rewards\": -238.365997}, {\"timesteps\": 539, \"rewards\": -483.080462}, {\"timesteps\": 540, \"rewards\": -393.07056}, {\"timesteps\": 541, \"rewards\": -222.702068}, {\"timesteps\": 542, \"rewards\": -447.120212}, {\"timesteps\": 543, \"rewards\": -443.85447}, {\"timesteps\": 544, \"rewards\": -469.425854}, {\"timesteps\": 545, \"rewards\": -286.40838}, {\"timesteps\": 546, \"rewards\": -148.565572}, {\"timesteps\": 547, \"rewards\": -480.954992}, {\"timesteps\": 548, \"rewards\": -334.479773}, {\"timesteps\": 549, \"rewards\": -275.499275}, {\"timesteps\": 550, \"rewards\": -270.814082}, {\"timesteps\": 551, \"rewards\": -251.775345}, {\"timesteps\": 552, \"rewards\": -540.12076}, {\"timesteps\": 553, \"rewards\": -261.616982}, {\"timesteps\": 554, \"rewards\": -327.371838}, {\"timesteps\": 555, \"rewards\": -313.739551}, {\"timesteps\": 556, \"rewards\": -345.132788}, {\"timesteps\": 557, \"rewards\": -243.78605}, {\"timesteps\": 558, \"rewards\": -47.491882}, {\"timesteps\": 559, \"rewards\": -545.675976}, {\"timesteps\": 560, \"rewards\": -454.067607}, {\"timesteps\": 561, \"rewards\": -431.924356}, {\"timesteps\": 562, \"rewards\": -379.173283}, {\"timesteps\": 563, \"rewards\": -576.023632}, {\"timesteps\": 564, \"rewards\": -358.334038}, {\"timesteps\": 565, \"rewards\": -692.483224}, {\"timesteps\": 566, \"rewards\": -478.511072}, {\"timesteps\": 567, \"rewards\": -269.774325}, {\"timesteps\": 568, \"rewards\": -353.046583}, {\"timesteps\": 569, \"rewards\": -470.739787}, {\"timesteps\": 570, \"rewards\": -111.971202}, {\"timesteps\": 571, \"rewards\": -393.910355}, {\"timesteps\": 572, \"rewards\": -375.217759}, {\"timesteps\": 573, \"rewards\": -421.423534}, {\"timesteps\": 574, \"rewards\": -478.278811}, {\"timesteps\": 575, \"rewards\": -579.243329}, {\"timesteps\": 576, \"rewards\": -196.520243}, {\"timesteps\": 577, \"rewards\": -256.249254}, {\"timesteps\": 578, \"rewards\": -255.73736}, {\"timesteps\": 579, \"rewards\": -392.392226}, {\"timesteps\": 580, \"rewards\": -375.032512}, {\"timesteps\": 581, \"rewards\": -291.746381}, {\"timesteps\": 582, \"rewards\": -473.870283}, {\"timesteps\": 583, \"rewards\": -498.945151}, {\"timesteps\": 584, \"rewards\": -227.306841}, {\"timesteps\": 585, \"rewards\": -341.203809}, {\"timesteps\": 586, \"rewards\": -318.690223}, {\"timesteps\": 587, \"rewards\": -33.848195}, {\"timesteps\": 588, \"rewards\": -307.799123}, {\"timesteps\": 589, \"rewards\": -65.129952}, {\"timesteps\": 590, \"rewards\": -101.023207}, {\"timesteps\": 591, \"rewards\": -200.782651}, {\"timesteps\": 592, \"rewards\": -247.390005}, {\"timesteps\": 593, \"rewards\": -278.933895}, {\"timesteps\": 594, \"rewards\": -351.234292}, {\"timesteps\": 595, \"rewards\": -215.432011}, {\"timesteps\": 596, \"rewards\": -232.30805}, {\"timesteps\": 597, \"rewards\": -227.972748}, {\"timesteps\": 598, \"rewards\": -200.254643}, {\"timesteps\": 599, \"rewards\": -310.439679}, {\"timesteps\": 600, \"rewards\": -111.363413}, {\"timesteps\": 601, \"rewards\": -129.68055}, {\"timesteps\": 602, \"rewards\": -161.336713}, {\"timesteps\": 603, \"rewards\": -195.345299}, {\"timesteps\": 604, \"rewards\": -297.360717}, {\"timesteps\": 605, \"rewards\": -115.803511}, {\"timesteps\": 606, \"rewards\": -152.509174}, {\"timesteps\": 607, \"rewards\": -255.720049}, {\"timesteps\": 608, \"rewards\": -257.290296}, {\"timesteps\": 609, \"rewards\": -93.709127}, {\"timesteps\": 610, \"rewards\": -247.252209}, {\"timesteps\": 611, \"rewards\": -148.992645}, {\"timesteps\": 612, \"rewards\": -31.082338}, {\"timesteps\": 613, \"rewards\": -228.748109}, {\"timesteps\": 614, \"rewards\": -102.640871}, {\"timesteps\": 615, \"rewards\": -253.063596}, {\"timesteps\": 616, \"rewards\": -104.868803}, {\"timesteps\": 617, \"rewards\": 173.461404}, {\"timesteps\": 618, \"rewards\": 5.494159}, {\"timesteps\": 619, \"rewards\": -75.89682}, {\"timesteps\": 620, \"rewards\": -114.917174}, {\"timesteps\": 621, \"rewards\": -141.766428}, {\"timesteps\": 622, \"rewards\": -178.820213}, {\"timesteps\": 623, \"rewards\": -9.14301}, {\"timesteps\": 624, \"rewards\": -191.391559}, {\"timesteps\": 625, \"rewards\": -247.818592}, {\"timesteps\": 626, \"rewards\": -276.832026}, {\"timesteps\": 627, \"rewards\": -13.588171}, {\"timesteps\": 628, \"rewards\": -201.433395}, {\"timesteps\": 629, \"rewards\": -330.799934}, {\"timesteps\": 630, \"rewards\": -162.061047}, {\"timesteps\": 631, \"rewards\": -230.487576}, {\"timesteps\": 632, \"rewards\": -247.626484}, {\"timesteps\": 633, \"rewards\": -131.549475}, {\"timesteps\": 634, \"rewards\": -264.130255}, {\"timesteps\": 635, \"rewards\": -323.652609}, {\"timesteps\": 636, \"rewards\": -29.318006}, {\"timesteps\": 637, \"rewards\": -110.479894}, {\"timesteps\": 638, \"rewards\": -249.829599}, {\"timesteps\": 639, \"rewards\": -111.771644}, {\"timesteps\": 640, \"rewards\": -233.996291}, {\"timesteps\": 641, \"rewards\": -160.412141}, {\"timesteps\": 642, \"rewards\": 0.148813}, {\"timesteps\": 643, \"rewards\": -155.870445}, {\"timesteps\": 644, \"rewards\": -183.867335}, {\"timesteps\": 645, \"rewards\": -143.808579}, {\"timesteps\": 646, \"rewards\": -339.10238}, {\"timesteps\": 647, \"rewards\": -213.88717}, {\"timesteps\": 648, \"rewards\": -10.651478}, {\"timesteps\": 649, \"rewards\": -61.143529}, {\"timesteps\": 650, \"rewards\": -157.581688}, {\"timesteps\": 651, \"rewards\": -246.584656}, {\"timesteps\": 652, \"rewards\": -247.590666}, {\"timesteps\": 653, \"rewards\": -225.179347}, {\"timesteps\": 654, \"rewards\": -259.58426}, {\"timesteps\": 655, \"rewards\": -231.413413}, {\"timesteps\": 656, \"rewards\": -175.461391}, {\"timesteps\": 657, \"rewards\": -195.500032}, {\"timesteps\": 658, \"rewards\": -343.74565}, {\"timesteps\": 659, \"rewards\": -190.273132}, {\"timesteps\": 660, \"rewards\": -160.881725}, {\"timesteps\": 661, \"rewards\": -175.894369}, {\"timesteps\": 662, \"rewards\": -257.253946}, {\"timesteps\": 663, \"rewards\": -225.591087}, {\"timesteps\": 664, \"rewards\": -219.85944}, {\"timesteps\": 665, \"rewards\": -72.048877}, {\"timesteps\": 666, \"rewards\": -177.461083}, {\"timesteps\": 667, \"rewards\": -135.145808}, {\"timesteps\": 668, \"rewards\": -214.288048}, {\"timesteps\": 669, \"rewards\": -22.02168}, {\"timesteps\": 670, \"rewards\": -67.985751}, {\"timesteps\": 671, \"rewards\": -195.433077}, {\"timesteps\": 672, \"rewards\": -158.491231}, {\"timesteps\": 673, \"rewards\": -222.471345}, {\"timesteps\": 674, \"rewards\": -108.800295}, {\"timesteps\": 675, \"rewards\": -194.8561}, {\"timesteps\": 676, \"rewards\": -27.365093}, {\"timesteps\": 677, \"rewards\": -209.345031}, {\"timesteps\": 678, \"rewards\": 164.258072}, {\"timesteps\": 679, \"rewards\": -132.555471}, {\"timesteps\": 680, \"rewards\": -180.336549}, {\"timesteps\": 681, \"rewards\": -70.895813}, {\"timesteps\": 682, \"rewards\": -202.218049}, {\"timesteps\": 683, \"rewards\": -248.015999}, {\"timesteps\": 684, \"rewards\": 149.802534}, {\"timesteps\": 685, \"rewards\": -125.693634}, {\"timesteps\": 686, \"rewards\": -139.779067}, {\"timesteps\": 687, \"rewards\": 253.805138}, {\"timesteps\": 688, \"rewards\": -186.248476}, {\"timesteps\": 689, \"rewards\": -184.021386}, {\"timesteps\": 690, \"rewards\": -62.964619}, {\"timesteps\": 691, \"rewards\": -60.237024}, {\"timesteps\": 692, \"rewards\": -206.64351}, {\"timesteps\": 693, \"rewards\": -34.524727}, {\"timesteps\": 694, \"rewards\": 167.215838}, {\"timesteps\": 695, \"rewards\": -158.856108}, {\"timesteps\": 696, \"rewards\": -71.971446}, {\"timesteps\": 697, \"rewards\": -82.60031}, {\"timesteps\": 698, \"rewards\": -293.075408}, {\"timesteps\": 699, \"rewards\": 6.44919}, {\"timesteps\": 700, \"rewards\": -75.643458}, {\"timesteps\": 701, \"rewards\": -39.275423}, {\"timesteps\": 702, \"rewards\": -210.424751}, {\"timesteps\": 703, \"rewards\": -185.080109}, {\"timesteps\": 704, \"rewards\": -80.156292}, {\"timesteps\": 705, \"rewards\": -171.707352}, {\"timesteps\": 706, \"rewards\": -8.923482}, {\"timesteps\": 707, \"rewards\": 184.184052}, {\"timesteps\": 708, \"rewards\": -178.595614}, {\"timesteps\": 709, \"rewards\": -73.478557}, {\"timesteps\": 710, \"rewards\": 49.310453}, {\"timesteps\": 711, \"rewards\": -55.202767}, {\"timesteps\": 712, \"rewards\": 207.584585}, {\"timesteps\": 713, \"rewards\": -115.819459}, {\"timesteps\": 714, \"rewards\": -108.936689}, {\"timesteps\": 715, \"rewards\": -140.890117}, {\"timesteps\": 716, \"rewards\": -218.090416}, {\"timesteps\": 717, \"rewards\": -55.769802}, {\"timesteps\": 718, \"rewards\": 208.412458}, {\"timesteps\": 719, \"rewards\": -212.716465}, {\"timesteps\": 720, \"rewards\": -178.95725}, {\"timesteps\": 721, \"rewards\": -160.165208}, {\"timesteps\": 722, \"rewards\": 134.851741}, {\"timesteps\": 723, \"rewards\": -74.048636}, {\"timesteps\": 724, \"rewards\": 128.69137}, {\"timesteps\": 725, \"rewards\": -175.967802}, {\"timesteps\": 726, \"rewards\": -277.229081}, {\"timesteps\": 727, \"rewards\": -268.025948}, {\"timesteps\": 728, \"rewards\": -32.867605}, {\"timesteps\": 729, \"rewards\": -30.1046}, {\"timesteps\": 730, \"rewards\": -182.312553}, {\"timesteps\": 731, \"rewards\": -32.376817}, {\"timesteps\": 732, \"rewards\": -253.202515}, {\"timesteps\": 733, \"rewards\": -22.567356}, {\"timesteps\": 734, \"rewards\": 100.981182}, {\"timesteps\": 735, \"rewards\": -196.528259}, {\"timesteps\": 736, \"rewards\": -190.243863}, {\"timesteps\": 737, \"rewards\": 207.969058}, {\"timesteps\": 738, \"rewards\": -215.41402}, {\"timesteps\": 739, \"rewards\": -0.287995}, {\"timesteps\": 740, \"rewards\": -220.858517}, {\"timesteps\": 741, \"rewards\": 143.770044}, {\"timesteps\": 742, \"rewards\": -241.714888}, {\"timesteps\": 743, \"rewards\": -88.912814}, {\"timesteps\": 744, \"rewards\": 191.716136}, {\"timesteps\": 745, \"rewards\": 207.603125}, {\"timesteps\": 746, \"rewards\": -48.157425}, {\"timesteps\": 747, \"rewards\": 175.965979}, {\"timesteps\": 748, \"rewards\": 115.766029}, {\"timesteps\": 749, \"rewards\": -37.76535}, {\"timesteps\": 750, \"rewards\": -10.100858}, {\"timesteps\": 751, \"rewards\": -229.626139}, {\"timesteps\": 752, \"rewards\": -143.844919}, {\"timesteps\": 753, \"rewards\": -40.333867}, {\"timesteps\": 754, \"rewards\": -128.722241}, {\"timesteps\": 755, \"rewards\": -232.51961}, {\"timesteps\": 756, \"rewards\": -43.381307}, {\"timesteps\": 757, \"rewards\": -95.031779}, {\"timesteps\": 758, \"rewards\": -311.005443}, {\"timesteps\": 759, \"rewards\": -263.134044}, {\"timesteps\": 760, \"rewards\": -74.284543}, {\"timesteps\": 761, \"rewards\": -253.126424}, {\"timesteps\": 762, \"rewards\": -87.941269}, {\"timesteps\": 763, \"rewards\": 129.91609}, {\"timesteps\": 764, \"rewards\": -215.291849}, {\"timesteps\": 765, \"rewards\": -111.578457}, {\"timesteps\": 766, \"rewards\": -82.077286}, {\"timesteps\": 767, \"rewards\": -257.21611}, {\"timesteps\": 768, \"rewards\": -286.742602}, {\"timesteps\": 769, \"rewards\": 159.742237}, {\"timesteps\": 770, \"rewards\": -97.814645}, {\"timesteps\": 771, \"rewards\": -268.679389}, {\"timesteps\": 772, \"rewards\": -259.615488}, {\"timesteps\": 773, \"rewards\": -296.379032}, {\"timesteps\": 774, \"rewards\": -311.302861}, {\"timesteps\": 775, \"rewards\": -250.200071}, {\"timesteps\": 776, \"rewards\": -314.9743}, {\"timesteps\": 777, \"rewards\": 138.863632}, {\"timesteps\": 778, \"rewards\": -88.626228}, {\"timesteps\": 779, \"rewards\": -217.019033}, {\"timesteps\": 780, \"rewards\": -179.335379}, {\"timesteps\": 781, \"rewards\": -173.957371}, {\"timesteps\": 782, \"rewards\": -224.889385}, {\"timesteps\": 783, \"rewards\": -11.961422}, {\"timesteps\": 784, \"rewards\": -222.468467}, {\"timesteps\": 785, \"rewards\": -169.001678}, {\"timesteps\": 786, \"rewards\": 163.701846}, {\"timesteps\": 787, \"rewards\": -308.800131}, {\"timesteps\": 788, \"rewards\": -245.908855}, {\"timesteps\": 789, \"rewards\": 75.301764}, {\"timesteps\": 790, \"rewards\": -36.155514}, {\"timesteps\": 791, \"rewards\": -204.280376}, {\"timesteps\": 792, \"rewards\": -269.741549}, {\"timesteps\": 793, \"rewards\": -191.196426}, {\"timesteps\": 794, \"rewards\": -319.991516}, {\"timesteps\": 795, \"rewards\": -185.115968}, {\"timesteps\": 796, \"rewards\": -78.599323}, {\"timesteps\": 797, \"rewards\": -57.421789}, {\"timesteps\": 798, \"rewards\": -339.29297}, {\"timesteps\": 799, \"rewards\": -277.264437}, {\"timesteps\": 800, \"rewards\": -208.821291}, {\"timesteps\": 801, \"rewards\": -14.825092}, {\"timesteps\": 802, \"rewards\": -116.841298}, {\"timesteps\": 803, \"rewards\": -205.027985}, {\"timesteps\": 804, \"rewards\": -138.686133}, {\"timesteps\": 805, \"rewards\": -250.375802}, {\"timesteps\": 806, \"rewards\": -212.429399}, {\"timesteps\": 807, \"rewards\": -59.146074}, {\"timesteps\": 808, \"rewards\": 178.979514}, {\"timesteps\": 809, \"rewards\": 136.841809}, {\"timesteps\": 810, \"rewards\": -123.276258}, {\"timesteps\": 811, \"rewards\": -51.521528}, {\"timesteps\": 812, \"rewards\": 236.561879}, {\"timesteps\": 813, \"rewards\": -399.330729}, {\"timesteps\": 814, \"rewards\": -280.749355}, {\"timesteps\": 815, \"rewards\": -281.13446}, {\"timesteps\": 816, \"rewards\": -290.779614}, {\"timesteps\": 817, \"rewards\": 173.021118}, {\"timesteps\": 818, \"rewards\": -299.180092}, {\"timesteps\": 819, \"rewards\": -111.398961}, {\"timesteps\": 820, \"rewards\": -76.821641}, {\"timesteps\": 821, \"rewards\": 129.6484}, {\"timesteps\": 822, \"rewards\": 138.10855}, {\"timesteps\": 823, \"rewards\": 188.507942}, {\"timesteps\": 824, \"rewards\": 93.166959}, {\"timesteps\": 825, \"rewards\": -179.853769}, {\"timesteps\": 826, \"rewards\": -200.31776}, {\"timesteps\": 827, \"rewards\": 187.121119}, {\"timesteps\": 828, \"rewards\": 110.722412}, {\"timesteps\": 829, \"rewards\": -108.591746}, {\"timesteps\": 830, \"rewards\": -85.483142}, {\"timesteps\": 831, \"rewards\": -91.328525}, {\"timesteps\": 832, \"rewards\": 116.140738}, {\"timesteps\": 833, \"rewards\": 130.161822}, {\"timesteps\": 834, \"rewards\": -71.096452}, {\"timesteps\": 835, \"rewards\": 1.957959}, {\"timesteps\": 836, \"rewards\": -186.123955}, {\"timesteps\": 837, \"rewards\": 5.540189}, {\"timesteps\": 838, \"rewards\": -25.210983}, {\"timesteps\": 839, \"rewards\": -182.914696}, {\"timesteps\": 840, \"rewards\": -129.618792}, {\"timesteps\": 841, \"rewards\": -45.911937}, {\"timesteps\": 842, \"rewards\": -117.953796}, {\"timesteps\": 843, \"rewards\": -196.509358}, {\"timesteps\": 844, \"rewards\": -296.184711}, {\"timesteps\": 845, \"rewards\": -212.429249}, {\"timesteps\": 846, \"rewards\": -204.222372}, {\"timesteps\": 847, \"rewards\": -207.324677}, {\"timesteps\": 848, \"rewards\": 110.298301}, {\"timesteps\": 849, \"rewards\": -221.83363}, {\"timesteps\": 850, \"rewards\": -95.599956}, {\"timesteps\": 851, \"rewards\": -164.199964}, {\"timesteps\": 852, \"rewards\": 120.403657}, {\"timesteps\": 853, \"rewards\": 133.812967}, {\"timesteps\": 854, \"rewards\": -227.608693}, {\"timesteps\": 855, \"rewards\": -29.519851}, {\"timesteps\": 856, \"rewards\": -77.841256}, {\"timesteps\": 857, \"rewards\": -70.061079}, {\"timesteps\": 858, \"rewards\": 159.176129}, {\"timesteps\": 859, \"rewards\": -66.238577}, {\"timesteps\": 860, \"rewards\": -164.2048}, {\"timesteps\": 861, \"rewards\": -167.610182}, {\"timesteps\": 862, \"rewards\": -124.370263}, {\"timesteps\": 863, \"rewards\": -182.775992}, {\"timesteps\": 864, \"rewards\": -151.655697}, {\"timesteps\": 865, \"rewards\": -42.866449}, {\"timesteps\": 866, \"rewards\": -181.977424}, {\"timesteps\": 867, \"rewards\": -95.617618}, {\"timesteps\": 868, \"rewards\": 163.33185}, {\"timesteps\": 869, \"rewards\": -156.309338}, {\"timesteps\": 870, \"rewards\": -6.105524}, {\"timesteps\": 871, \"rewards\": -155.308887}, {\"timesteps\": 872, \"rewards\": -187.542406}, {\"timesteps\": 873, \"rewards\": 173.190879}, {\"timesteps\": 874, \"rewards\": 131.820529}, {\"timesteps\": 875, \"rewards\": 3.879753}, {\"timesteps\": 876, \"rewards\": -333.281118}, {\"timesteps\": 877, \"rewards\": 107.615048}, {\"timesteps\": 878, \"rewards\": -351.41958}, {\"timesteps\": 879, \"rewards\": -276.018719}, {\"timesteps\": 880, \"rewards\": -61.236857}, {\"timesteps\": 881, \"rewards\": -150.527738}, {\"timesteps\": 882, \"rewards\": -229.300707}, {\"timesteps\": 883, \"rewards\": -232.632752}, {\"timesteps\": 884, \"rewards\": -248.682645}, {\"timesteps\": 885, \"rewards\": -206.408419}, {\"timesteps\": 886, \"rewards\": -247.192097}, {\"timesteps\": 887, \"rewards\": -198.274369}, {\"timesteps\": 888, \"rewards\": -289.603321}, {\"timesteps\": 889, \"rewards\": -263.997895}, {\"timesteps\": 890, \"rewards\": -204.354163}, {\"timesteps\": 891, \"rewards\": -193.479342}, {\"timesteps\": 892, \"rewards\": -73.311204}, {\"timesteps\": 893, \"rewards\": -73.042958}, {\"timesteps\": 894, \"rewards\": -264.882892}, {\"timesteps\": 895, \"rewards\": -85.984114}, {\"timesteps\": 896, \"rewards\": -309.789869}, {\"timesteps\": 897, \"rewards\": 166.73178}, {\"timesteps\": 898, \"rewards\": -136.824532}, {\"timesteps\": 899, \"rewards\": -43.934455}, {\"timesteps\": 900, \"rewards\": -184.22693}, {\"timesteps\": 901, \"rewards\": -257.059778}, {\"timesteps\": 902, \"rewards\": -273.038594}, {\"timesteps\": 903, \"rewards\": -234.027935}, {\"timesteps\": 904, \"rewards\": 102.545806}, {\"timesteps\": 905, \"rewards\": -36.825782}, {\"timesteps\": 906, \"rewards\": -31.421285}, {\"timesteps\": 907, \"rewards\": -89.88418}, {\"timesteps\": 908, \"rewards\": -258.06311}, {\"timesteps\": 909, \"rewards\": -231.944392}, {\"timesteps\": 910, \"rewards\": -244.00421}, {\"timesteps\": 911, \"rewards\": -140.76425}, {\"timesteps\": 912, \"rewards\": -43.491116}, {\"timesteps\": 913, \"rewards\": -274.957431}, {\"timesteps\": 914, \"rewards\": -29.672293}, {\"timesteps\": 915, \"rewards\": -45.510836}, {\"timesteps\": 916, \"rewards\": 164.369494}, {\"timesteps\": 917, \"rewards\": -67.296333}, {\"timesteps\": 918, \"rewards\": -46.924441}, {\"timesteps\": 919, \"rewards\": -67.724211}, {\"timesteps\": 920, \"rewards\": -198.846746}, {\"timesteps\": 921, \"rewards\": -222.54677}, {\"timesteps\": 922, \"rewards\": -163.407013}, {\"timesteps\": 923, \"rewards\": -385.694678}, {\"timesteps\": 924, \"rewards\": 271.53682}, {\"timesteps\": 925, \"rewards\": 192.201934}, {\"timesteps\": 926, \"rewards\": -190.008484}, {\"timesteps\": 927, \"rewards\": -332.860434}, {\"timesteps\": 928, \"rewards\": -247.006687}, {\"timesteps\": 929, \"rewards\": 56.922837}, {\"timesteps\": 930, \"rewards\": -55.901478}, {\"timesteps\": 931, \"rewards\": -136.464269}, {\"timesteps\": 932, \"rewards\": 135.987087}, {\"timesteps\": 933, \"rewards\": -249.06514}, {\"timesteps\": 934, \"rewards\": -76.631467}, {\"timesteps\": 935, \"rewards\": -27.903609}, {\"timesteps\": 936, \"rewards\": -215.597445}, {\"timesteps\": 937, \"rewards\": -47.908474}, {\"timesteps\": 938, \"rewards\": 22.895402}, {\"timesteps\": 939, \"rewards\": -224.961995}, {\"timesteps\": 940, \"rewards\": 200.163699}, {\"timesteps\": 941, \"rewards\": -213.030749}, {\"timesteps\": 942, \"rewards\": -65.713631}, {\"timesteps\": 943, \"rewards\": 19.052179}, {\"timesteps\": 944, \"rewards\": -267.664226}, {\"timesteps\": 945, \"rewards\": -67.577122}, {\"timesteps\": 946, \"rewards\": 113.089057}, {\"timesteps\": 947, \"rewards\": -318.240534}, {\"timesteps\": 948, \"rewards\": -54.515172}, {\"timesteps\": 949, \"rewards\": -231.328864}, {\"timesteps\": 950, \"rewards\": -147.09591}, {\"timesteps\": 951, \"rewards\": 111.951017}, {\"timesteps\": 952, \"rewards\": -17.323617}, {\"timesteps\": 953, \"rewards\": -289.000923}, {\"timesteps\": 954, \"rewards\": -194.498437}, {\"timesteps\": 955, \"rewards\": -305.309733}, {\"timesteps\": 956, \"rewards\": -151.606716}, {\"timesteps\": 957, \"rewards\": 165.204064}, {\"timesteps\": 958, \"rewards\": 168.513162}, {\"timesteps\": 959, \"rewards\": -162.379145}, {\"timesteps\": 960, \"rewards\": -178.572074}, {\"timesteps\": 961, \"rewards\": -200.186034}, {\"timesteps\": 962, \"rewards\": -307.532567}, {\"timesteps\": 963, \"rewards\": -197.439312}, {\"timesteps\": 964, \"rewards\": -235.535202}, {\"timesteps\": 965, \"rewards\": -223.802615}, {\"timesteps\": 966, \"rewards\": -2.605354}, {\"timesteps\": 967, \"rewards\": 184.528051}, {\"timesteps\": 968, \"rewards\": -199.932097}, {\"timesteps\": 969, \"rewards\": -88.49657}, {\"timesteps\": 970, \"rewards\": -8.787481}, {\"timesteps\": 971, \"rewards\": -86.130213}, {\"timesteps\": 972, \"rewards\": -151.356496}, {\"timesteps\": 973, \"rewards\": 141.222515}, {\"timesteps\": 974, \"rewards\": 63.453543}, {\"timesteps\": 975, \"rewards\": -140.556041}, {\"timesteps\": 976, \"rewards\": -260.297974}, {\"timesteps\": 977, \"rewards\": -98.693665}, {\"timesteps\": 978, \"rewards\": -179.99329}, {\"timesteps\": 979, \"rewards\": -256.684986}, {\"timesteps\": 980, \"rewards\": -305.88859}, {\"timesteps\": 981, \"rewards\": -18.360724}, {\"timesteps\": 982, \"rewards\": -131.58372}, {\"timesteps\": 983, \"rewards\": 110.645379}, {\"timesteps\": 984, \"rewards\": -259.776561}, {\"timesteps\": 985, \"rewards\": -107.697629}, {\"timesteps\": 986, \"rewards\": -209.698692}, {\"timesteps\": 987, \"rewards\": -130.861473}, {\"timesteps\": 988, \"rewards\": -378.020793}, {\"timesteps\": 989, \"rewards\": -264.507122}, {\"timesteps\": 990, \"rewards\": -212.005493}, {\"timesteps\": 991, \"rewards\": -214.384587}, {\"timesteps\": 992, \"rewards\": -281.429608}, {\"timesteps\": 993, \"rewards\": -132.907558}, {\"timesteps\": 994, \"rewards\": -225.951426}, {\"timesteps\": 995, \"rewards\": 144.723554}, {\"timesteps\": 996, \"rewards\": 106.888863}, {\"timesteps\": 997, \"rewards\": 104.988579}, {\"timesteps\": 998, \"rewards\": -318.63712}, {\"timesteps\": 999, \"rewards\": -90.213302}, {\"timesteps\": 1000, \"rewards\": -200.804846}, {\"timesteps\": 1001, \"rewards\": 159.667421}, {\"timesteps\": 1002, \"rewards\": 232.708441}, {\"timesteps\": 1003, \"rewards\": -245.722785}, {\"timesteps\": 1004, \"rewards\": 9.411393}, {\"timesteps\": 1005, \"rewards\": -34.033276}, {\"timesteps\": 1006, \"rewards\": -180.666791}, {\"timesteps\": 1007, \"rewards\": 146.062864}, {\"timesteps\": 1008, \"rewards\": -46.920385}, {\"timesteps\": 1009, \"rewards\": -56.820321}, {\"timesteps\": 1010, \"rewards\": -205.075381}, {\"timesteps\": 1011, \"rewards\": 66.146911}, {\"timesteps\": 1012, \"rewards\": -154.110332}, {\"timesteps\": 1013, \"rewards\": -137.075383}, {\"timesteps\": 1014, \"rewards\": -160.503453}, {\"timesteps\": 1015, \"rewards\": -189.251963}, {\"timesteps\": 1016, \"rewards\": -203.08392}, {\"timesteps\": 1017, \"rewards\": 185.234949}, {\"timesteps\": 1018, \"rewards\": -62.174295}, {\"timesteps\": 1019, \"rewards\": -108.013921}, {\"timesteps\": 1020, \"rewards\": -150.996257}, {\"timesteps\": 1021, \"rewards\": -182.711886}, {\"timesteps\": 1022, \"rewards\": -230.115566}, {\"timesteps\": 1023, \"rewards\": -156.053362}, {\"timesteps\": 1024, \"rewards\": -5.472379}, {\"timesteps\": 1025, \"rewards\": -97.358846}, {\"timesteps\": 1026, \"rewards\": -156.093069}, {\"timesteps\": 1027, \"rewards\": -214.795715}, {\"timesteps\": 1028, \"rewards\": 158.461402}, {\"timesteps\": 1029, \"rewards\": -331.803618}, {\"timesteps\": 1030, \"rewards\": -172.576598}, {\"timesteps\": 1031, \"rewards\": 107.639397}, {\"timesteps\": 1032, \"rewards\": 88.533478}, {\"timesteps\": 1033, \"rewards\": -232.594671}, {\"timesteps\": 1034, \"rewards\": 120.2424}, {\"timesteps\": 1035, \"rewards\": -240.087976}, {\"timesteps\": 1036, \"rewards\": -227.836593}, {\"timesteps\": 1037, \"rewards\": 168.902879}, {\"timesteps\": 1038, \"rewards\": -145.269032}, {\"timesteps\": 1039, \"rewards\": -342.453746}, {\"timesteps\": 1040, \"rewards\": -154.93573}, {\"timesteps\": 1041, \"rewards\": -206.516494}, {\"timesteps\": 1042, \"rewards\": -291.811505}, {\"timesteps\": 1043, \"rewards\": -208.338645}, {\"timesteps\": 1044, \"rewards\": 183.62964}, {\"timesteps\": 1045, \"rewards\": -110.606441}, {\"timesteps\": 1046, \"rewards\": -75.664744}, {\"timesteps\": 1047, \"rewards\": -177.106044}, {\"timesteps\": 1048, \"rewards\": -96.447965}, {\"timesteps\": 1049, \"rewards\": -298.309618}, {\"timesteps\": 1050, \"rewards\": -92.479742}, {\"timesteps\": 1051, \"rewards\": -254.711748}, {\"timesteps\": 1052, \"rewards\": 129.871138}, {\"timesteps\": 1053, \"rewards\": -184.373832}, {\"timesteps\": 1054, \"rewards\": 27.94089}, {\"timesteps\": 1055, \"rewards\": -132.66902}, {\"timesteps\": 1056, \"rewards\": -22.898402}, {\"timesteps\": 1057, \"rewards\": 192.137991}, {\"timesteps\": 1058, \"rewards\": -216.127142}, {\"timesteps\": 1059, \"rewards\": -282.969676}, {\"timesteps\": 1060, \"rewards\": -210.579578}, {\"timesteps\": 1061, \"rewards\": -54.451804}, {\"timesteps\": 1062, \"rewards\": -334.416765}, {\"timesteps\": 1063, \"rewards\": 143.947731}, {\"timesteps\": 1064, \"rewards\": -242.174671}, {\"timesteps\": 1065, \"rewards\": -189.395755}, {\"timesteps\": 1066, \"rewards\": 35.4769}, {\"timesteps\": 1067, \"rewards\": -73.890542}, {\"timesteps\": 1068, \"rewards\": 58.929104}, {\"timesteps\": 1069, \"rewards\": 198.226352}, {\"timesteps\": 1070, \"rewards\": 176.827535}, {\"timesteps\": 1071, \"rewards\": -260.72863}, {\"timesteps\": 1072, \"rewards\": -174.254868}, {\"timesteps\": 1073, \"rewards\": -186.150092}, {\"timesteps\": 1074, \"rewards\": -262.373767}, {\"timesteps\": 1075, \"rewards\": -85.350346}, {\"timesteps\": 1076, \"rewards\": -247.550389}, {\"timesteps\": 1077, \"rewards\": -276.046011}, {\"timesteps\": 1078, \"rewards\": 171.189946}, {\"timesteps\": 1079, \"rewards\": -266.844264}, {\"timesteps\": 1080, \"rewards\": 148.773318}, {\"timesteps\": 1081, \"rewards\": 147.839317}, {\"timesteps\": 1082, \"rewards\": 80.585684}, {\"timesteps\": 1083, \"rewards\": -16.453145}, {\"timesteps\": 1084, \"rewards\": 174.341067}, {\"timesteps\": 1085, \"rewards\": 107.941276}, {\"timesteps\": 1086, \"rewards\": -95.778591}, {\"timesteps\": 1087, \"rewards\": -63.83889}, {\"timesteps\": 1088, \"rewards\": 4.954163}, {\"timesteps\": 1089, \"rewards\": 137.044726}, {\"timesteps\": 1090, \"rewards\": -325.713413}, {\"timesteps\": 1091, \"rewards\": 124.456623}, {\"timesteps\": 1092, \"rewards\": 51.379324}, {\"timesteps\": 1093, \"rewards\": -225.887939}, {\"timesteps\": 1094, \"rewards\": -78.803857}, {\"timesteps\": 1095, \"rewards\": -72.637129}, {\"timesteps\": 1096, \"rewards\": -37.087922}, {\"timesteps\": 1097, \"rewards\": -23.258838}, {\"timesteps\": 1098, \"rewards\": -302.570323}, {\"timesteps\": 1099, \"rewards\": -120.681575}, {\"timesteps\": 1100, \"rewards\": 199.721946}, {\"timesteps\": 1101, \"rewards\": -109.730679}, {\"timesteps\": 1102, \"rewards\": -275.599978}, {\"timesteps\": 1103, \"rewards\": -249.73995}, {\"timesteps\": 1104, \"rewards\": -225.455375}, {\"timesteps\": 1105, \"rewards\": -38.417094}, {\"timesteps\": 1106, \"rewards\": -154.785697}, {\"timesteps\": 1107, \"rewards\": -152.33068}, {\"timesteps\": 1108, \"rewards\": 11.846115}, {\"timesteps\": 1109, \"rewards\": -157.282431}, {\"timesteps\": 1110, \"rewards\": 128.666406}, {\"timesteps\": 1111, \"rewards\": -339.576258}, {\"timesteps\": 1112, \"rewards\": -338.321556}, {\"timesteps\": 1113, \"rewards\": -125.261134}, {\"timesteps\": 1114, \"rewards\": -10.106461}, {\"timesteps\": 1115, \"rewards\": -38.45824}, {\"timesteps\": 1116, \"rewards\": -53.958495}, {\"timesteps\": 1117, \"rewards\": -100.63392}, {\"timesteps\": 1118, \"rewards\": -225.506147}, {\"timesteps\": 1119, \"rewards\": 72.807325}, {\"timesteps\": 1120, \"rewards\": -256.379396}, {\"timesteps\": 1121, \"rewards\": -55.753101}, {\"timesteps\": 1122, \"rewards\": -42.813702}, {\"timesteps\": 1123, \"rewards\": 19.361212}, {\"timesteps\": 1124, \"rewards\": -79.070735}, {\"timesteps\": 1125, \"rewards\": -13.954187}, {\"timesteps\": 1126, \"rewards\": -91.058939}, {\"timesteps\": 1127, \"rewards\": 102.570696}, {\"timesteps\": 1128, \"rewards\": -31.424852}, {\"timesteps\": 1129, \"rewards\": -126.244194}, {\"timesteps\": 1130, \"rewards\": -184.666697}, {\"timesteps\": 1131, \"rewards\": -227.148069}, {\"timesteps\": 1132, \"rewards\": -306.866974}, {\"timesteps\": 1133, \"rewards\": -72.639313}, {\"timesteps\": 1134, \"rewards\": -20.838888}, {\"timesteps\": 1135, \"rewards\": 195.091633}, {\"timesteps\": 1136, \"rewards\": 129.25341}, {\"timesteps\": 1137, \"rewards\": -185.868923}, {\"timesteps\": 1138, \"rewards\": -60.190332}, {\"timesteps\": 1139, \"rewards\": -183.56778}, {\"timesteps\": 1140, \"rewards\": -15.658764}, {\"timesteps\": 1141, \"rewards\": 194.190634}, {\"timesteps\": 1142, \"rewards\": 132.831131}, {\"timesteps\": 1143, \"rewards\": -305.259627}, {\"timesteps\": 1144, \"rewards\": -304.154551}, {\"timesteps\": 1145, \"rewards\": -86.168615}, {\"timesteps\": 1146, \"rewards\": 202.624962}, {\"timesteps\": 1147, \"rewards\": -184.821655}, {\"timesteps\": 1148, \"rewards\": -158.956457}, {\"timesteps\": 1149, \"rewards\": 147.197799}, {\"timesteps\": 1150, \"rewards\": 54.412242}, {\"timesteps\": 1151, \"rewards\": -191.888429}, {\"timesteps\": 1152, \"rewards\": -52.612705}, {\"timesteps\": 1153, \"rewards\": 134.295366}, {\"timesteps\": 1154, \"rewards\": -225.606584}, {\"timesteps\": 1155, \"rewards\": -45.758569}, {\"timesteps\": 1156, \"rewards\": -114.346108}, {\"timesteps\": 1157, \"rewards\": -36.107879}, {\"timesteps\": 1158, \"rewards\": -220.368364}, {\"timesteps\": 1159, \"rewards\": -18.294135}, {\"timesteps\": 1160, \"rewards\": -236.51479}, {\"timesteps\": 1161, \"rewards\": -76.867336}, {\"timesteps\": 1162, \"rewards\": 38.051369}, {\"timesteps\": 1163, \"rewards\": 29.137294}, {\"timesteps\": 1164, \"rewards\": -45.834213}, {\"timesteps\": 1165, \"rewards\": -218.42557}, {\"timesteps\": 1166, \"rewards\": -84.766576}, {\"timesteps\": 1167, \"rewards\": 92.324196}, {\"timesteps\": 1168, \"rewards\": 34.472886}, {\"timesteps\": 1169, \"rewards\": -209.27553}, {\"timesteps\": 1170, \"rewards\": -184.116731}, {\"timesteps\": 1171, \"rewards\": -147.580334}, {\"timesteps\": 1172, \"rewards\": -100.544373}, {\"timesteps\": 1173, \"rewards\": -252.659525}, {\"timesteps\": 1174, \"rewards\": -101.06191}, {\"timesteps\": 1175, \"rewards\": -216.122325}, {\"timesteps\": 1176, \"rewards\": -234.237317}, {\"timesteps\": 1177, \"rewards\": -228.866073}, {\"timesteps\": 1178, \"rewards\": -294.727982}, {\"timesteps\": 1179, \"rewards\": -71.775945}, {\"timesteps\": 1180, \"rewards\": -127.120701}, {\"timesteps\": 1181, \"rewards\": 7.611367}, {\"timesteps\": 1182, \"rewards\": 164.230216}, {\"timesteps\": 1183, \"rewards\": -8.886685}, {\"timesteps\": 1184, \"rewards\": 121.21802}, {\"timesteps\": 1185, \"rewards\": 196.703156}, {\"timesteps\": 1186, \"rewards\": 100.713219}, {\"timesteps\": 1187, \"rewards\": -260.342289}, {\"timesteps\": 1188, \"rewards\": 72.939801}, {\"timesteps\": 1189, \"rewards\": 29.528232}, {\"timesteps\": 1190, \"rewards\": -209.858538}, {\"timesteps\": 1191, \"rewards\": -63.612071}, {\"timesteps\": 1192, \"rewards\": 144.613272}, {\"timesteps\": 1193, \"rewards\": 178.692692}, {\"timesteps\": 1194, \"rewards\": -279.172405}, {\"timesteps\": 1195, \"rewards\": -53.825286}, {\"timesteps\": 1196, \"rewards\": -121.217483}, {\"timesteps\": 1197, \"rewards\": -106.846929}, {\"timesteps\": 1198, \"rewards\": 51.28477}, {\"timesteps\": 1199, \"rewards\": 173.876913}, {\"timesteps\": 1200, \"rewards\": -262.642923}, {\"timesteps\": 1201, \"rewards\": 175.518783}, {\"timesteps\": 1202, \"rewards\": -49.87268}, {\"timesteps\": 1203, \"rewards\": 46.565572}, {\"timesteps\": 1204, \"rewards\": 211.510462}, {\"timesteps\": 1205, \"rewards\": 196.881201}, {\"timesteps\": 1206, \"rewards\": 105.436934}, {\"timesteps\": 1207, \"rewards\": 62.773673}, {\"timesteps\": 1208, \"rewards\": 102.127514}, {\"timesteps\": 1209, \"rewards\": -247.154212}, {\"timesteps\": 1210, \"rewards\": -212.932278}, {\"timesteps\": 1211, \"rewards\": 202.282177}, {\"timesteps\": 1212, \"rewards\": -60.650705}, {\"timesteps\": 1213, \"rewards\": -240.839729}, {\"timesteps\": 1214, \"rewards\": 64.744248}, {\"timesteps\": 1215, \"rewards\": -26.799041}, {\"timesteps\": 1216, \"rewards\": 153.482914}, {\"timesteps\": 1217, \"rewards\": -146.372994}, {\"timesteps\": 1218, \"rewards\": 112.824057}, {\"timesteps\": 1219, \"rewards\": 100.506591}, {\"timesteps\": 1220, \"rewards\": -28.559393}, {\"timesteps\": 1221, \"rewards\": 159.633069}, {\"timesteps\": 1222, \"rewards\": 6.934616}, {\"timesteps\": 1223, \"rewards\": -320.50362}, {\"timesteps\": 1224, \"rewards\": 102.500172}, {\"timesteps\": 1225, \"rewards\": -53.952121}, {\"timesteps\": 1226, \"rewards\": -41.064001}, {\"timesteps\": 1227, \"rewards\": 239.764033}, {\"timesteps\": 1228, \"rewards\": 130.279621}, {\"timesteps\": 1229, \"rewards\": 110.708909}, {\"timesteps\": 1230, \"rewards\": -223.494404}, {\"timesteps\": 1231, \"rewards\": 253.105179}, {\"timesteps\": 1232, \"rewards\": -66.305825}, {\"timesteps\": 1233, \"rewards\": -178.186292}, {\"timesteps\": 1234, \"rewards\": 156.366886}, {\"timesteps\": 1235, \"rewards\": -230.543023}, {\"timesteps\": 1236, \"rewards\": 206.904755}, {\"timesteps\": 1237, \"rewards\": -214.243035}, {\"timesteps\": 1238, \"rewards\": 94.391833}, {\"timesteps\": 1239, \"rewards\": -254.419205}, {\"timesteps\": 1240, \"rewards\": 211.607376}, {\"timesteps\": 1241, \"rewards\": 222.033853}, {\"timesteps\": 1242, \"rewards\": 107.970794}, {\"timesteps\": 1243, \"rewards\": 110.554548}, {\"timesteps\": 1244, \"rewards\": -170.751556}, {\"timesteps\": 1245, \"rewards\": -60.838899}, {\"timesteps\": 1246, \"rewards\": 146.812143}, {\"timesteps\": 1247, \"rewards\": 163.685249}, {\"timesteps\": 1248, \"rewards\": -88.048477}, {\"timesteps\": 1249, \"rewards\": 100.485414}, {\"timesteps\": 1250, \"rewards\": -321.11185}, {\"timesteps\": 1251, \"rewards\": 123.721543}, {\"timesteps\": 1252, \"rewards\": 62.317417}, {\"timesteps\": 1253, \"rewards\": 134.621015}, {\"timesteps\": 1254, \"rewards\": 111.857453}, {\"timesteps\": 1255, \"rewards\": -52.403397}, {\"timesteps\": 1256, \"rewards\": 184.221087}, {\"timesteps\": 1257, \"rewards\": -396.467004}, {\"timesteps\": 1258, \"rewards\": 36.952513}, {\"timesteps\": 1259, \"rewards\": 249.788044}, {\"timesteps\": 1260, \"rewards\": -35.202546}, {\"timesteps\": 1261, \"rewards\": -3.121647}, {\"timesteps\": 1262, \"rewards\": -59.913089}, {\"timesteps\": 1263, \"rewards\": 258.268375}, {\"timesteps\": 1264, \"rewards\": -22.791847}, {\"timesteps\": 1265, \"rewards\": 205.137964}, {\"timesteps\": 1266, \"rewards\": -121.270395}, {\"timesteps\": 1267, \"rewards\": 97.943446}, {\"timesteps\": 1268, \"rewards\": -45.65424}, {\"timesteps\": 1269, \"rewards\": -244.907994}, {\"timesteps\": 1270, \"rewards\": -268.951686}, {\"timesteps\": 1271, \"rewards\": -132.754352}, {\"timesteps\": 1272, \"rewards\": -30.28637}, {\"timesteps\": 1273, \"rewards\": -107.067836}, {\"timesteps\": 1274, \"rewards\": -259.348324}, {\"timesteps\": 1275, \"rewards\": 220.055191}, {\"timesteps\": 1276, \"rewards\": -96.993146}, {\"timesteps\": 1277, \"rewards\": 169.368841}, {\"timesteps\": 1278, \"rewards\": 8.425115}, {\"timesteps\": 1279, \"rewards\": 186.403813}, {\"timesteps\": 1280, \"rewards\": -59.236247}, {\"timesteps\": 1281, \"rewards\": -70.456552}, {\"timesteps\": 1282, \"rewards\": 187.163176}, {\"timesteps\": 1283, \"rewards\": -217.523633}, {\"timesteps\": 1284, \"rewards\": 233.635967}, {\"timesteps\": 1285, \"rewards\": -85.29335}, {\"timesteps\": 1286, \"rewards\": 190.556271}, {\"timesteps\": 1287, \"rewards\": 157.666273}, {\"timesteps\": 1288, \"rewards\": -216.365739}, {\"timesteps\": 1289, \"rewards\": 60.855319}, {\"timesteps\": 1290, \"rewards\": -82.139594}, {\"timesteps\": 1291, \"rewards\": -62.534851}, {\"timesteps\": 1292, \"rewards\": -96.613209}, {\"timesteps\": 1293, \"rewards\": -280.662969}, {\"timesteps\": 1294, \"rewards\": -69.638762}, {\"timesteps\": 1295, \"rewards\": -215.213258}, {\"timesteps\": 1296, \"rewards\": 169.033054}, {\"timesteps\": 1297, \"rewards\": 143.944057}, {\"timesteps\": 1298, \"rewards\": -127.705378}, {\"timesteps\": 1299, \"rewards\": 92.109942}, {\"timesteps\": 1300, \"rewards\": -14.203068}, {\"timesteps\": 1301, \"rewards\": -229.727114}, {\"timesteps\": 1302, \"rewards\": -141.702451}, {\"timesteps\": 1303, \"rewards\": -83.443162}, {\"timesteps\": 1304, \"rewards\": -66.575537}, {\"timesteps\": 1305, \"rewards\": 49.761715}, {\"timesteps\": 1306, \"rewards\": -81.684228}, {\"timesteps\": 1307, \"rewards\": 166.018843}, {\"timesteps\": 1308, \"rewards\": -4.517845}, {\"timesteps\": 1309, \"rewards\": 117.946277}, {\"timesteps\": 1310, \"rewards\": 58.764946}, {\"timesteps\": 1311, \"rewards\": -225.099951}, {\"timesteps\": 1312, \"rewards\": -324.464332}, {\"timesteps\": 1313, \"rewards\": 213.426918}, {\"timesteps\": 1314, \"rewards\": 104.185585}, {\"timesteps\": 1315, \"rewards\": -41.413543}, {\"timesteps\": 1316, \"rewards\": 125.603747}, {\"timesteps\": 1317, \"rewards\": 118.487201}, {\"timesteps\": 1318, \"rewards\": 35.948717}, {\"timesteps\": 1319, \"rewards\": 29.927344}, {\"timesteps\": 1320, \"rewards\": -5.26565}, {\"timesteps\": 1321, \"rewards\": -69.485516}, {\"timesteps\": 1322, \"rewards\": 168.438565}, {\"timesteps\": 1323, \"rewards\": 170.476374}, {\"timesteps\": 1324, \"rewards\": 204.645966}, {\"timesteps\": 1325, \"rewards\": -160.222105}, {\"timesteps\": 1326, \"rewards\": 124.897314}, {\"timesteps\": 1327, \"rewards\": 211.705302}, {\"timesteps\": 1328, \"rewards\": 93.021662}, {\"timesteps\": 1329, \"rewards\": 206.654175}, {\"timesteps\": 1330, \"rewards\": 28.410538}, {\"timesteps\": 1331, \"rewards\": -246.699459}, {\"timesteps\": 1332, \"rewards\": 135.201286}, {\"timesteps\": 1333, \"rewards\": 92.567244}, {\"timesteps\": 1334, \"rewards\": 209.258396}, {\"timesteps\": 1335, \"rewards\": -38.13948}, {\"timesteps\": 1336, \"rewards\": -24.949868}, {\"timesteps\": 1337, \"rewards\": -9.219347}, {\"timesteps\": 1338, \"rewards\": 139.153449}, {\"timesteps\": 1339, \"rewards\": -194.450746}, {\"timesteps\": 1340, \"rewards\": -209.963285}, {\"timesteps\": 1341, \"rewards\": -84.520106}, {\"timesteps\": 1342, \"rewards\": 108.39675}, {\"timesteps\": 1343, \"rewards\": 40.615304}, {\"timesteps\": 1344, \"rewards\": 130.798157}, {\"timesteps\": 1345, \"rewards\": -54.500754}, {\"timesteps\": 1346, \"rewards\": 122.198218}, {\"timesteps\": 1347, \"rewards\": 145.952539}, {\"timesteps\": 1348, \"rewards\": 94.060104}, {\"timesteps\": 1349, \"rewards\": -27.024114}, {\"timesteps\": 1350, \"rewards\": -1.528048}, {\"timesteps\": 1351, \"rewards\": 57.882853}, {\"timesteps\": 1352, \"rewards\": 10.462542}, {\"timesteps\": 1353, \"rewards\": 149.864659}, {\"timesteps\": 1354, \"rewards\": 220.266719}, {\"timesteps\": 1355, \"rewards\": 41.240265}, {\"timesteps\": 1356, \"rewards\": -112.37355}, {\"timesteps\": 1357, \"rewards\": -86.120743}, {\"timesteps\": 1358, \"rewards\": -58.711408}, {\"timesteps\": 1359, \"rewards\": -45.124438}, {\"timesteps\": 1360, \"rewards\": -67.44988}, {\"timesteps\": 1361, \"rewards\": 232.606066}, {\"timesteps\": 1362, \"rewards\": -202.980063}, {\"timesteps\": 1363, \"rewards\": 98.195287}, {\"timesteps\": 1364, \"rewards\": 135.083744}, {\"timesteps\": 1365, \"rewards\": -41.986334}, {\"timesteps\": 1366, \"rewards\": -56.004661}, {\"timesteps\": 1367, \"rewards\": 140.626475}, {\"timesteps\": 1368, \"rewards\": 126.748818}, {\"timesteps\": 1369, \"rewards\": -209.729118}, {\"timesteps\": 1370, \"rewards\": -224.042486}, {\"timesteps\": 1371, \"rewards\": -187.170533}, {\"timesteps\": 1372, \"rewards\": 228.452809}, {\"timesteps\": 1373, \"rewards\": 87.761815}, {\"timesteps\": 1374, \"rewards\": -252.724515}, {\"timesteps\": 1375, \"rewards\": -268.982673}, {\"timesteps\": 1376, \"rewards\": -53.443338}, {\"timesteps\": 1377, \"rewards\": 125.402653}, {\"timesteps\": 1378, \"rewards\": 32.93645}, {\"timesteps\": 1379, \"rewards\": -194.368622}, {\"timesteps\": 1380, \"rewards\": -46.485447}, {\"timesteps\": 1381, \"rewards\": 217.051341}, {\"timesteps\": 1382, \"rewards\": 55.783581}, {\"timesteps\": 1383, \"rewards\": 95.400773}, {\"timesteps\": 1384, \"rewards\": -48.762794}, {\"timesteps\": 1385, \"rewards\": 2.335999}, {\"timesteps\": 1386, \"rewards\": -193.241346}, {\"timesteps\": 1387, \"rewards\": -177.402451}, {\"timesteps\": 1388, \"rewards\": -26.861638}, {\"timesteps\": 1389, \"rewards\": 187.031354}, {\"timesteps\": 1390, \"rewards\": -88.639615}, {\"timesteps\": 1391, \"rewards\": -61.204993}, {\"timesteps\": 1392, \"rewards\": 81.632361}, {\"timesteps\": 1393, \"rewards\": 108.925633}, {\"timesteps\": 1394, \"rewards\": 43.746763}, {\"timesteps\": 1395, \"rewards\": -23.104715}, {\"timesteps\": 1396, \"rewards\": -202.356559}, {\"timesteps\": 1397, \"rewards\": -27.990048}, {\"timesteps\": 1398, \"rewards\": 237.464969}, {\"timesteps\": 1399, \"rewards\": 42.340236}, {\"timesteps\": 1400, \"rewards\": -69.113147}, {\"timesteps\": 1401, \"rewards\": 219.180008}, {\"timesteps\": 1402, \"rewards\": 203.602333}, {\"timesteps\": 1403, \"rewards\": 106.495832}, {\"timesteps\": 1404, \"rewards\": 93.498835}, {\"timesteps\": 1405, \"rewards\": -29.744736}, {\"timesteps\": 1406, \"rewards\": -251.719705}, {\"timesteps\": 1407, \"rewards\": 132.31667}, {\"timesteps\": 1408, \"rewards\": 123.075637}, {\"timesteps\": 1409, \"rewards\": -166.213077}, {\"timesteps\": 1410, \"rewards\": 209.513969}, {\"timesteps\": 1411, \"rewards\": 204.744316}, {\"timesteps\": 1412, \"rewards\": 100.72344}, {\"timesteps\": 1413, \"rewards\": -206.695085}, {\"timesteps\": 1414, \"rewards\": 158.493611}, {\"timesteps\": 1415, \"rewards\": -183.852579}, {\"timesteps\": 1416, \"rewards\": 16.595662}, {\"timesteps\": 1417, \"rewards\": -30.214797}, {\"timesteps\": 1418, \"rewards\": -61.943413}, {\"timesteps\": 1419, \"rewards\": 229.81376}, {\"timesteps\": 1420, \"rewards\": -24.86191}, {\"timesteps\": 1421, \"rewards\": 144.924223}, {\"timesteps\": 1422, \"rewards\": 134.878026}, {\"timesteps\": 1423, \"rewards\": 53.123997}, {\"timesteps\": 1424, \"rewards\": 43.286867}, {\"timesteps\": 1425, \"rewards\": 251.644576}, {\"timesteps\": 1426, \"rewards\": -201.161905}, {\"timesteps\": 1427, \"rewards\": 134.373621}, {\"timesteps\": 1428, \"rewards\": 75.073662}, {\"timesteps\": 1429, \"rewards\": 183.581666}, {\"timesteps\": 1430, \"rewards\": -36.633971}, {\"timesteps\": 1431, \"rewards\": 170.877152}, {\"timesteps\": 1432, \"rewards\": -269.835318}, {\"timesteps\": 1433, \"rewards\": 235.746369}, {\"timesteps\": 1434, \"rewards\": 144.596064}, {\"timesteps\": 1435, \"rewards\": -134.390129}, {\"timesteps\": 1436, \"rewards\": 144.278066}, {\"timesteps\": 1437, \"rewards\": 131.860007}, {\"timesteps\": 1438, \"rewards\": -45.136844}, {\"timesteps\": 1439, \"rewards\": 29.639747}, {\"timesteps\": 1440, \"rewards\": -62.107788}, {\"timesteps\": 1441, \"rewards\": -288.439317}, {\"timesteps\": 1442, \"rewards\": 154.899472}, {\"timesteps\": 1443, \"rewards\": 198.215047}, {\"timesteps\": 1444, \"rewards\": 1.953922}, {\"timesteps\": 1445, \"rewards\": -20.512484}, {\"timesteps\": 1446, \"rewards\": 175.218029}, {\"timesteps\": 1447, \"rewards\": 161.039869}, {\"timesteps\": 1448, \"rewards\": -42.700692}, {\"timesteps\": 1449, \"rewards\": 112.624668}, {\"timesteps\": 1450, \"rewards\": -201.406847}, {\"timesteps\": 1451, \"rewards\": 169.27135}, {\"timesteps\": 1452, \"rewards\": 109.325271}, {\"timesteps\": 1453, \"rewards\": 119.209631}, {\"timesteps\": 1454, \"rewards\": 36.952683}, {\"timesteps\": 1455, \"rewards\": -193.177146}, {\"timesteps\": 1456, \"rewards\": 209.289275}, {\"timesteps\": 1457, \"rewards\": -233.944403}, {\"timesteps\": 1458, \"rewards\": 190.45209}, {\"timesteps\": 1459, \"rewards\": 203.074959}, {\"timesteps\": 1460, \"rewards\": 241.563937}, {\"timesteps\": 1461, \"rewards\": 225.187127}, {\"timesteps\": 1462, \"rewards\": 79.605453}, {\"timesteps\": 1463, \"rewards\": 104.125086}, {\"timesteps\": 1464, \"rewards\": -87.598447}, {\"timesteps\": 1465, \"rewards\": -23.259081}, {\"timesteps\": 1466, \"rewards\": -3.893679}, {\"timesteps\": 1467, \"rewards\": -62.319814}, {\"timesteps\": 1468, \"rewards\": 160.301345}, {\"timesteps\": 1469, \"rewards\": -39.128393}, {\"timesteps\": 1470, \"rewards\": 168.287698}, {\"timesteps\": 1471, \"rewards\": -21.22557}, {\"timesteps\": 1472, \"rewards\": 167.558494}, {\"timesteps\": 1473, \"rewards\": 109.577847}, {\"timesteps\": 1474, \"rewards\": 186.045924}, {\"timesteps\": 1475, \"rewards\": -241.693431}, {\"timesteps\": 1476, \"rewards\": 132.529472}, {\"timesteps\": 1477, \"rewards\": 115.542157}, {\"timesteps\": 1478, \"rewards\": 55.602565}, {\"timesteps\": 1479, \"rewards\": 146.407609}, {\"timesteps\": 1480, \"rewards\": -72.347244}, {\"timesteps\": 1481, \"rewards\": 173.451585}, {\"timesteps\": 1482, \"rewards\": 142.66953}, {\"timesteps\": 1483, \"rewards\": -39.58213}, {\"timesteps\": 1484, \"rewards\": 133.697726}, {\"timesteps\": 1485, \"rewards\": 243.2542}, {\"timesteps\": 1486, \"rewards\": 142.738495}, {\"timesteps\": 1487, \"rewards\": 190.749002}, {\"timesteps\": 1488, \"rewards\": 61.339308}, {\"timesteps\": 1489, \"rewards\": 120.744624}, {\"timesteps\": 1490, \"rewards\": 211.527299}, {\"timesteps\": 1491, \"rewards\": -158.688949}, {\"timesteps\": 1492, \"rewards\": 247.673277}, {\"timesteps\": 1493, \"rewards\": 13.963637}, {\"timesteps\": 1494, \"rewards\": -190.103429}, {\"timesteps\": 1495, \"rewards\": 2.821581}, {\"timesteps\": 1496, \"rewards\": -291.833531}, {\"timesteps\": 1497, \"rewards\": 233.058998}, {\"timesteps\": 1498, \"rewards\": 163.733928}, {\"timesteps\": 1499, \"rewards\": 26.301165}, {\"timesteps\": 1500, \"rewards\": -37.769953}, {\"timesteps\": 1501, \"rewards\": 171.61563}, {\"timesteps\": 1502, \"rewards\": -317.042004}, {\"timesteps\": 1503, \"rewards\": -95.252148}, {\"timesteps\": 1504, \"rewards\": 114.821107}, {\"timesteps\": 1505, \"rewards\": -234.603561}, {\"timesteps\": 1506, \"rewards\": 182.057316}, {\"timesteps\": 1507, \"rewards\": -49.124358}, {\"timesteps\": 1508, \"rewards\": 194.16119}, {\"timesteps\": 1509, \"rewards\": 107.663073}, {\"timesteps\": 1510, \"rewards\": -163.978467}, {\"timesteps\": 1511, \"rewards\": -57.783151}, {\"timesteps\": 1512, \"rewards\": -150.772677}, {\"timesteps\": 1513, \"rewards\": 5.443453}, {\"timesteps\": 1514, \"rewards\": 51.57075}, {\"timesteps\": 1515, \"rewards\": 52.926361}, {\"timesteps\": 1516, \"rewards\": 132.811911}, {\"timesteps\": 1517, \"rewards\": 125.190106}, {\"timesteps\": 1518, \"rewards\": 64.562136}, {\"timesteps\": 1519, \"rewards\": 96.100493}, {\"timesteps\": 1520, \"rewards\": 170.355452}, {\"timesteps\": 1521, \"rewards\": 63.579665}, {\"timesteps\": 1522, \"rewards\": 19.470435}, {\"timesteps\": 1523, \"rewards\": -128.773475}, {\"timesteps\": 1524, \"rewards\": 157.91215}, {\"timesteps\": 1525, \"rewards\": -250.828588}, {\"timesteps\": 1526, \"rewards\": 98.019624}, {\"timesteps\": 1527, \"rewards\": 77.295999}, {\"timesteps\": 1528, \"rewards\": 102.584151}, {\"timesteps\": 1529, \"rewards\": 118.043952}, {\"timesteps\": 1530, \"rewards\": -46.977135}, {\"timesteps\": 1531, \"rewards\": -62.625376}, {\"timesteps\": 1532, \"rewards\": 101.287044}, {\"timesteps\": 1533, \"rewards\": 89.962179}, {\"timesteps\": 1534, \"rewards\": -145.042283}, {\"timesteps\": 1535, \"rewards\": 49.678795}, {\"timesteps\": 1536, \"rewards\": 190.741177}, {\"timesteps\": 1537, \"rewards\": -92.647578}, {\"timesteps\": 1538, \"rewards\": 118.369069}, {\"timesteps\": 1539, \"rewards\": 155.521489}, {\"timesteps\": 1540, \"rewards\": -55.088421}, {\"timesteps\": 1541, \"rewards\": -37.310777}, {\"timesteps\": 1542, \"rewards\": 183.694883}, {\"timesteps\": 1543, \"rewards\": -123.686869}, {\"timesteps\": 1544, \"rewards\": 44.14965}, {\"timesteps\": 1545, \"rewards\": 25.556204}, {\"timesteps\": 1546, \"rewards\": -69.780986}, {\"timesteps\": 1547, \"rewards\": -242.065186}, {\"timesteps\": 1548, \"rewards\": 199.798764}, {\"timesteps\": 1549, \"rewards\": -29.698104}, {\"timesteps\": 1550, \"rewards\": -32.398961}, {\"timesteps\": 1551, \"rewards\": -69.349288}, {\"timesteps\": 1552, \"rewards\": 177.456259}, {\"timesteps\": 1553, \"rewards\": -31.194133}, {\"timesteps\": 1554, \"rewards\": -229.697382}, {\"timesteps\": 1555, \"rewards\": 224.755561}, {\"timesteps\": 1556, \"rewards\": -207.508649}, {\"timesteps\": 1557, \"rewards\": 107.745871}, {\"timesteps\": 1558, \"rewards\": 139.977028}, {\"timesteps\": 1559, \"rewards\": 119.354053}, {\"timesteps\": 1560, \"rewards\": -103.312285}, {\"timesteps\": 1561, \"rewards\": -113.52389}, {\"timesteps\": 1562, \"rewards\": -74.517763}, {\"timesteps\": 1563, \"rewards\": 169.866315}, {\"timesteps\": 1564, \"rewards\": 69.398768}, {\"timesteps\": 1565, \"rewards\": 204.979514}, {\"timesteps\": 1566, \"rewards\": 189.05377}, {\"timesteps\": 1567, \"rewards\": 102.120882}, {\"timesteps\": 1568, \"rewards\": 53.508733}, {\"timesteps\": 1569, \"rewards\": -35.375696}, {\"timesteps\": 1570, \"rewards\": 145.248328}, {\"timesteps\": 1571, \"rewards\": 112.249052}, {\"timesteps\": 1572, \"rewards\": 86.75017}, {\"timesteps\": 1573, \"rewards\": 80.216266}, {\"timesteps\": 1574, \"rewards\": 154.431814}, {\"timesteps\": 1575, \"rewards\": 164.120124}, {\"timesteps\": 1576, \"rewards\": 106.974901}, {\"timesteps\": 1577, \"rewards\": 51.516969}, {\"timesteps\": 1578, \"rewards\": -135.846358}, {\"timesteps\": 1579, \"rewards\": -143.188252}, {\"timesteps\": 1580, \"rewards\": 85.173981}, {\"timesteps\": 1581, \"rewards\": 33.067463}, {\"timesteps\": 1582, \"rewards\": 82.257118}, {\"timesteps\": 1583, \"rewards\": -183.996008}, {\"timesteps\": 1584, \"rewards\": -34.689687}, {\"timesteps\": 1585, \"rewards\": -49.864573}, {\"timesteps\": 1586, \"rewards\": 172.094249}, {\"timesteps\": 1587, \"rewards\": 186.937337}, {\"timesteps\": 1588, \"rewards\": 78.30432}, {\"timesteps\": 1589, \"rewards\": -158.145122}, {\"timesteps\": 1590, \"rewards\": 211.140717}, {\"timesteps\": 1591, \"rewards\": 34.063332}, {\"timesteps\": 1592, \"rewards\": 96.40671}, {\"timesteps\": 1593, \"rewards\": 41.923526}, {\"timesteps\": 1594, \"rewards\": 96.30077}, {\"timesteps\": 1595, \"rewards\": 38.597529}, {\"timesteps\": 1596, \"rewards\": -53.773008}, {\"timesteps\": 1597, \"rewards\": -51.104846}, {\"timesteps\": 1598, \"rewards\": -45.392516}, {\"timesteps\": 1599, \"rewards\": -164.592395}, {\"timesteps\": 1600, \"rewards\": -38.063473}, {\"timesteps\": 1601, \"rewards\": -50.598073}, {\"timesteps\": 1602, \"rewards\": 112.381647}, {\"timesteps\": 1603, \"rewards\": 222.488678}, {\"timesteps\": 1604, \"rewards\": 47.039175}, {\"timesteps\": 1605, \"rewards\": -89.069025}, {\"timesteps\": 1606, \"rewards\": 201.013353}, {\"timesteps\": 1607, \"rewards\": 44.73587}, {\"timesteps\": 1608, \"rewards\": 68.85883}, {\"timesteps\": 1609, \"rewards\": 181.041471}, {\"timesteps\": 1610, \"rewards\": 159.975091}, {\"timesteps\": 1611, \"rewards\": 144.351267}, {\"timesteps\": 1612, \"rewards\": 120.121444}, {\"timesteps\": 1613, \"rewards\": -148.54851}, {\"timesteps\": 1614, \"rewards\": 223.760912}, {\"timesteps\": 1615, \"rewards\": 188.902787}, {\"timesteps\": 1616, \"rewards\": 36.180208}, {\"timesteps\": 1617, \"rewards\": 64.58186}, {\"timesteps\": 1618, \"rewards\": 59.411316}, {\"timesteps\": 1619, \"rewards\": 92.440175}, {\"timesteps\": 1620, \"rewards\": 175.214921}, {\"timesteps\": 1621, \"rewards\": -199.95633}, {\"timesteps\": 1622, \"rewards\": 110.555659}, {\"timesteps\": 1623, \"rewards\": -91.84998}, {\"timesteps\": 1624, \"rewards\": 81.730958}, {\"timesteps\": 1625, \"rewards\": -82.58649}, {\"timesteps\": 1626, \"rewards\": 148.981929}, {\"timesteps\": 1627, \"rewards\": 129.230287}, {\"timesteps\": 1628, \"rewards\": 61.128815}, {\"timesteps\": 1629, \"rewards\": 142.866735}, {\"timesteps\": 1630, \"rewards\": -121.253409}, {\"timesteps\": 1631, \"rewards\": -135.541337}, {\"timesteps\": 1632, \"rewards\": -111.525884}, {\"timesteps\": 1633, \"rewards\": 13.978537}, {\"timesteps\": 1634, \"rewards\": 157.613376}, {\"timesteps\": 1635, \"rewards\": -112.368693}, {\"timesteps\": 1636, \"rewards\": 158.942648}, {\"timesteps\": 1637, \"rewards\": 58.950173}, {\"timesteps\": 1638, \"rewards\": 78.833311}, {\"timesteps\": 1639, \"rewards\": -151.643232}, {\"timesteps\": 1640, \"rewards\": -122.581161}, {\"timesteps\": 1641, \"rewards\": 191.385821}, {\"timesteps\": 1642, \"rewards\": 122.358346}, {\"timesteps\": 1643, \"rewards\": -183.682253}, {\"timesteps\": 1644, \"rewards\": 113.164943}, {\"timesteps\": 1645, \"rewards\": 93.290324}, {\"timesteps\": 1646, \"rewards\": 94.463879}, {\"timesteps\": 1647, \"rewards\": -127.762465}, {\"timesteps\": 1648, \"rewards\": -104.384482}, {\"timesteps\": 1649, \"rewards\": -166.655012}, {\"timesteps\": 1650, \"rewards\": 4.017808}, {\"timesteps\": 1651, \"rewards\": -146.050223}, {\"timesteps\": 1652, \"rewards\": -114.412344}, {\"timesteps\": 1653, \"rewards\": 72.221609}, {\"timesteps\": 1654, \"rewards\": -99.424303}, {\"timesteps\": 1655, \"rewards\": -187.679443}, {\"timesteps\": 1656, \"rewards\": -110.925194}, {\"timesteps\": 1657, \"rewards\": -149.104136}, {\"timesteps\": 1658, \"rewards\": -150.304331}, {\"timesteps\": 1659, \"rewards\": -157.514929}, {\"timesteps\": 1660, \"rewards\": -104.864361}, {\"timesteps\": 1661, \"rewards\": -207.659306}, {\"timesteps\": 1662, \"rewards\": -84.923089}, {\"timesteps\": 1663, \"rewards\": -129.140751}, {\"timesteps\": 1664, \"rewards\": 39.674165}, {\"timesteps\": 1665, \"rewards\": -145.61201}, {\"timesteps\": 1666, \"rewards\": 153.291057}, {\"timesteps\": 1667, \"rewards\": -132.0746}, {\"timesteps\": 1668, \"rewards\": -169.698434}, {\"timesteps\": 1669, \"rewards\": -193.008224}, {\"timesteps\": 1670, \"rewards\": -126.186364}, {\"timesteps\": 1671, \"rewards\": 94.571873}, {\"timesteps\": 1672, \"rewards\": -172.488887}, {\"timesteps\": 1673, \"rewards\": 85.381404}, {\"timesteps\": 1674, \"rewards\": -121.981667}, {\"timesteps\": 1675, \"rewards\": -250.701397}, {\"timesteps\": 1676, \"rewards\": -19.805363}, {\"timesteps\": 1677, \"rewards\": -7.950504}, {\"timesteps\": 1678, \"rewards\": -187.478914}, {\"timesteps\": 1679, \"rewards\": 146.121843}, {\"timesteps\": 1680, \"rewards\": -50.537925}, {\"timesteps\": 1681, \"rewards\": 288.013035}, {\"timesteps\": 1682, \"rewards\": 4.45419}, {\"timesteps\": 1683, \"rewards\": 28.977863}, {\"timesteps\": 1684, \"rewards\": -138.684024}, {\"timesteps\": 1685, \"rewards\": -118.224573}, {\"timesteps\": 1686, \"rewards\": -143.674855}, {\"timesteps\": 1687, \"rewards\": -112.342677}, {\"timesteps\": 1688, \"rewards\": -139.573641}, {\"timesteps\": 1689, \"rewards\": 96.104684}, {\"timesteps\": 1690, \"rewards\": 209.680982}, {\"timesteps\": 1691, \"rewards\": -201.737318}, {\"timesteps\": 1692, \"rewards\": -208.121461}, {\"timesteps\": 1693, \"rewards\": -130.21496}, {\"timesteps\": 1694, \"rewards\": -180.509384}, {\"timesteps\": 1695, \"rewards\": 99.131428}, {\"timesteps\": 1696, \"rewards\": 41.550622}, {\"timesteps\": 1697, \"rewards\": -43.804527}, {\"timesteps\": 1698, \"rewards\": -131.859865}, {\"timesteps\": 1699, \"rewards\": -191.346105}, {\"timesteps\": 1700, \"rewards\": 21.166114}, {\"timesteps\": 1701, \"rewards\": -118.559264}, {\"timesteps\": 1702, \"rewards\": -115.363415}, {\"timesteps\": 1703, \"rewards\": -122.267841}, {\"timesteps\": 1704, \"rewards\": -174.232102}, {\"timesteps\": 1705, \"rewards\": -163.665123}, {\"timesteps\": 1706, \"rewards\": -212.283334}, {\"timesteps\": 1707, \"rewards\": 167.436222}, {\"timesteps\": 1708, \"rewards\": -61.769435}, {\"timesteps\": 1709, \"rewards\": -206.062613}, {\"timesteps\": 1710, \"rewards\": -109.499509}, {\"timesteps\": 1711, \"rewards\": -127.016321}, {\"timesteps\": 1712, \"rewards\": 87.088639}, {\"timesteps\": 1713, \"rewards\": -94.931285}, {\"timesteps\": 1714, \"rewards\": 143.520314}, {\"timesteps\": 1715, \"rewards\": 118.330582}, {\"timesteps\": 1716, \"rewards\": -109.118509}, {\"timesteps\": 1717, \"rewards\": 54.320729}, {\"timesteps\": 1718, \"rewards\": 76.300827}, {\"timesteps\": 1719, \"rewards\": -100.48022}, {\"timesteps\": 1720, \"rewards\": 110.351806}, {\"timesteps\": 1721, \"rewards\": 107.224953}, {\"timesteps\": 1722, \"rewards\": 92.219284}, {\"timesteps\": 1723, \"rewards\": -116.482541}, {\"timesteps\": 1724, \"rewards\": -100.322257}, {\"timesteps\": 1725, \"rewards\": 155.791966}, {\"timesteps\": 1726, \"rewards\": -90.759816}, {\"timesteps\": 1727, \"rewards\": 93.62118}, {\"timesteps\": 1728, \"rewards\": 51.370294}, {\"timesteps\": 1729, \"rewards\": -119.748698}, {\"timesteps\": 1730, \"rewards\": -85.100531}, {\"timesteps\": 1731, \"rewards\": -42.424638}, {\"timesteps\": 1732, \"rewards\": 146.444443}, {\"timesteps\": 1733, \"rewards\": -76.888712}, {\"timesteps\": 1734, \"rewards\": -110.707714}, {\"timesteps\": 1735, \"rewards\": 171.47425}, {\"timesteps\": 1736, \"rewards\": 153.551096}, {\"timesteps\": 1737, \"rewards\": 39.576009}, {\"timesteps\": 1738, \"rewards\": 143.730866}, {\"timesteps\": 1739, \"rewards\": 118.543064}, {\"timesteps\": 1740, \"rewards\": -229.15645}, {\"timesteps\": 1741, \"rewards\": -190.189419}, {\"timesteps\": 1742, \"rewards\": -119.297021}, {\"timesteps\": 1743, \"rewards\": 32.252723}, {\"timesteps\": 1744, \"rewards\": 10.728262}, {\"timesteps\": 1745, \"rewards\": -29.057334}, {\"timesteps\": 1746, \"rewards\": -203.305022}, {\"timesteps\": 1747, \"rewards\": 244.118182}, {\"timesteps\": 1748, \"rewards\": -142.131514}, {\"timesteps\": 1749, \"rewards\": 5.37845}, {\"timesteps\": 1750, \"rewards\": -127.636795}, {\"timesteps\": 1751, \"rewards\": 153.828174}, {\"timesteps\": 1752, \"rewards\": -90.471107}, {\"timesteps\": 1753, \"rewards\": 187.981778}, {\"timesteps\": 1754, \"rewards\": 125.051772}, {\"timesteps\": 1755, \"rewards\": -208.690817}, {\"timesteps\": 1756, \"rewards\": -141.024923}, {\"timesteps\": 1757, \"rewards\": 33.507485}, {\"timesteps\": 1758, \"rewards\": -90.825505}, {\"timesteps\": 1759, \"rewards\": -139.284144}, {\"timesteps\": 1760, \"rewards\": 270.600475}, {\"timesteps\": 1761, \"rewards\": -63.851104}, {\"timesteps\": 1762, \"rewards\": 166.746695}, {\"timesteps\": 1763, \"rewards\": -111.585959}, {\"timesteps\": 1764, \"rewards\": -137.335458}, {\"timesteps\": 1765, \"rewards\": 206.698902}, {\"timesteps\": 1766, \"rewards\": -194.621221}, {\"timesteps\": 1767, \"rewards\": -104.380615}, {\"timesteps\": 1768, \"rewards\": -83.711368}, {\"timesteps\": 1769, \"rewards\": -118.963644}, {\"timesteps\": 1770, \"rewards\": 168.422996}, {\"timesteps\": 1771, \"rewards\": -105.105174}, {\"timesteps\": 1772, \"rewards\": -191.58715}, {\"timesteps\": 1773, \"rewards\": -77.465686}, {\"timesteps\": 1774, \"rewards\": -217.383913}, {\"timesteps\": 1775, \"rewards\": -32.553077}, {\"timesteps\": 1776, \"rewards\": 158.31599}, {\"timesteps\": 1777, \"rewards\": 136.189972}, {\"timesteps\": 1778, \"rewards\": -196.201492}, {\"timesteps\": 1779, \"rewards\": -114.112643}, {\"timesteps\": 1780, \"rewards\": 240.346551}, {\"timesteps\": 1781, \"rewards\": 156.587839}, {\"timesteps\": 1782, \"rewards\": 198.272193}, {\"timesteps\": 1783, \"rewards\": -138.40301}, {\"timesteps\": 1784, \"rewards\": -159.719857}, {\"timesteps\": 1785, \"rewards\": -132.89652}, {\"timesteps\": 1786, \"rewards\": -220.30225}, {\"timesteps\": 1787, \"rewards\": -138.64446}, {\"timesteps\": 1788, \"rewards\": -132.334011}, {\"timesteps\": 1789, \"rewards\": 154.969995}, {\"timesteps\": 1790, \"rewards\": -207.334091}, {\"timesteps\": 1791, \"rewards\": -120.174185}, {\"timesteps\": 1792, \"rewards\": -128.916679}, {\"timesteps\": 1793, \"rewards\": -109.443316}, {\"timesteps\": 1794, \"rewards\": 101.194731}, {\"timesteps\": 1795, \"rewards\": -75.890396}, {\"timesteps\": 1796, \"rewards\": 202.347294}, {\"timesteps\": 1797, \"rewards\": -171.646151}, {\"timesteps\": 1798, \"rewards\": 157.633847}, {\"timesteps\": 1799, \"rewards\": 84.407966}, {\"timesteps\": 1800, \"rewards\": -9.403863}, {\"timesteps\": 1801, \"rewards\": 163.51936}, {\"timesteps\": 1802, \"rewards\": -158.682934}, {\"timesteps\": 1803, \"rewards\": -142.788852}, {\"timesteps\": 1804, \"rewards\": -104.536367}, {\"timesteps\": 1805, \"rewards\": 275.826654}, {\"timesteps\": 1806, \"rewards\": -100.17083}, {\"timesteps\": 1807, \"rewards\": -107.278194}, {\"timesteps\": 1808, \"rewards\": -132.551767}, {\"timesteps\": 1809, \"rewards\": -67.891109}, {\"timesteps\": 1810, \"rewards\": -119.848891}, {\"timesteps\": 1811, \"rewards\": -118.047177}, {\"timesteps\": 1812, \"rewards\": -101.61643}, {\"timesteps\": 1813, \"rewards\": -97.929327}, {\"timesteps\": 1814, \"rewards\": -134.343598}, {\"timesteps\": 1815, \"rewards\": -130.372549}, {\"timesteps\": 1816, \"rewards\": -226.099687}, {\"timesteps\": 1817, \"rewards\": -114.295191}, {\"timesteps\": 1818, \"rewards\": -95.705946}, {\"timesteps\": 1819, \"rewards\": -86.895038}, {\"timesteps\": 1820, \"rewards\": -202.460834}, {\"timesteps\": 1821, \"rewards\": -87.294826}, {\"timesteps\": 1822, \"rewards\": -128.067236}, {\"timesteps\": 1823, \"rewards\": -122.063569}, {\"timesteps\": 1824, \"rewards\": -197.122914}, {\"timesteps\": 1825, \"rewards\": -141.245776}, {\"timesteps\": 1826, \"rewards\": -91.85498}, {\"timesteps\": 1827, \"rewards\": -123.598826}, {\"timesteps\": 1828, \"rewards\": -91.618216}, {\"timesteps\": 1829, \"rewards\": -227.861997}, {\"timesteps\": 1830, \"rewards\": -102.964058}, {\"timesteps\": 1831, \"rewards\": -173.57019}, {\"timesteps\": 1832, \"rewards\": -73.700816}, {\"timesteps\": 1833, \"rewards\": -112.358619}, {\"timesteps\": 1834, \"rewards\": 138.762826}, {\"timesteps\": 1835, \"rewards\": -101.265021}, {\"timesteps\": 1836, \"rewards\": -95.412747}, {\"timesteps\": 1837, \"rewards\": -130.024105}, {\"timesteps\": 1838, \"rewards\": -44.825274}, {\"timesteps\": 1839, \"rewards\": 63.057074}, {\"timesteps\": 1840, \"rewards\": -96.568395}, {\"timesteps\": 1841, \"rewards\": -181.750939}, {\"timesteps\": 1842, \"rewards\": -208.630796}, {\"timesteps\": 1843, \"rewards\": -81.123737}, {\"timesteps\": 1844, \"rewards\": 158.399427}, {\"timesteps\": 1845, \"rewards\": -111.891671}, {\"timesteps\": 1846, \"rewards\": -100.045649}, {\"timesteps\": 1847, \"rewards\": -79.566797}, {\"timesteps\": 1848, \"rewards\": -81.052623}, {\"timesteps\": 1849, \"rewards\": -243.779541}, {\"timesteps\": 1850, \"rewards\": -112.624309}, {\"timesteps\": 1851, \"rewards\": -116.056389}, {\"timesteps\": 1852, \"rewards\": 79.412489}, {\"timesteps\": 1853, \"rewards\": -103.86415}, {\"timesteps\": 1854, \"rewards\": -110.640541}, {\"timesteps\": 1855, \"rewards\": -155.452098}, {\"timesteps\": 1856, \"rewards\": -118.846392}, {\"timesteps\": 1857, \"rewards\": -130.419416}, {\"timesteps\": 1858, \"rewards\": -139.856591}, {\"timesteps\": 1859, \"rewards\": -169.224613}, {\"timesteps\": 1860, \"rewards\": -158.255441}, {\"timesteps\": 1861, \"rewards\": -219.64634}, {\"timesteps\": 1862, \"rewards\": -181.902456}, {\"timesteps\": 1863, \"rewards\": -124.545897}, {\"timesteps\": 1864, \"rewards\": -205.835904}, {\"timesteps\": 1865, \"rewards\": -208.986173}, {\"timesteps\": 1866, \"rewards\": -144.591897}, {\"timesteps\": 1867, \"rewards\": -112.879396}, {\"timesteps\": 1868, \"rewards\": 101.786081}, {\"timesteps\": 1869, \"rewards\": -206.025052}, {\"timesteps\": 1870, \"rewards\": -119.94439}, {\"timesteps\": 1871, \"rewards\": -98.337712}, {\"timesteps\": 1872, \"rewards\": -126.392099}, {\"timesteps\": 1873, \"rewards\": -89.073029}, {\"timesteps\": 1874, \"rewards\": -92.977491}, {\"timesteps\": 1875, \"rewards\": -177.390134}, {\"timesteps\": 1876, \"rewards\": 59.354221}, {\"timesteps\": 1877, \"rewards\": -91.415106}, {\"timesteps\": 1878, \"rewards\": -107.610373}, {\"timesteps\": 1879, \"rewards\": -141.010835}, {\"timesteps\": 1880, \"rewards\": 158.964306}, {\"timesteps\": 1881, \"rewards\": 145.562327}, {\"timesteps\": 1882, \"rewards\": 132.53082}, {\"timesteps\": 1883, \"rewards\": -108.137195}, {\"timesteps\": 1884, \"rewards\": -198.939738}, {\"timesteps\": 1885, \"rewards\": 213.867338}, {\"timesteps\": 1886, \"rewards\": 34.077355}, {\"timesteps\": 1887, \"rewards\": -100.804086}, {\"timesteps\": 1888, \"rewards\": -100.932456}, {\"timesteps\": 1889, \"rewards\": -117.229903}, {\"timesteps\": 1890, \"rewards\": -163.61652}, {\"timesteps\": 1891, \"rewards\": -133.560003}, {\"timesteps\": 1892, \"rewards\": -222.392119}, {\"timesteps\": 1893, \"rewards\": -109.607552}, {\"timesteps\": 1894, \"rewards\": -98.298626}, {\"timesteps\": 1895, \"rewards\": -112.577728}, {\"timesteps\": 1896, \"rewards\": -80.488302}, {\"timesteps\": 1897, \"rewards\": -117.485947}, {\"timesteps\": 1898, \"rewards\": -95.520635}, {\"timesteps\": 1899, \"rewards\": -69.539062}, {\"timesteps\": 1900, \"rewards\": -134.91708}, {\"timesteps\": 1901, \"rewards\": -103.576746}, {\"timesteps\": 1902, \"rewards\": -134.530545}, {\"timesteps\": 1903, \"rewards\": -110.74199}, {\"timesteps\": 1904, \"rewards\": -211.929556}, {\"timesteps\": 1905, \"rewards\": -144.79986}, {\"timesteps\": 1906, \"rewards\": -98.503175}, {\"timesteps\": 1907, \"rewards\": -90.827493}, {\"timesteps\": 1908, \"rewards\": -107.33694}, {\"timesteps\": 1909, \"rewards\": -59.276606}, {\"timesteps\": 1910, \"rewards\": 136.531597}, {\"timesteps\": 1911, \"rewards\": -94.297592}, {\"timesteps\": 1912, \"rewards\": -87.73227}, {\"timesteps\": 1913, \"rewards\": -110.66427}, {\"timesteps\": 1914, \"rewards\": -72.556519}, {\"timesteps\": 1915, \"rewards\": -47.7848}, {\"timesteps\": 1916, \"rewards\": -123.835072}, {\"timesteps\": 1917, \"rewards\": -155.586216}, {\"timesteps\": 1918, \"rewards\": -110.5942}, {\"timesteps\": 1919, \"rewards\": -64.026035}, {\"timesteps\": 1920, \"rewards\": -8.763499}, {\"timesteps\": 1921, \"rewards\": -135.88336}, {\"timesteps\": 1922, \"rewards\": -96.204916}, {\"timesteps\": 1923, \"rewards\": 54.904091}, {\"timesteps\": 1924, \"rewards\": -123.901323}, {\"timesteps\": 1925, \"rewards\": -193.419079}, {\"timesteps\": 1926, \"rewards\": -102.076546}, {\"timesteps\": 1927, \"rewards\": -89.552903}, {\"timesteps\": 1928, \"rewards\": -55.165118}, {\"timesteps\": 1929, \"rewards\": -133.941832}, {\"timesteps\": 1930, \"rewards\": -162.504869}, {\"timesteps\": 1931, \"rewards\": -200.2695}, {\"timesteps\": 1932, \"rewards\": -113.120783}, {\"timesteps\": 1933, \"rewards\": -59.267287}, {\"timesteps\": 1934, \"rewards\": -88.599038}, {\"timesteps\": 1935, \"rewards\": -121.391604}, {\"timesteps\": 1936, \"rewards\": -74.682506}, {\"timesteps\": 1937, \"rewards\": -103.601449}, {\"timesteps\": 1938, \"rewards\": -131.375596}, {\"timesteps\": 1939, \"rewards\": -110.240398}, {\"timesteps\": 1940, \"rewards\": -105.026209}, {\"timesteps\": 1941, \"rewards\": -67.659895}, {\"timesteps\": 1942, \"rewards\": -77.281581}, {\"timesteps\": 1943, \"rewards\": -104.430605}, {\"timesteps\": 1944, \"rewards\": -146.970391}, {\"timesteps\": 1945, \"rewards\": -112.954191}, {\"timesteps\": 1946, \"rewards\": -95.518275}, {\"timesteps\": 1947, \"rewards\": 40.682543}, {\"timesteps\": 1948, \"rewards\": -123.645996}, {\"timesteps\": 1949, \"rewards\": -113.340888}, {\"timesteps\": 1950, \"rewards\": -89.392881}, {\"timesteps\": 1951, \"rewards\": 172.532117}, {\"timesteps\": 1952, \"rewards\": -190.763563}, {\"timesteps\": 1953, \"rewards\": -248.605631}, {\"timesteps\": 1954, \"rewards\": -141.279135}, {\"timesteps\": 1955, \"rewards\": 155.399376}, {\"timesteps\": 1956, \"rewards\": -135.781387}, {\"timesteps\": 1957, \"rewards\": -123.614025}, {\"timesteps\": 1958, \"rewards\": -96.50802}, {\"timesteps\": 1959, \"rewards\": -104.85465}, {\"timesteps\": 1960, \"rewards\": -134.722887}, {\"timesteps\": 1961, \"rewards\": -86.105823}, {\"timesteps\": 1962, \"rewards\": -104.302744}, {\"timesteps\": 1963, \"rewards\": -100.834333}, {\"timesteps\": 1964, \"rewards\": -128.546088}, {\"timesteps\": 1965, \"rewards\": -21.796579}, {\"timesteps\": 1966, \"rewards\": -130.953945}, {\"timesteps\": 1967, \"rewards\": -193.77307}, {\"timesteps\": 1968, \"rewards\": -92.355199}, {\"timesteps\": 1969, \"rewards\": -101.28062}, {\"timesteps\": 1970, \"rewards\": -106.056505}, {\"timesteps\": 1971, \"rewards\": -86.099839}, {\"timesteps\": 1972, \"rewards\": -109.287926}, {\"timesteps\": 1973, \"rewards\": -116.139084}, {\"timesteps\": 1974, \"rewards\": -113.936654}, {\"timesteps\": 1975, \"rewards\": -93.930968}, {\"timesteps\": 1976, \"rewards\": -103.698193}, {\"timesteps\": 1977, \"rewards\": -82.912993}, {\"timesteps\": 1978, \"rewards\": -128.872843}, {\"timesteps\": 1979, \"rewards\": -210.507662}, {\"timesteps\": 1980, \"rewards\": -93.33852}, {\"timesteps\": 1981, \"rewards\": -139.250732}, {\"timesteps\": 1982, \"rewards\": -30.977839}, {\"timesteps\": 1983, \"rewards\": -68.835449}, {\"timesteps\": 1984, \"rewards\": -77.389877}, {\"timesteps\": 1985, \"rewards\": -145.683548}, {\"timesteps\": 1986, \"rewards\": -75.272383}, {\"timesteps\": 1987, \"rewards\": -128.348369}, {\"timesteps\": 1988, \"rewards\": -75.784828}, {\"timesteps\": 1989, \"rewards\": -117.714031}, {\"timesteps\": 1990, \"rewards\": -127.165296}, {\"timesteps\": 1991, \"rewards\": -165.8208}, {\"timesteps\": 1992, \"rewards\": -167.027096}, {\"timesteps\": 1993, \"rewards\": 135.701863}, {\"timesteps\": 1994, \"rewards\": -82.312158}, {\"timesteps\": 1995, \"rewards\": 67.127368}, {\"timesteps\": 1996, \"rewards\": 103.20412}, {\"timesteps\": 1997, \"rewards\": -86.569737}, {\"timesteps\": 1998, \"rewards\": -69.634995}, {\"timesteps\": 1999, \"rewards\": 67.042411}, {\"timesteps\": 2000, \"rewards\": 215.637214}, {\"timesteps\": 2001, \"rewards\": -100.071827}, {\"timesteps\": 2002, \"rewards\": -203.445823}, {\"timesteps\": 2003, \"rewards\": -159.94477}, {\"timesteps\": 2004, \"rewards\": -149.898799}, {\"timesteps\": 2005, \"rewards\": -93.791583}, {\"timesteps\": 2006, \"rewards\": -140.246987}, {\"timesteps\": 2007, \"rewards\": -60.776198}, {\"timesteps\": 2008, \"rewards\": -97.26543}, {\"timesteps\": 2009, \"rewards\": -82.760315}, {\"timesteps\": 2010, \"rewards\": -111.184704}, {\"timesteps\": 2011, \"rewards\": -133.226756}, {\"timesteps\": 2012, \"rewards\": -101.294162}, {\"timesteps\": 2013, \"rewards\": -65.966204}, {\"timesteps\": 2014, \"rewards\": -78.212611}, {\"timesteps\": 2015, \"rewards\": -124.968212}, {\"timesteps\": 2016, \"rewards\": -98.831416}, {\"timesteps\": 2017, \"rewards\": -60.021511}, {\"timesteps\": 2018, \"rewards\": -91.132602}, {\"timesteps\": 2019, \"rewards\": -90.533046}, {\"timesteps\": 2020, \"rewards\": 178.698849}, {\"timesteps\": 2021, \"rewards\": -174.842053}, {\"timesteps\": 2022, \"rewards\": -183.145073}, {\"timesteps\": 2023, \"rewards\": -80.022472}, {\"timesteps\": 2024, \"rewards\": -103.659583}, {\"timesteps\": 2025, \"rewards\": -92.178073}, {\"timesteps\": 2026, \"rewards\": -93.745913}, {\"timesteps\": 2027, \"rewards\": -92.358275}, {\"timesteps\": 2028, \"rewards\": -80.127579}, {\"timesteps\": 2029, \"rewards\": -39.687962}, {\"timesteps\": 2030, \"rewards\": -119.798864}, {\"timesteps\": 2031, \"rewards\": -85.319182}, {\"timesteps\": 2032, \"rewards\": -61.76278}, {\"timesteps\": 2033, \"rewards\": -107.040241}, {\"timesteps\": 2034, \"rewards\": -98.993938}, {\"timesteps\": 2035, \"rewards\": -198.910378}, {\"timesteps\": 2036, \"rewards\": -136.721755}, {\"timesteps\": 2037, \"rewards\": -109.629851}, {\"timesteps\": 2038, \"rewards\": -118.525427}, {\"timesteps\": 2039, \"rewards\": -66.154378}, {\"timesteps\": 2040, \"rewards\": -148.668657}, {\"timesteps\": 2041, \"rewards\": -81.534306}, {\"timesteps\": 2042, \"rewards\": -80.990093}, {\"timesteps\": 2043, \"rewards\": -105.032232}, {\"timesteps\": 2044, \"rewards\": -97.949219}, {\"timesteps\": 2045, \"rewards\": -104.609078}, {\"timesteps\": 2046, \"rewards\": -105.628347}, {\"timesteps\": 2047, \"rewards\": -84.015504}, {\"timesteps\": 2048, \"rewards\": -56.789748}, {\"timesteps\": 2049, \"rewards\": -72.31389}, {\"timesteps\": 2050, \"rewards\": -172.036546}, {\"timesteps\": 2051, \"rewards\": -211.932899}, {\"timesteps\": 2052, \"rewards\": -90.137187}, {\"timesteps\": 2053, \"rewards\": -68.72423}, {\"timesteps\": 2054, \"rewards\": -58.863037}, {\"timesteps\": 2055, \"rewards\": -85.275296}, {\"timesteps\": 2056, \"rewards\": -82.545149}, {\"timesteps\": 2057, \"rewards\": -99.0942}, {\"timesteps\": 2058, \"rewards\": -68.162965}, {\"timesteps\": 2059, \"rewards\": -69.629357}, {\"timesteps\": 2060, \"rewards\": -180.490145}, {\"timesteps\": 2061, \"rewards\": -87.266559}, {\"timesteps\": 2062, \"rewards\": -85.891215}, {\"timesteps\": 2063, \"rewards\": -96.493872}, {\"timesteps\": 2064, \"rewards\": -111.811374}, {\"timesteps\": 2065, \"rewards\": -216.382052}, {\"timesteps\": 2066, \"rewards\": -88.460013}, {\"timesteps\": 2067, \"rewards\": -82.006588}, {\"timesteps\": 2068, \"rewards\": -86.812686}, {\"timesteps\": 2069, \"rewards\": -134.914677}, {\"timesteps\": 2070, \"rewards\": -153.305837}, {\"timesteps\": 2071, \"rewards\": -160.227579}, {\"timesteps\": 2072, \"rewards\": -82.027821}, {\"timesteps\": 2073, \"rewards\": -64.117195}, {\"timesteps\": 2074, \"rewards\": -104.672412}, {\"timesteps\": 2075, \"rewards\": -154.099224}, {\"timesteps\": 2076, \"rewards\": -117.742376}, {\"timesteps\": 2077, \"rewards\": -70.162676}, {\"timesteps\": 2078, \"rewards\": -80.135349}, {\"timesteps\": 2079, \"rewards\": -100.646334}, {\"timesteps\": 2080, \"rewards\": -89.843174}, {\"timesteps\": 2081, \"rewards\": -110.871302}, {\"timesteps\": 2082, \"rewards\": -115.638688}, {\"timesteps\": 2083, \"rewards\": -83.56938}, {\"timesteps\": 2084, \"rewards\": -164.549205}, {\"timesteps\": 2085, \"rewards\": -102.860968}, {\"timesteps\": 2086, \"rewards\": -75.015453}, {\"timesteps\": 2087, \"rewards\": -94.493697}, {\"timesteps\": 2088, \"rewards\": -70.406172}, {\"timesteps\": 2089, \"rewards\": -145.980184}, {\"timesteps\": 2090, \"rewards\": -88.719053}, {\"timesteps\": 2091, \"rewards\": -133.498833}, {\"timesteps\": 2092, \"rewards\": -98.299817}, {\"timesteps\": 2093, \"rewards\": -109.415699}, {\"timesteps\": 2094, \"rewards\": -111.376654}, {\"timesteps\": 2095, \"rewards\": -101.195934}, {\"timesteps\": 2096, \"rewards\": -104.375448}, {\"timesteps\": 2097, \"rewards\": -89.074427}, {\"timesteps\": 2098, \"rewards\": -127.996668}, {\"timesteps\": 2099, \"rewards\": -91.970921}, {\"timesteps\": 2100, \"rewards\": -207.607809}, {\"timesteps\": 2101, \"rewards\": -95.734596}, {\"timesteps\": 2102, \"rewards\": -124.691784}, {\"timesteps\": 2103, \"rewards\": -110.594}, {\"timesteps\": 2104, \"rewards\": -102.006072}, {\"timesteps\": 2105, \"rewards\": 209.942373}, {\"timesteps\": 2106, \"rewards\": -88.77795}, {\"timesteps\": 2107, \"rewards\": -64.002049}, {\"timesteps\": 2108, \"rewards\": -71.321986}, {\"timesteps\": 2109, \"rewards\": -87.069287}, {\"timesteps\": 2110, \"rewards\": -152.211003}, {\"timesteps\": 2111, \"rewards\": -172.650341}, {\"timesteps\": 2112, \"rewards\": -85.842265}, {\"timesteps\": 2113, \"rewards\": -77.685693}, {\"timesteps\": 2114, \"rewards\": -123.626386}, {\"timesteps\": 2115, \"rewards\": -152.933267}, {\"timesteps\": 2116, \"rewards\": -87.880447}, {\"timesteps\": 2117, \"rewards\": -89.40622}, {\"timesteps\": 2118, \"rewards\": -63.363385}, {\"timesteps\": 2119, \"rewards\": -112.512417}, {\"timesteps\": 2120, \"rewards\": -97.810333}, {\"timesteps\": 2121, \"rewards\": -104.305409}, {\"timesteps\": 2122, \"rewards\": -75.972273}, {\"timesteps\": 2123, \"rewards\": -43.26365}, {\"timesteps\": 2124, \"rewards\": -78.601147}, {\"timesteps\": 2125, \"rewards\": -180.907183}, {\"timesteps\": 2126, \"rewards\": -98.897971}, {\"timesteps\": 2127, \"rewards\": -61.236995}, {\"timesteps\": 2128, \"rewards\": -81.133888}, {\"timesteps\": 2129, \"rewards\": -59.91715}, {\"timesteps\": 2130, \"rewards\": -126.386059}, {\"timesteps\": 2131, \"rewards\": -154.870619}, {\"timesteps\": 2132, \"rewards\": -162.708112}, {\"timesteps\": 2133, \"rewards\": -66.921191}, {\"timesteps\": 2134, \"rewards\": -82.611555}, {\"timesteps\": 2135, \"rewards\": -148.342896}, {\"timesteps\": 2136, \"rewards\": -220.606199}, {\"timesteps\": 2137, \"rewards\": -62.319399}, {\"timesteps\": 2138, \"rewards\": -84.313938}, {\"timesteps\": 2139, \"rewards\": -70.655193}, {\"timesteps\": 2140, \"rewards\": -138.678615}, {\"timesteps\": 2141, \"rewards\": -87.648071}, {\"timesteps\": 2142, \"rewards\": -96.065516}, {\"timesteps\": 2143, \"rewards\": -54.086574}, {\"timesteps\": 2144, \"rewards\": -76.95841}, {\"timesteps\": 2145, \"rewards\": -189.455213}, {\"timesteps\": 2146, \"rewards\": -85.768295}, {\"timesteps\": 2147, \"rewards\": -124.109313}, {\"timesteps\": 2148, \"rewards\": -61.827528}, {\"timesteps\": 2149, \"rewards\": -116.852932}, {\"timesteps\": 2150, \"rewards\": -140.491211}, {\"timesteps\": 2151, \"rewards\": -89.323386}, {\"timesteps\": 2152, \"rewards\": -55.017018}, {\"timesteps\": 2153, \"rewards\": -103.193796}, {\"timesteps\": 2154, \"rewards\": -80.733473}, {\"timesteps\": 2155, \"rewards\": -186.80105}, {\"timesteps\": 2156, \"rewards\": -64.934396}, {\"timesteps\": 2157, \"rewards\": -108.410325}, {\"timesteps\": 2158, \"rewards\": -92.588389}, {\"timesteps\": 2159, \"rewards\": -104.534394}, {\"timesteps\": 2160, \"rewards\": -165.772087}, {\"timesteps\": 2161, \"rewards\": -114.301146}, {\"timesteps\": 2162, \"rewards\": -198.256671}, {\"timesteps\": 2163, \"rewards\": -109.999648}, {\"timesteps\": 2164, \"rewards\": -39.632243}, {\"timesteps\": 2165, \"rewards\": -133.710913}, {\"timesteps\": 2166, \"rewards\": -161.689742}, {\"timesteps\": 2167, \"rewards\": -74.364286}, {\"timesteps\": 2168, \"rewards\": -90.357228}, {\"timesteps\": 2169, \"rewards\": -64.267902}, {\"timesteps\": 2170, \"rewards\": -178.332099}, {\"timesteps\": 2171, \"rewards\": -118.904672}, {\"timesteps\": 2172, \"rewards\": -87.813986}, {\"timesteps\": 2173, \"rewards\": -73.907826}, {\"timesteps\": 2174, \"rewards\": -95.821216}, {\"timesteps\": 2175, \"rewards\": -153.990701}, {\"timesteps\": 2176, \"rewards\": -76.354615}, {\"timesteps\": 2177, \"rewards\": -54.338342}, {\"timesteps\": 2178, \"rewards\": -75.154917}, {\"timesteps\": 2179, \"rewards\": -110.527629}, {\"timesteps\": 2180, \"rewards\": -140.705304}, {\"timesteps\": 2181, \"rewards\": -105.076836}, {\"timesteps\": 2182, \"rewards\": -88.009316}, {\"timesteps\": 2183, \"rewards\": -111.074391}, {\"timesteps\": 2184, \"rewards\": -112.719513}, {\"timesteps\": 2185, \"rewards\": -111.021167}, {\"timesteps\": 2186, \"rewards\": -110.982275}, {\"timesteps\": 2187, \"rewards\": -74.901428}, {\"timesteps\": 2188, \"rewards\": -60.024166}, {\"timesteps\": 2189, \"rewards\": -85.686298}, {\"timesteps\": 2190, \"rewards\": -196.147206}, {\"timesteps\": 2191, \"rewards\": -93.258666}, {\"timesteps\": 2192, \"rewards\": -88.245013}, {\"timesteps\": 2193, \"rewards\": -79.61841}, {\"timesteps\": 2194, \"rewards\": -73.449589}, {\"timesteps\": 2195, \"rewards\": -160.941015}, {\"timesteps\": 2196, \"rewards\": -79.156415}, {\"timesteps\": 2197, \"rewards\": -70.283461}, {\"timesteps\": 2198, \"rewards\": -71.33068}, {\"timesteps\": 2199, \"rewards\": -112.989871}, {\"timesteps\": 2200, \"rewards\": -84.813554}, {\"timesteps\": 2201, \"rewards\": -112.035331}, {\"timesteps\": 2202, \"rewards\": 91.181417}, {\"timesteps\": 2203, \"rewards\": -125.555956}, {\"timesteps\": 2204, \"rewards\": -128.361656}, {\"timesteps\": 2205, \"rewards\": -117.328484}, {\"timesteps\": 2206, \"rewards\": -206.375872}, {\"timesteps\": 2207, \"rewards\": -190.863415}, {\"timesteps\": 2208, \"rewards\": -58.506105}, {\"timesteps\": 2209, \"rewards\": -59.547389}, {\"timesteps\": 2210, \"rewards\": -29.225854}, {\"timesteps\": 2211, \"rewards\": -123.761786}, {\"timesteps\": 2212, \"rewards\": -74.049896}, {\"timesteps\": 2213, \"rewards\": -105.383749}, {\"timesteps\": 2214, \"rewards\": -101.616089}, {\"timesteps\": 2215, \"rewards\": -178.342283}, {\"timesteps\": 2216, \"rewards\": -81.395298}, {\"timesteps\": 2217, \"rewards\": -51.792855}, {\"timesteps\": 2218, \"rewards\": -57.924581}, {\"timesteps\": 2219, \"rewards\": 168.876888}, {\"timesteps\": 2220, \"rewards\": -64.410951}, {\"timesteps\": 2221, \"rewards\": -79.144063}, {\"timesteps\": 2222, \"rewards\": -70.432139}, {\"timesteps\": 2223, \"rewards\": -62.772809}, {\"timesteps\": 2224, \"rewards\": -38.998125}, {\"timesteps\": 2225, \"rewards\": -54.950231}, {\"timesteps\": 2226, \"rewards\": -79.14251}, {\"timesteps\": 2227, \"rewards\": -43.445501}, {\"timesteps\": 2228, \"rewards\": -43.121974}, {\"timesteps\": 2229, \"rewards\": -101.446317}, {\"timesteps\": 2230, \"rewards\": -95.595632}, {\"timesteps\": 2231, \"rewards\": -66.447884}, {\"timesteps\": 2232, \"rewards\": -113.658067}, {\"timesteps\": 2233, \"rewards\": -100.186921}, {\"timesteps\": 2234, \"rewards\": -64.227186}, {\"timesteps\": 2235, \"rewards\": -191.157588}, {\"timesteps\": 2236, \"rewards\": -79.355163}, {\"timesteps\": 2237, \"rewards\": -84.941903}, {\"timesteps\": 2238, \"rewards\": -120.755619}, {\"timesteps\": 2239, \"rewards\": -82.218917}, {\"timesteps\": 2240, \"rewards\": -73.052038}, {\"timesteps\": 2241, \"rewards\": -39.942749}, {\"timesteps\": 2242, \"rewards\": -112.175254}, {\"timesteps\": 2243, \"rewards\": -97.759908}, {\"timesteps\": 2244, \"rewards\": -170.79568}, {\"timesteps\": 2245, \"rewards\": -89.195306}, {\"timesteps\": 2246, \"rewards\": -104.527363}, {\"timesteps\": 2247, \"rewards\": -78.973873}, {\"timesteps\": 2248, \"rewards\": -50.08431}, {\"timesteps\": 2249, \"rewards\": -66.54792}, {\"timesteps\": 2250, \"rewards\": -179.925245}, {\"timesteps\": 2251, \"rewards\": -99.961184}, {\"timesteps\": 2252, \"rewards\": -85.165632}, {\"timesteps\": 2253, \"rewards\": -59.416715}, {\"timesteps\": 2254, \"rewards\": -60.895054}, {\"timesteps\": 2255, \"rewards\": -73.641956}, {\"timesteps\": 2256, \"rewards\": -111.39411}, {\"timesteps\": 2257, \"rewards\": -79.36237}, {\"timesteps\": 2258, \"rewards\": -88.455212}, {\"timesteps\": 2259, \"rewards\": -113.842667}, {\"timesteps\": 2260, \"rewards\": -141.07871}, {\"timesteps\": 2261, \"rewards\": -27.371994}, {\"timesteps\": 2262, \"rewards\": -75.337048}, {\"timesteps\": 2263, \"rewards\": -34.336823}, {\"timesteps\": 2264, \"rewards\": -171.71133}, {\"timesteps\": 2265, \"rewards\": -83.102508}, {\"timesteps\": 2266, \"rewards\": -26.488607}, {\"timesteps\": 2267, \"rewards\": -93.354035}, {\"timesteps\": 2268, \"rewards\": -38.726213}, {\"timesteps\": 2269, \"rewards\": -96.422725}, {\"timesteps\": 2270, \"rewards\": -89.162859}, {\"timesteps\": 2271, \"rewards\": -110.701715}, {\"timesteps\": 2272, \"rewards\": -91.122257}, {\"timesteps\": 2273, \"rewards\": -93.16634}, {\"timesteps\": 2274, \"rewards\": -33.658183}, {\"timesteps\": 2275, \"rewards\": -84.735136}, {\"timesteps\": 2276, \"rewards\": -83.252193}, {\"timesteps\": 2277, \"rewards\": -96.291798}, {\"timesteps\": 2278, \"rewards\": -91.618049}, {\"timesteps\": 2279, \"rewards\": -106.165861}, {\"timesteps\": 2280, \"rewards\": -99.036798}, {\"timesteps\": 2281, \"rewards\": -87.992345}, {\"timesteps\": 2282, \"rewards\": -115.077504}, {\"timesteps\": 2283, \"rewards\": -112.267599}, {\"timesteps\": 2284, \"rewards\": -62.961728}, {\"timesteps\": 2285, \"rewards\": -80.076505}, {\"timesteps\": 2286, \"rewards\": -99.705209}, {\"timesteps\": 2287, \"rewards\": -77.739016}, {\"timesteps\": 2288, \"rewards\": -86.517218}, {\"timesteps\": 2289, \"rewards\": -59.593268}, {\"timesteps\": 2290, \"rewards\": -97.353216}, {\"timesteps\": 2291, \"rewards\": -63.229757}, {\"timesteps\": 2292, \"rewards\": -106.382021}, {\"timesteps\": 2293, \"rewards\": -82.267885}, {\"timesteps\": 2294, \"rewards\": -32.495612}, {\"timesteps\": 2295, \"rewards\": -81.005156}, {\"timesteps\": 2296, \"rewards\": -73.985447}, {\"timesteps\": 2297, \"rewards\": -101.192321}, {\"timesteps\": 2298, \"rewards\": -103.572389}, {\"timesteps\": 2299, \"rewards\": -77.893647}, {\"timesteps\": 2300, \"rewards\": -181.699731}, {\"timesteps\": 2301, \"rewards\": -192.754498}, {\"timesteps\": 2302, \"rewards\": -104.347963}, {\"timesteps\": 2303, \"rewards\": -65.558147}, {\"timesteps\": 2304, \"rewards\": -84.894471}, {\"timesteps\": 2305, \"rewards\": -95.620402}, {\"timesteps\": 2306, \"rewards\": -73.593415}, {\"timesteps\": 2307, \"rewards\": -69.618052}, {\"timesteps\": 2308, \"rewards\": -46.193659}, {\"timesteps\": 2309, \"rewards\": -82.632881}, {\"timesteps\": 2310, \"rewards\": -174.106479}, {\"timesteps\": 2311, \"rewards\": -62.909473}, {\"timesteps\": 2312, \"rewards\": -104.018419}, {\"timesteps\": 2313, \"rewards\": -101.066189}, {\"timesteps\": 2314, \"rewards\": -68.498107}, {\"timesteps\": 2315, \"rewards\": -46.336501}, {\"timesteps\": 2316, \"rewards\": -75.411131}, {\"timesteps\": 2317, \"rewards\": -84.944201}, {\"timesteps\": 2318, \"rewards\": -114.708585}, {\"timesteps\": 2319, \"rewards\": -116.326462}, {\"timesteps\": 2320, \"rewards\": -125.064628}, {\"timesteps\": 2321, \"rewards\": -73.985787}, {\"timesteps\": 2322, \"rewards\": -66.449035}, {\"timesteps\": 2323, \"rewards\": -57.408003}, {\"timesteps\": 2324, \"rewards\": -207.062972}, {\"timesteps\": 2325, \"rewards\": -159.191628}, {\"timesteps\": 2326, \"rewards\": -182.992908}, {\"timesteps\": 2327, \"rewards\": -86.649164}, {\"timesteps\": 2328, \"rewards\": -72.731881}, {\"timesteps\": 2329, \"rewards\": -83.668058}, {\"timesteps\": 2330, \"rewards\": -115.529557}, {\"timesteps\": 2331, \"rewards\": -99.700646}, {\"timesteps\": 2332, \"rewards\": -85.866938}, {\"timesteps\": 2333, \"rewards\": -114.37481}, {\"timesteps\": 2334, \"rewards\": -67.465677}, {\"timesteps\": 2335, \"rewards\": -66.010043}, {\"timesteps\": 2336, \"rewards\": -66.671716}, {\"timesteps\": 2337, \"rewards\": -76.716872}, {\"timesteps\": 2338, \"rewards\": -70.88818}, {\"timesteps\": 2339, \"rewards\": -115.970867}, {\"timesteps\": 2340, \"rewards\": -142.477907}, {\"timesteps\": 2341, \"rewards\": -112.357734}, {\"timesteps\": 2342, \"rewards\": -99.169959}, {\"timesteps\": 2343, \"rewards\": -138.954636}, {\"timesteps\": 2344, \"rewards\": -76.858954}, {\"timesteps\": 2345, \"rewards\": -102.149506}, {\"timesteps\": 2346, \"rewards\": -139.404479}, {\"timesteps\": 2347, \"rewards\": -111.537172}, {\"timesteps\": 2348, \"rewards\": -63.400803}, {\"timesteps\": 2349, \"rewards\": -82.700121}, {\"timesteps\": 2350, \"rewards\": -156.41749}, {\"timesteps\": 2351, \"rewards\": -83.99573}, {\"timesteps\": 2352, \"rewards\": -64.278901}, {\"timesteps\": 2353, \"rewards\": -105.405788}, {\"timesteps\": 2354, \"rewards\": -103.768309}, {\"timesteps\": 2355, \"rewards\": -122.641375}, {\"timesteps\": 2356, \"rewards\": -74.31502}, {\"timesteps\": 2357, \"rewards\": -84.330265}, {\"timesteps\": 2358, \"rewards\": -34.870159}, {\"timesteps\": 2359, \"rewards\": -105.326882}, {\"timesteps\": 2360, \"rewards\": -71.691095}, {\"timesteps\": 2361, \"rewards\": -116.161533}, {\"timesteps\": 2362, \"rewards\": -59.664216}, {\"timesteps\": 2363, \"rewards\": -85.278223}, {\"timesteps\": 2364, \"rewards\": -87.374327}, {\"timesteps\": 2365, \"rewards\": -106.371081}, {\"timesteps\": 2366, \"rewards\": -29.449643}, {\"timesteps\": 2367, \"rewards\": -26.10459}, {\"timesteps\": 2368, \"rewards\": -99.806613}, {\"timesteps\": 2369, \"rewards\": -77.531768}, {\"timesteps\": 2370, \"rewards\": -217.491571}, {\"timesteps\": 2371, \"rewards\": -59.194701}, {\"timesteps\": 2372, \"rewards\": -111.056607}, {\"timesteps\": 2373, \"rewards\": -114.826865}, {\"timesteps\": 2374, \"rewards\": -45.68192}, {\"timesteps\": 2375, \"rewards\": -161.055612}, {\"timesteps\": 2376, \"rewards\": -147.341326}, {\"timesteps\": 2377, \"rewards\": -175.976519}, {\"timesteps\": 2378, \"rewards\": -83.298603}, {\"timesteps\": 2379, \"rewards\": -79.897303}, {\"timesteps\": 2380, \"rewards\": -205.959127}, {\"timesteps\": 2381, \"rewards\": -59.686149}, {\"timesteps\": 2382, \"rewards\": -62.124856}, {\"timesteps\": 2383, \"rewards\": -6.250097}, {\"timesteps\": 2384, \"rewards\": -87.642838}, {\"timesteps\": 2385, \"rewards\": -69.743492}, {\"timesteps\": 2386, \"rewards\": -31.230577}, {\"timesteps\": 2387, \"rewards\": -51.045191}, {\"timesteps\": 2388, \"rewards\": -44.400405}, {\"timesteps\": 2389, \"rewards\": -114.148167}, {\"timesteps\": 2390, \"rewards\": -35.95096}, {\"timesteps\": 2391, \"rewards\": -80.771932}, {\"timesteps\": 2392, \"rewards\": -92.983186}, {\"timesteps\": 2393, \"rewards\": -24.909517}, {\"timesteps\": 2394, \"rewards\": -32.171342}, {\"timesteps\": 2395, \"rewards\": -198.614768}, {\"timesteps\": 2396, \"rewards\": -62.019537}, {\"timesteps\": 2397, \"rewards\": -70.624589}, {\"timesteps\": 2398, \"rewards\": -102.363778}, {\"timesteps\": 2399, \"rewards\": -87.640729}, {\"timesteps\": 2400, \"rewards\": -154.471316}, {\"timesteps\": 2401, \"rewards\": -81.446809}, {\"timesteps\": 2402, \"rewards\": -52.581947}, {\"timesteps\": 2403, \"rewards\": -53.63414}, {\"timesteps\": 2404, \"rewards\": -66.650068}, {\"timesteps\": 2405, \"rewards\": -41.421338}, {\"timesteps\": 2406, \"rewards\": -69.092601}, {\"timesteps\": 2407, \"rewards\": -65.882961}, {\"timesteps\": 2408, \"rewards\": -86.029141}, {\"timesteps\": 2409, \"rewards\": -60.48143}, {\"timesteps\": 2410, \"rewards\": -79.707155}, {\"timesteps\": 2411, \"rewards\": -55.784875}, {\"timesteps\": 2412, \"rewards\": -81.916884}, {\"timesteps\": 2413, \"rewards\": -96.978635}, {\"timesteps\": 2414, \"rewards\": -96.471374}, {\"timesteps\": 2415, \"rewards\": -71.476286}, {\"timesteps\": 2416, \"rewards\": -105.269322}, {\"timesteps\": 2417, \"rewards\": -67.96276}, {\"timesteps\": 2418, \"rewards\": -113.444431}, {\"timesteps\": 2419, \"rewards\": -66.302916}, {\"timesteps\": 2420, \"rewards\": -86.907578}, {\"timesteps\": 2421, \"rewards\": -121.130604}, {\"timesteps\": 2422, \"rewards\": -73.896649}, {\"timesteps\": 2423, \"rewards\": -52.138873}, {\"timesteps\": 2424, \"rewards\": -94.817066}, {\"timesteps\": 2425, \"rewards\": -62.309156}, {\"timesteps\": 2426, \"rewards\": -70.718759}, {\"timesteps\": 2427, \"rewards\": -114.776584}, {\"timesteps\": 2428, \"rewards\": -92.923281}, {\"timesteps\": 2429, \"rewards\": -78.756312}, {\"timesteps\": 2430, \"rewards\": -96.822254}, {\"timesteps\": 2431, \"rewards\": -82.881776}, {\"timesteps\": 2432, \"rewards\": -102.786552}, {\"timesteps\": 2433, \"rewards\": -81.702035}, {\"timesteps\": 2434, \"rewards\": -84.306123}, {\"timesteps\": 2435, \"rewards\": -97.38839}, {\"timesteps\": 2436, \"rewards\": -119.18886}, {\"timesteps\": 2437, \"rewards\": -138.960069}, {\"timesteps\": 2438, \"rewards\": -97.779047}, {\"timesteps\": 2439, \"rewards\": -59.766178}, {\"timesteps\": 2440, \"rewards\": -172.553033}, {\"timesteps\": 2441, \"rewards\": -101.527628}, {\"timesteps\": 2442, \"rewards\": -72.300055}, {\"timesteps\": 2443, \"rewards\": -53.114747}, {\"timesteps\": 2444, \"rewards\": -98.651392}, {\"timesteps\": 2445, \"rewards\": -145.333134}, {\"timesteps\": 2446, \"rewards\": -85.703234}, {\"timesteps\": 2447, \"rewards\": -107.109763}, {\"timesteps\": 2448, \"rewards\": -63.923997}, {\"timesteps\": 2449, \"rewards\": -74.380994}, {\"timesteps\": 2450, \"rewards\": -59.416686}, {\"timesteps\": 2451, \"rewards\": -66.427359}, {\"timesteps\": 2452, \"rewards\": -51.951568}, {\"timesteps\": 2453, \"rewards\": -83.73178}, {\"timesteps\": 2454, \"rewards\": -63.856119}, {\"timesteps\": 2455, \"rewards\": -64.784632}, {\"timesteps\": 2456, \"rewards\": -54.059021}, {\"timesteps\": 2457, \"rewards\": -88.850165}, {\"timesteps\": 2458, \"rewards\": -81.195879}, {\"timesteps\": 2459, \"rewards\": -84.801723}, {\"timesteps\": 2460, \"rewards\": -88.482485}, {\"timesteps\": 2461, \"rewards\": -122.384852}, {\"timesteps\": 2462, \"rewards\": -56.220135}, {\"timesteps\": 2463, \"rewards\": -82.362693}, {\"timesteps\": 2464, \"rewards\": -58.917036}, {\"timesteps\": 2465, \"rewards\": -80.38968}, {\"timesteps\": 2466, \"rewards\": -67.474588}, {\"timesteps\": 2467, \"rewards\": -87.338304}, {\"timesteps\": 2468, \"rewards\": -49.78076}, {\"timesteps\": 2469, \"rewards\": -112.387717}, {\"timesteps\": 2470, \"rewards\": -49.474855}, {\"timesteps\": 2471, \"rewards\": -71.452067}, {\"timesteps\": 2472, \"rewards\": -76.609185}, {\"timesteps\": 2473, \"rewards\": -94.99659}, {\"timesteps\": 2474, \"rewards\": -104.916323}, {\"timesteps\": 2475, \"rewards\": -51.53862}, {\"timesteps\": 2476, \"rewards\": -149.526645}, {\"timesteps\": 2477, \"rewards\": -56.956437}, {\"timesteps\": 2478, \"rewards\": -99.703202}, {\"timesteps\": 2479, \"rewards\": -51.961143}, {\"timesteps\": 2480, \"rewards\": -110.263796}, {\"timesteps\": 2481, \"rewards\": -93.414846}, {\"timesteps\": 2482, \"rewards\": -86.271563}, {\"timesteps\": 2483, \"rewards\": -57.695148}, {\"timesteps\": 2484, \"rewards\": -75.73842}, {\"timesteps\": 2485, \"rewards\": -83.892192}, {\"timesteps\": 2486, \"rewards\": -118.162114}, {\"timesteps\": 2487, \"rewards\": -113.191862}, {\"timesteps\": 2488, \"rewards\": -81.141425}, {\"timesteps\": 2489, \"rewards\": -68.289255}, {\"timesteps\": 2490, \"rewards\": -81.266094}, {\"timesteps\": 2491, \"rewards\": -79.377628}, {\"timesteps\": 2492, \"rewards\": -80.657285}, {\"timesteps\": 2493, \"rewards\": -64.263164}, {\"timesteps\": 2494, \"rewards\": -87.497959}, {\"timesteps\": 2495, \"rewards\": -58.820108}, {\"timesteps\": 2496, \"rewards\": -127.771307}, {\"timesteps\": 2497, \"rewards\": -95.883653}, {\"timesteps\": 2498, \"rewards\": -118.420742}, {\"timesteps\": 2499, \"rewards\": -101.273783}, {\"timesteps\": 2500, \"rewards\": -159.260718}, {\"timesteps\": 2501, \"rewards\": -75.937174}, {\"timesteps\": 2502, \"rewards\": -75.110424}, {\"timesteps\": 2503, \"rewards\": -106.556476}, {\"timesteps\": 2504, \"rewards\": -57.218612}, {\"timesteps\": 2505, \"rewards\": -65.047658}, {\"timesteps\": 2506, \"rewards\": -88.289604}, {\"timesteps\": 2507, \"rewards\": -96.517407}, {\"timesteps\": 2508, \"rewards\": -76.953933}, {\"timesteps\": 2509, \"rewards\": -75.435311}, {\"timesteps\": 2510, \"rewards\": -84.92735}, {\"timesteps\": 2511, \"rewards\": -99.52762}, {\"timesteps\": 2512, \"rewards\": -64.50417}, {\"timesteps\": 2513, \"rewards\": -60.717626}, {\"timesteps\": 2514, \"rewards\": -66.630665}, {\"timesteps\": 2515, \"rewards\": -327.566973}, {\"timesteps\": 2516, \"rewards\": -134.83609}, {\"timesteps\": 2517, \"rewards\": -131.259488}, {\"timesteps\": 2518, \"rewards\": -105.534505}, {\"timesteps\": 2519, \"rewards\": -541.885442}, {\"timesteps\": 2520, \"rewards\": -125.559752}, {\"timesteps\": 2521, \"rewards\": -100.19554}, {\"timesteps\": 2522, \"rewards\": -450.947414}, {\"timesteps\": 2523, \"rewards\": -139.496455}, {\"timesteps\": 2524, \"rewards\": -137.304378}, {\"timesteps\": 2525, \"rewards\": -477.122275}, {\"timesteps\": 2526, \"rewards\": -146.8181}, {\"timesteps\": 2527, \"rewards\": -580.609651}, {\"timesteps\": 2528, \"rewards\": -645.571113}, {\"timesteps\": 2529, \"rewards\": -105.929868}, {\"timesteps\": 2530, \"rewards\": -433.427325}, {\"timesteps\": 2531, \"rewards\": -144.373311}, {\"timesteps\": 2532, \"rewards\": -534.610847}, {\"timesteps\": 2533, \"rewards\": -128.09658}, {\"timesteps\": 2534, \"rewards\": -109.943455}, {\"timesteps\": 2535, \"rewards\": -98.425037}, {\"timesteps\": 2536, \"rewards\": -5.064912}, {\"timesteps\": 2537, \"rewards\": -118.503554}, {\"timesteps\": 2538, \"rewards\": -140.242504}, {\"timesteps\": 2539, \"rewards\": -643.829667}, {\"timesteps\": 2540, \"rewards\": -395.854865}, {\"timesteps\": 2541, \"rewards\": -559.273858}, {\"timesteps\": 2542, \"rewards\": -137.285439}, {\"timesteps\": 2543, \"rewards\": -610.459246}, {\"timesteps\": 2544, \"rewards\": -142.403021}, {\"timesteps\": 2545, \"rewards\": -96.983825}, {\"timesteps\": 2546, \"rewards\": -97.610855}, {\"timesteps\": 2547, \"rewards\": -118.071161}, {\"timesteps\": 2548, \"rewards\": -414.680875}, {\"timesteps\": 2549, \"rewards\": -480.870252}, {\"timesteps\": 2550, \"rewards\": -139.586484}, {\"timesteps\": 2551, \"rewards\": -164.30118}, {\"timesteps\": 2552, \"rewards\": -138.612151}, {\"timesteps\": 2553, \"rewards\": -123.618926}, {\"timesteps\": 2554, \"rewards\": -402.577016}, {\"timesteps\": 2555, \"rewards\": -110.913503}, {\"timesteps\": 2556, \"rewards\": -510.743315}, {\"timesteps\": 2557, \"rewards\": -134.272334}, {\"timesteps\": 2558, \"rewards\": -137.495045}, {\"timesteps\": 2559, \"rewards\": -137.190905}, {\"timesteps\": 2560, \"rewards\": -127.227723}, {\"timesteps\": 2561, \"rewards\": -504.387478}, {\"timesteps\": 2562, \"rewards\": -766.017154}, {\"timesteps\": 2563, \"rewards\": -569.308515}, {\"timesteps\": 2564, \"rewards\": -158.044189}, {\"timesteps\": 2565, \"rewards\": -555.278766}, {\"timesteps\": 2566, \"rewards\": -503.089594}, {\"timesteps\": 2567, \"rewards\": -89.850846}, {\"timesteps\": 2568, \"rewards\": -139.609919}, {\"timesteps\": 2569, \"rewards\": -124.769459}, {\"timesteps\": 2570, \"rewards\": -132.810122}, {\"timesteps\": 2571, \"rewards\": -108.473822}, {\"timesteps\": 2572, \"rewards\": -133.454409}, {\"timesteps\": 2573, \"rewards\": -120.694012}, {\"timesteps\": 2574, \"rewards\": -384.425353}, {\"timesteps\": 2575, \"rewards\": -130.461584}, {\"timesteps\": 2576, \"rewards\": -136.721368}, {\"timesteps\": 2577, \"rewards\": -132.271309}, {\"timesteps\": 2578, \"rewards\": -150.212392}, {\"timesteps\": 2579, \"rewards\": -131.56154}, {\"timesteps\": 2580, \"rewards\": -319.775511}, {\"timesteps\": 2581, \"rewards\": -437.908002}, {\"timesteps\": 2582, \"rewards\": -492.402877}, {\"timesteps\": 2583, \"rewards\": -449.567414}, {\"timesteps\": 2584, \"rewards\": -122.829668}, {\"timesteps\": 2585, \"rewards\": -93.418375}, {\"timesteps\": 2586, \"rewards\": -417.337007}, {\"timesteps\": 2587, \"rewards\": -616.315461}, {\"timesteps\": 2588, \"rewards\": -114.171943}, {\"timesteps\": 2589, \"rewards\": -503.995033}, {\"timesteps\": 2590, \"rewards\": -127.425097}, {\"timesteps\": 2591, \"rewards\": -132.839308}, {\"timesteps\": 2592, \"rewards\": -497.589652}, {\"timesteps\": 2593, \"rewards\": -109.031143}, {\"timesteps\": 2594, \"rewards\": -148.363546}, {\"timesteps\": 2595, \"rewards\": -101.317803}, {\"timesteps\": 2596, \"rewards\": -159.178621}, {\"timesteps\": 2597, \"rewards\": -522.063821}, {\"timesteps\": 2598, \"rewards\": -170.17498}, {\"timesteps\": 2599, \"rewards\": -124.179075}, {\"timesteps\": 2600, \"rewards\": -359.146597}, {\"timesteps\": 2601, \"rewards\": -136.846736}, {\"timesteps\": 2602, \"rewards\": -432.365729}, {\"timesteps\": 2603, \"rewards\": -124.912667}, {\"timesteps\": 2604, \"rewards\": -130.495035}, {\"timesteps\": 2605, \"rewards\": -89.295226}, {\"timesteps\": 2606, \"rewards\": -132.318961}, {\"timesteps\": 2607, \"rewards\": -129.901921}, {\"timesteps\": 2608, \"rewards\": -664.068539}, {\"timesteps\": 2609, \"rewards\": -112.725594}, {\"timesteps\": 2610, \"rewards\": -147.778007}, {\"timesteps\": 2611, \"rewards\": -468.371039}, {\"timesteps\": 2612, \"rewards\": -115.125334}, {\"timesteps\": 2613, \"rewards\": -536.566301}, {\"timesteps\": 2614, \"rewards\": -161.439191}, {\"timesteps\": 2615, \"rewards\": -638.384612}, {\"timesteps\": 2616, \"rewards\": -490.257421}, {\"timesteps\": 2617, \"rewards\": -114.812035}, {\"timesteps\": 2618, \"rewards\": -71.287652}, {\"timesteps\": 2619, \"rewards\": -76.729411}, {\"timesteps\": 2620, \"rewards\": -214.967424}, {\"timesteps\": 2621, \"rewards\": -277.64346}, {\"timesteps\": 2622, \"rewards\": -680.251659}, {\"timesteps\": 2623, \"rewards\": -68.109247}, {\"timesteps\": 2624, \"rewards\": -53.719741}, {\"timesteps\": 2625, \"rewards\": -590.486639}, {\"timesteps\": 2626, \"rewards\": -705.280795}, {\"timesteps\": 2627, \"rewards\": -607.268032}, {\"timesteps\": 2628, \"rewards\": -68.389089}, {\"timesteps\": 2629, \"rewards\": -68.245084}, {\"timesteps\": 2630, \"rewards\": -620.078898}, {\"timesteps\": 2631, \"rewards\": -68.171807}, {\"timesteps\": 2632, \"rewards\": -59.092223}, {\"timesteps\": 2633, \"rewards\": -84.192019}, {\"timesteps\": 2634, \"rewards\": -70.385876}, {\"timesteps\": 2635, \"rewards\": -627.845855}, {\"timesteps\": 2636, \"rewards\": -608.274626}, {\"timesteps\": 2637, \"rewards\": -664.631622}, {\"timesteps\": 2638, \"rewards\": -743.766407}, {\"timesteps\": 2639, \"rewards\": -59.528683}, {\"timesteps\": 2640, \"rewards\": -76.026873}, {\"timesteps\": 2641, \"rewards\": -70.425974}, {\"timesteps\": 2642, \"rewards\": -75.913974}, {\"timesteps\": 2643, \"rewards\": -95.745921}, {\"timesteps\": 2644, \"rewards\": -102.011236}, {\"timesteps\": 2645, \"rewards\": -604.766715}, {\"timesteps\": 2646, \"rewards\": -555.232486}, {\"timesteps\": 2647, \"rewards\": -661.087975}, {\"timesteps\": 2648, \"rewards\": -108.014448}, {\"timesteps\": 2649, \"rewards\": -102.339797}, {\"timesteps\": 2650, \"rewards\": -414.68357}, {\"timesteps\": 2651, \"rewards\": -81.73985}, {\"timesteps\": 2652, \"rewards\": -71.104424}, {\"timesteps\": 2653, \"rewards\": -69.284661}, {\"timesteps\": 2654, \"rewards\": -77.057695}, {\"timesteps\": 2655, \"rewards\": -104.115827}, {\"timesteps\": 2656, \"rewards\": -88.351233}, {\"timesteps\": 2657, \"rewards\": -84.956561}, {\"timesteps\": 2658, \"rewards\": -78.181774}, {\"timesteps\": 2659, \"rewards\": -485.581437}, {\"timesteps\": 2660, \"rewards\": -496.568155}, {\"timesteps\": 2661, \"rewards\": -609.014735}, {\"timesteps\": 2662, \"rewards\": -86.50403}, {\"timesteps\": 2663, \"rewards\": -56.528924}, {\"timesteps\": 2664, \"rewards\": -82.555339}, {\"timesteps\": 2665, \"rewards\": -613.100145}, {\"timesteps\": 2666, \"rewards\": -590.619524}, {\"timesteps\": 2667, \"rewards\": -550.855154}, {\"timesteps\": 2668, \"rewards\": -86.297466}, {\"timesteps\": 2669, \"rewards\": -92.198336}, {\"timesteps\": 2670, \"rewards\": -72.238478}, {\"timesteps\": 2671, \"rewards\": -75.951026}, {\"timesteps\": 2672, \"rewards\": -77.373113}, {\"timesteps\": 2673, \"rewards\": -80.938247}, {\"timesteps\": 2674, \"rewards\": -88.842173}, {\"timesteps\": 2675, \"rewards\": -698.554526}, {\"timesteps\": 2676, \"rewards\": -606.411293}, {\"timesteps\": 2677, \"rewards\": -72.252146}, {\"timesteps\": 2678, \"rewards\": -67.208023}, {\"timesteps\": 2679, \"rewards\": -92.960369}, {\"timesteps\": 2680, \"rewards\": -514.479792}, {\"timesteps\": 2681, \"rewards\": -374.289073}, {\"timesteps\": 2682, \"rewards\": -63.61339}, {\"timesteps\": 2683, \"rewards\": -69.007708}, {\"timesteps\": 2684, \"rewards\": -88.468993}, {\"timesteps\": 2685, \"rewards\": -568.814375}, {\"timesteps\": 2686, \"rewards\": -303.858018}, {\"timesteps\": 2687, \"rewards\": -72.655507}, {\"timesteps\": 2688, \"rewards\": -92.916245}, {\"timesteps\": 2689, \"rewards\": -56.499958}, {\"timesteps\": 2690, \"rewards\": -564.994974}, {\"timesteps\": 2691, \"rewards\": -74.484886}, {\"timesteps\": 2692, \"rewards\": -79.863214}, {\"timesteps\": 2693, \"rewards\": -74.408475}, {\"timesteps\": 2694, \"rewards\": -90.519193}, {\"timesteps\": 2695, \"rewards\": -403.702591}, {\"timesteps\": 2696, \"rewards\": -541.286246}, {\"timesteps\": 2697, \"rewards\": -86.744479}, {\"timesteps\": 2698, \"rewards\": -77.52341}, {\"timesteps\": 2699, \"rewards\": -103.283909}, {\"timesteps\": 2700, \"rewards\": -310.366619}, {\"timesteps\": 2701, \"rewards\": -658.844358}, {\"timesteps\": 2702, \"rewards\": -103.70406}, {\"timesteps\": 2703, \"rewards\": -88.79163}, {\"timesteps\": 2704, \"rewards\": -73.360652}, {\"timesteps\": 2705, \"rewards\": -626.575922}, {\"timesteps\": 2706, \"rewards\": -612.864321}, {\"timesteps\": 2707, \"rewards\": -105.807677}, {\"timesteps\": 2708, \"rewards\": -49.692297}, {\"timesteps\": 2709, \"rewards\": -59.918418}, {\"timesteps\": 2710, \"rewards\": -405.2824}, {\"timesteps\": 2711, \"rewards\": -90.932849}, {\"timesteps\": 2712, \"rewards\": -92.760905}, {\"timesteps\": 2713, \"rewards\": -95.093578}, {\"timesteps\": 2714, \"rewards\": -343.21205}, {\"timesteps\": 2715, \"rewards\": -453.625574}, {\"timesteps\": 2716, \"rewards\": -397.374495}, {\"timesteps\": 2717, \"rewards\": -477.337787}, {\"timesteps\": 2718, \"rewards\": -458.641682}, {\"timesteps\": 2719, \"rewards\": -538.563092}, {\"timesteps\": 2720, \"rewards\": -433.558996}, {\"timesteps\": 2721, \"rewards\": -525.164832}, {\"timesteps\": 2722, \"rewards\": -500.27513}, {\"timesteps\": 2723, \"rewards\": -573.919257}, {\"timesteps\": 2724, \"rewards\": -506.053502}, {\"timesteps\": 2725, \"rewards\": -416.812052}, {\"timesteps\": 2726, \"rewards\": -513.178119}, {\"timesteps\": 2727, \"rewards\": -554.272561}, {\"timesteps\": 2728, \"rewards\": -845.809917}, {\"timesteps\": 2729, \"rewards\": -431.617316}, {\"timesteps\": 2730, \"rewards\": -459.437122}, {\"timesteps\": 2731, \"rewards\": -518.382515}, {\"timesteps\": 2732, \"rewards\": -511.978726}, {\"timesteps\": 2733, \"rewards\": -798.215105}, {\"timesteps\": 2734, \"rewards\": -908.732098}, {\"timesteps\": 2735, \"rewards\": -517.290576}, {\"timesteps\": 2736, \"rewards\": -520.162189}, {\"timesteps\": 2737, \"rewards\": -581.381986}, {\"timesteps\": 2738, \"rewards\": -535.250676}, {\"timesteps\": 2739, \"rewards\": -512.004068}, {\"timesteps\": 2740, \"rewards\": -392.403625}, {\"timesteps\": 2741, \"rewards\": -518.088255}, {\"timesteps\": 2742, \"rewards\": -548.014681}, {\"timesteps\": 2743, \"rewards\": -489.745799}, {\"timesteps\": 2744, \"rewards\": -409.277569}, {\"timesteps\": 2745, \"rewards\": -406.896224}, {\"timesteps\": 2746, \"rewards\": -405.253283}, {\"timesteps\": 2747, \"rewards\": -543.703648}, {\"timesteps\": 2748, \"rewards\": -516.815336}, {\"timesteps\": 2749, \"rewards\": -852.846528}, {\"timesteps\": 2750, \"rewards\": -504.587438}, {\"timesteps\": 2751, \"rewards\": -377.131199}, {\"timesteps\": 2752, \"rewards\": -511.647542}, {\"timesteps\": 2753, \"rewards\": -558.977209}, {\"timesteps\": 2754, \"rewards\": -356.064259}, {\"timesteps\": 2755, \"rewards\": -420.6161}, {\"timesteps\": 2756, \"rewards\": -514.384268}, {\"timesteps\": 2757, \"rewards\": -519.512172}, {\"timesteps\": 2758, \"rewards\": -493.208772}, {\"timesteps\": 2759, \"rewards\": -439.020178}, {\"timesteps\": 2760, \"rewards\": -478.736376}, {\"timesteps\": 2761, \"rewards\": -383.267087}, {\"timesteps\": 2762, \"rewards\": -534.911791}, {\"timesteps\": 2763, \"rewards\": -369.856627}, {\"timesteps\": 2764, \"rewards\": -724.98295}, {\"timesteps\": 2765, \"rewards\": -421.812069}, {\"timesteps\": 2766, \"rewards\": -391.025869}, {\"timesteps\": 2767, \"rewards\": -361.403569}, {\"timesteps\": 2768, \"rewards\": -536.72415}, {\"timesteps\": 2769, \"rewards\": -538.797471}, {\"timesteps\": 2770, \"rewards\": -493.582224}, {\"timesteps\": 2771, \"rewards\": -445.117873}, {\"timesteps\": 2772, \"rewards\": -550.7805}, {\"timesteps\": 2773, \"rewards\": -849.785839}, {\"timesteps\": 2774, \"rewards\": -483.974762}, {\"timesteps\": 2775, \"rewards\": -517.580613}, {\"timesteps\": 2776, \"rewards\": -524.015821}, {\"timesteps\": 2777, \"rewards\": -553.554182}, {\"timesteps\": 2778, \"rewards\": -519.495467}, {\"timesteps\": 2779, \"rewards\": -401.066956}, {\"timesteps\": 2780, \"rewards\": -390.945943}, {\"timesteps\": 2781, \"rewards\": -474.341218}, {\"timesteps\": 2782, \"rewards\": -530.532584}, {\"timesteps\": 2783, \"rewards\": -569.436678}, {\"timesteps\": 2784, \"rewards\": -706.580121}, {\"timesteps\": 2785, \"rewards\": -431.720003}, {\"timesteps\": 2786, \"rewards\": -487.080348}, {\"timesteps\": 2787, \"rewards\": -368.939346}, {\"timesteps\": 2788, \"rewards\": -694.072669}, {\"timesteps\": 2789, \"rewards\": -564.12554}, {\"timesteps\": 2790, \"rewards\": -453.549237}, {\"timesteps\": 2791, \"rewards\": -430.049889}, {\"timesteps\": 2792, \"rewards\": -523.791692}, {\"timesteps\": 2793, \"rewards\": -971.167721}, {\"timesteps\": 2794, \"rewards\": -430.612982}, {\"timesteps\": 2795, \"rewards\": -418.775353}, {\"timesteps\": 2796, \"rewards\": -563.729692}, {\"timesteps\": 2797, \"rewards\": -536.013665}, {\"timesteps\": 2798, \"rewards\": -570.412243}, {\"timesteps\": 2799, \"rewards\": -367.847719}, {\"timesteps\": 2800, \"rewards\": -420.322396}, {\"timesteps\": 2801, \"rewards\": -495.948312}, {\"timesteps\": 2802, \"rewards\": -447.638828}, {\"timesteps\": 2803, \"rewards\": -396.176097}, {\"timesteps\": 2804, \"rewards\": -425.645141}, {\"timesteps\": 2805, \"rewards\": -451.229886}, {\"timesteps\": 2806, \"rewards\": -488.147453}, {\"timesteps\": 2807, \"rewards\": -392.478982}, {\"timesteps\": 2808, \"rewards\": -1016.179856}, {\"timesteps\": 2809, \"rewards\": -389.942909}, {\"timesteps\": 2810, \"rewards\": -410.855987}, {\"timesteps\": 2811, \"rewards\": -391.44492}, {\"timesteps\": 2812, \"rewards\": -535.703947}, {\"timesteps\": 2813, \"rewards\": -542.275847}, {\"timesteps\": 2814, \"rewards\": -463.76376}, {\"timesteps\": 2815, \"rewards\": -449.051907}, {\"timesteps\": 2816, \"rewards\": -390.392229}, {\"timesteps\": 2817, \"rewards\": -574.870985}, {\"timesteps\": 2818, \"rewards\": -668.489319}, {\"timesteps\": 2819, \"rewards\": -542.34633}, {\"timesteps\": 2820, \"rewards\": -3128.728989}, {\"timesteps\": 2821, \"rewards\": -3219.334828}, {\"timesteps\": 2822, \"rewards\": -3170.281198}, {\"timesteps\": 2823, \"rewards\": -3340.12857}, {\"timesteps\": 2824, \"rewards\": -3465.791374}, {\"timesteps\": 2825, \"rewards\": -3059.026236}, {\"timesteps\": 2826, \"rewards\": -3132.336792}, {\"timesteps\": 2827, \"rewards\": -3508.889074}, {\"timesteps\": 2828, \"rewards\": -3117.94363}, {\"timesteps\": 2829, \"rewards\": -3256.032481}, {\"timesteps\": 2830, \"rewards\": -2966.696492}, {\"timesteps\": 2831, \"rewards\": -3390.733495}, {\"timesteps\": 2832, \"rewards\": -3618.960397}, {\"timesteps\": 2833, \"rewards\": -3482.402049}, {\"timesteps\": 2834, \"rewards\": -3167.042733}, {\"timesteps\": 2835, \"rewards\": -3118.426423}, {\"timesteps\": 2836, \"rewards\": -3264.596151}, {\"timesteps\": 2837, \"rewards\": -3388.265986}, {\"timesteps\": 2838, \"rewards\": -3283.162434}, {\"timesteps\": 2839, \"rewards\": -2976.466674}, {\"timesteps\": 2840, \"rewards\": -3265.672294}, {\"timesteps\": 2841, \"rewards\": -3340.200992}, {\"timesteps\": 2842, \"rewards\": -3305.043095}, {\"timesteps\": 2843, \"rewards\": -3391.87969}, {\"timesteps\": 2844, \"rewards\": -3265.592976}, {\"timesteps\": 2845, \"rewards\": -3269.661499}, {\"timesteps\": 2846, \"rewards\": -3245.286709}, {\"timesteps\": 2847, \"rewards\": -1861.305146}, {\"timesteps\": 2848, \"rewards\": -2383.24144}, {\"timesteps\": 2849, \"rewards\": -3337.812025}, {\"timesteps\": 2850, \"rewards\": -3209.153248}, {\"timesteps\": 2851, \"rewards\": -3234.966126}, {\"timesteps\": 2852, \"rewards\": -3267.160292}, {\"timesteps\": 2853, \"rewards\": -3243.809929}, {\"timesteps\": 2854, \"rewards\": -2922.992157}, {\"timesteps\": 2855, \"rewards\": -2822.419407}, {\"timesteps\": 2856, \"rewards\": -3147.683684}, {\"timesteps\": 2857, \"rewards\": -3398.472745}, {\"timesteps\": 2858, \"rewards\": -3400.620297}, {\"timesteps\": 2859, \"rewards\": -2928.642145}, {\"timesteps\": 2860, \"rewards\": -352.406321}, {\"timesteps\": 2861, \"rewards\": -2989.741872}, {\"timesteps\": 2862, \"rewards\": -3236.288147}, {\"timesteps\": 2863, \"rewards\": -3377.353224}, {\"timesteps\": 2864, \"rewards\": -3508.527106}, {\"timesteps\": 2865, \"rewards\": -3201.480765}, {\"timesteps\": 2866, \"rewards\": -3266.026764}, {\"timesteps\": 2867, \"rewards\": -3263.466438}, {\"timesteps\": 2868, \"rewards\": -3204.035881}, {\"timesteps\": 2869, \"rewards\": -1784.454375}, {\"timesteps\": 2870, \"rewards\": -3166.578264}, {\"timesteps\": 2871, \"rewards\": -3133.563273}, {\"timesteps\": 2872, \"rewards\": -3510.371636}, {\"timesteps\": 2873, \"rewards\": -3323.523764}, {\"timesteps\": 2874, \"rewards\": -3433.972055}, {\"timesteps\": 2875, \"rewards\": -3102.826021}, {\"timesteps\": 2876, \"rewards\": -3359.432925}, {\"timesteps\": 2877, \"rewards\": -3505.079786}, {\"timesteps\": 2878, \"rewards\": -2831.417436}, {\"timesteps\": 2879, \"rewards\": -3335.095098}, {\"timesteps\": 2880, \"rewards\": -3096.944924}, {\"timesteps\": 2881, \"rewards\": -3361.439831}, {\"timesteps\": 2882, \"rewards\": -3264.128092}, {\"timesteps\": 2883, \"rewards\": -3283.718828}, {\"timesteps\": 2884, \"rewards\": -3183.395972}, {\"timesteps\": 2885, \"rewards\": -3106.185004}, {\"timesteps\": 2886, \"rewards\": -3227.60352}, {\"timesteps\": 2887, \"rewards\": -3273.115955}, {\"timesteps\": 2888, \"rewards\": -3402.877981}, {\"timesteps\": 2889, \"rewards\": -2863.282232}, {\"timesteps\": 2890, \"rewards\": -3382.669833}, {\"timesteps\": 2891, \"rewards\": -3433.758216}, {\"timesteps\": 2892, \"rewards\": -3317.136046}, {\"timesteps\": 2893, \"rewards\": -3453.233261}, {\"timesteps\": 2894, \"rewards\": -3088.099039}, {\"timesteps\": 2895, \"rewards\": -3301.963686}, {\"timesteps\": 2896, \"rewards\": -3268.202838}, {\"timesteps\": 2897, \"rewards\": -2965.845645}, {\"timesteps\": 2898, \"rewards\": -3630.554845}, {\"timesteps\": 2899, \"rewards\": -3166.673274}, {\"timesteps\": 2900, \"rewards\": -3251.427417}, {\"timesteps\": 2901, \"rewards\": -3105.468962}, {\"timesteps\": 2902, \"rewards\": -3006.361559}, {\"timesteps\": 2903, \"rewards\": -3379.663663}, {\"timesteps\": 2904, \"rewards\": -3340.614998}, {\"timesteps\": 2905, \"rewards\": -472.003662}, {\"timesteps\": 2906, \"rewards\": -3091.148757}, {\"timesteps\": 2907, \"rewards\": -3117.350887}, {\"timesteps\": 2908, \"rewards\": -3179.326732}, {\"timesteps\": 2909, \"rewards\": -3411.759781}, {\"timesteps\": 2910, \"rewards\": -3310.133434}, {\"timesteps\": 2911, \"rewards\": -3269.783559}, {\"timesteps\": 2912, \"rewards\": -3220.328465}, {\"timesteps\": 2913, \"rewards\": -3389.802355}, {\"timesteps\": 2914, \"rewards\": -3477.843338}, {\"timesteps\": 2915, \"rewards\": -3189.597586}, {\"timesteps\": 2916, \"rewards\": -3175.300464}, {\"timesteps\": 2917, \"rewards\": -3334.651336}, {\"timesteps\": 2918, \"rewards\": -1680.142122}, {\"timesteps\": 2919, \"rewards\": -3153.95221}, {\"timesteps\": 2920, \"rewards\": -1917.834676}, {\"timesteps\": 2921, \"rewards\": -1656.776872}, {\"timesteps\": 2922, \"rewards\": -2220.428232}, {\"timesteps\": 2923, \"rewards\": -2230.256142}, {\"timesteps\": 2924, \"rewards\": -1777.80302}, {\"timesteps\": 2925, \"rewards\": -1241.661151}, {\"timesteps\": 2926, \"rewards\": -1597.338413}, {\"timesteps\": 2927, \"rewards\": -2083.42403}, {\"timesteps\": 2928, \"rewards\": -4105.460017}, {\"timesteps\": 2929, \"rewards\": -1629.32299}, {\"timesteps\": 2930, \"rewards\": -1699.1926}, {\"timesteps\": 2931, \"rewards\": -1830.836911}, {\"timesteps\": 2932, \"rewards\": -1877.750229}, {\"timesteps\": 2933, \"rewards\": -1680.873455}, {\"timesteps\": 2934, \"rewards\": -2376.221648}, {\"timesteps\": 2935, \"rewards\": -1605.515159}, {\"timesteps\": 2936, \"rewards\": -2043.169954}, {\"timesteps\": 2937, \"rewards\": -1810.71174}, {\"timesteps\": 2938, \"rewards\": -4163.953546}, {\"timesteps\": 2939, \"rewards\": -2265.833703}, {\"timesteps\": 2940, \"rewards\": -440.552816}, {\"timesteps\": 2941, \"rewards\": -1925.320654}, {\"timesteps\": 2942, \"rewards\": -1881.80553}, {\"timesteps\": 2943, \"rewards\": -3485.809704}, {\"timesteps\": 2944, \"rewards\": -1538.680978}, {\"timesteps\": 2945, \"rewards\": -1909.190229}, {\"timesteps\": 2946, \"rewards\": -1972.041002}, {\"timesteps\": 2947, \"rewards\": -1615.095805}, {\"timesteps\": 2948, \"rewards\": -1489.850002}, {\"timesteps\": 2949, \"rewards\": -1644.247868}, {\"timesteps\": 2950, \"rewards\": -1996.313053}, {\"timesteps\": 2951, \"rewards\": -2241.742659}, {\"timesteps\": 2952, \"rewards\": -1911.350787}, {\"timesteps\": 2953, \"rewards\": -1913.990877}, {\"timesteps\": 2954, \"rewards\": -2118.686371}, {\"timesteps\": 2955, \"rewards\": -566.203572}, {\"timesteps\": 2956, \"rewards\": -2335.442961}, {\"timesteps\": 2957, \"rewards\": -1233.975931}, {\"timesteps\": 2958, \"rewards\": -2629.987443}, {\"timesteps\": 2959, \"rewards\": -1563.557715}, {\"timesteps\": 2960, \"rewards\": -1540.209263}, {\"timesteps\": 2961, \"rewards\": -1004.742851}, {\"timesteps\": 2962, \"rewards\": -2234.21103}, {\"timesteps\": 2963, \"rewards\": -1443.215391}, {\"timesteps\": 2964, \"rewards\": -2315.430973}, {\"timesteps\": 2965, \"rewards\": -1782.651231}, {\"timesteps\": 2966, \"rewards\": -2000.841136}, {\"timesteps\": 2967, \"rewards\": -1712.765798}, {\"timesteps\": 2968, \"rewards\": -2000.731936}, {\"timesteps\": 2969, \"rewards\": -1645.003977}, {\"timesteps\": 2970, \"rewards\": -1854.098231}, {\"timesteps\": 2971, \"rewards\": -1210.664959}, {\"timesteps\": 2972, \"rewards\": -1752.441231}, {\"timesteps\": 2973, \"rewards\": -1359.001857}, {\"timesteps\": 2974, \"rewards\": -1632.393274}, {\"timesteps\": 2975, \"rewards\": -309.479024}, {\"timesteps\": 2976, \"rewards\": -1467.10016}, {\"timesteps\": 2977, \"rewards\": -1856.05695}, {\"timesteps\": 2978, \"rewards\": -1833.089253}, {\"timesteps\": 2979, \"rewards\": -1668.672046}, {\"timesteps\": 2980, \"rewards\": -360.746449}, {\"timesteps\": 2981, \"rewards\": -1705.727929}, {\"timesteps\": 2982, \"rewards\": -2127.583022}, {\"timesteps\": 2983, \"rewards\": -1847.571553}, {\"timesteps\": 2984, \"rewards\": -1844.898084}, {\"timesteps\": 2985, \"rewards\": -1453.266382}, {\"timesteps\": 2986, \"rewards\": -1447.886027}, {\"timesteps\": 2987, \"rewards\": -2179.49705}, {\"timesteps\": 2988, \"rewards\": -1571.93918}, {\"timesteps\": 2989, \"rewards\": -1863.102015}, {\"timesteps\": 2990, \"rewards\": -337.8755}, {\"timesteps\": 2991, \"rewards\": -1514.884444}, {\"timesteps\": 2992, \"rewards\": -2253.484612}, {\"timesteps\": 2993, \"rewards\": -2170.0722}, {\"timesteps\": 2994, \"rewards\": -1408.69801}, {\"timesteps\": 2995, \"rewards\": 8.7126}, {\"timesteps\": 2996, \"rewards\": -1867.755951}, {\"timesteps\": 2997, \"rewards\": -1681.39154}, {\"timesteps\": 2998, \"rewards\": -1802.809588}, {\"timesteps\": 2999, \"rewards\": -3678.353995}, {\"timesteps\": 3000, \"rewards\": -1540.996608}, {\"timesteps\": 3001, \"rewards\": -1480.07042}, {\"timesteps\": 3002, \"rewards\": -2058.545796}, {\"timesteps\": 3003, \"rewards\": -1359.63246}, {\"timesteps\": 3004, \"rewards\": -2011.676901}, {\"timesteps\": 3005, \"rewards\": -572.420738}, {\"timesteps\": 3006, \"rewards\": -1951.388726}, {\"timesteps\": 3007, \"rewards\": -1896.953974}, {\"timesteps\": 3008, \"rewards\": -2105.270213}, {\"timesteps\": 3009, \"rewards\": -1464.145629}, {\"timesteps\": 3010, \"rewards\": -1554.40321}, {\"timesteps\": 3011, \"rewards\": -1701.117701}, {\"timesteps\": 3012, \"rewards\": -2050.547956}, {\"timesteps\": 3013, \"rewards\": -2254.601495}, {\"timesteps\": 3014, \"rewards\": -2301.27257}, {\"timesteps\": 3015, \"rewards\": -1836.324752}, {\"timesteps\": 3016, \"rewards\": -1698.268465}, {\"timesteps\": 3017, \"rewards\": -1522.831357}, {\"timesteps\": 3018, \"rewards\": -4910.739825}, {\"timesteps\": 3019, \"rewards\": -1994.239056}, {\"timesteps\": 3020, \"rewards\": -342.793068}, {\"timesteps\": 3021, \"rewards\": -1914.181199}, {\"timesteps\": 3022, \"rewards\": -1785.959517}, {\"timesteps\": 3023, \"rewards\": -2382.894257}, {\"timesteps\": 3024, \"rewards\": -3860.774413}, {\"timesteps\": 3025, \"rewards\": -4143.434596}, {\"timesteps\": 3026, \"rewards\": -4775.940449}, {\"timesteps\": 3027, \"rewards\": -5497.924927}, {\"timesteps\": 3028, \"rewards\": -5654.313706}, {\"timesteps\": 3029, \"rewards\": -4169.418902}, {\"timesteps\": 3030, \"rewards\": -4065.966903}, {\"timesteps\": 3031, \"rewards\": -4151.323726}, {\"timesteps\": 3032, \"rewards\": -4014.759516}, {\"timesteps\": 3033, \"rewards\": -5915.405916}, {\"timesteps\": 3034, \"rewards\": -5005.044197}, {\"timesteps\": 3035, \"rewards\": -4314.469739}, {\"timesteps\": 3036, \"rewards\": -4135.067071}, {\"timesteps\": 3037, \"rewards\": -5521.280822}, {\"timesteps\": 3038, \"rewards\": -6320.620133}, {\"timesteps\": 3039, \"rewards\": -4320.449506}, {\"timesteps\": 3040, \"rewards\": -4271.515868}, {\"timesteps\": 3041, \"rewards\": -4387.507703}, {\"timesteps\": 3042, \"rewards\": -5352.161731}, {\"timesteps\": 3043, \"rewards\": -5400.690495}, {\"timesteps\": 3044, \"rewards\": -4078.248983}, {\"timesteps\": 3045, \"rewards\": -4021.326651}, {\"timesteps\": 3046, \"rewards\": -4777.864168}, {\"timesteps\": 3047, \"rewards\": -3760.489247}, {\"timesteps\": 3048, \"rewards\": -6670.06485}, {\"timesteps\": 3049, \"rewards\": -10714.271946}, {\"timesteps\": 3050, \"rewards\": -4146.577086}, {\"timesteps\": 3051, \"rewards\": -3971.534936}, {\"timesteps\": 3052, \"rewards\": -3703.426032}, {\"timesteps\": 3053, \"rewards\": -6513.351692}, {\"timesteps\": 3054, \"rewards\": -3843.45454}, {\"timesteps\": 3055, \"rewards\": -3896.897877}, {\"timesteps\": 3056, \"rewards\": -4329.079561}, {\"timesteps\": 3057, \"rewards\": -4721.202212}, {\"timesteps\": 3058, \"rewards\": -4011.759974}, {\"timesteps\": 3059, \"rewards\": -3829.93348}, {\"timesteps\": 3060, \"rewards\": -3584.101488}, {\"timesteps\": 3061, \"rewards\": -3865.332804}, {\"timesteps\": 3062, \"rewards\": -4680.929737}, {\"timesteps\": 3063, \"rewards\": -5482.524281}, {\"timesteps\": 3064, \"rewards\": -4586.676321}, {\"timesteps\": 3065, \"rewards\": -4498.54658}, {\"timesteps\": 3066, \"rewards\": -4512.780695}, {\"timesteps\": 3067, \"rewards\": -5494.464454}, {\"timesteps\": 3068, \"rewards\": -8139.332407}, {\"timesteps\": 3069, \"rewards\": -9337.851176}, {\"timesteps\": 3070, \"rewards\": -4267.625723}, {\"timesteps\": 3071, \"rewards\": -4646.722621}, {\"timesteps\": 3072, \"rewards\": -4918.015458}, {\"timesteps\": 3073, \"rewards\": -6028.887047}, {\"timesteps\": 3074, \"rewards\": -4654.792154}, {\"timesteps\": 3075, \"rewards\": -3877.810029}, {\"timesteps\": 3076, \"rewards\": -4388.496474}, {\"timesteps\": 3077, \"rewards\": -5569.602595}, {\"timesteps\": 3078, \"rewards\": -4389.407427}, {\"timesteps\": 3079, \"rewards\": -4611.454785}, {\"timesteps\": 3080, \"rewards\": -4115.08209}, {\"timesteps\": 3081, \"rewards\": -4829.938309}, {\"timesteps\": 3082, \"rewards\": -5800.383164}, {\"timesteps\": 3083, \"rewards\": -6744.006004}, {\"timesteps\": 3084, \"rewards\": -7725.158935}, {\"timesteps\": 3085, \"rewards\": -3663.597541}, {\"timesteps\": 3086, \"rewards\": -4827.670449}, {\"timesteps\": 3087, \"rewards\": -5837.611427}, {\"timesteps\": 3088, \"rewards\": -10227.656364}, {\"timesteps\": 3089, \"rewards\": -3840.867109}, {\"timesteps\": 3090, \"rewards\": -4006.247459}, {\"timesteps\": 3091, \"rewards\": -4034.567613}, {\"timesteps\": 3092, \"rewards\": -4324.365207}, {\"timesteps\": 3093, \"rewards\": -5437.148596}, {\"timesteps\": 3094, \"rewards\": -4899.788296}, {\"timesteps\": 3095, \"rewards\": -3734.157428}, {\"timesteps\": 3096, \"rewards\": -3912.779008}, {\"timesteps\": 3097, \"rewards\": -5143.38269}, {\"timesteps\": 3098, \"rewards\": -6628.10676}, {\"timesteps\": 3099, \"rewards\": -4659.523159}, {\"timesteps\": 3100, \"rewards\": -4147.411405}, {\"timesteps\": 3101, \"rewards\": -4242.322537}, {\"timesteps\": 3102, \"rewards\": -4545.922257}, {\"timesteps\": 3103, \"rewards\": -4751.087489}, {\"timesteps\": 3104, \"rewards\": -3972.129209}, {\"timesteps\": 3105, \"rewards\": -4558.354346}, {\"timesteps\": 3106, \"rewards\": -4313.516194}, {\"timesteps\": 3107, \"rewards\": -4669.219167}, {\"timesteps\": 3108, \"rewards\": -8368.142172}, {\"timesteps\": 3109, \"rewards\": -3953.326526}, {\"timesteps\": 3110, \"rewards\": -4243.018551}, {\"timesteps\": 3111, \"rewards\": -3645.345775}, {\"timesteps\": 3112, \"rewards\": -4953.534153}, {\"timesteps\": 3113, \"rewards\": -5847.83103}, {\"timesteps\": 3114, \"rewards\": -4035.789296}, {\"timesteps\": 3115, \"rewards\": -3901.877481}, {\"timesteps\": 3116, \"rewards\": -4193.143769}, {\"timesteps\": 3117, \"rewards\": -4472.454997}, {\"timesteps\": 3118, \"rewards\": -4290.088669}, {\"timesteps\": 3119, \"rewards\": -5527.003708}, {\"timesteps\": 3120, \"rewards\": -4438.741022}, {\"timesteps\": 3121, \"rewards\": -4249.068204}, {\"timesteps\": 3122, \"rewards\": -5312.742717}, {\"timesteps\": 3123, \"rewards\": -4914.275976}, {\"timesteps\": 3124, \"rewards\": -4370.238248}, {\"timesteps\": 3125, \"rewards\": -3457.642652}, {\"timesteps\": 3126, \"rewards\": -3607.869231}, {\"timesteps\": 3127, \"rewards\": -3546.592344}, {\"timesteps\": 3128, \"rewards\": -7430.921668}, {\"timesteps\": 3129, \"rewards\": -3244.575126}, {\"timesteps\": 3130, \"rewards\": -3655.523395}, {\"timesteps\": 3131, \"rewards\": -4192.626504}, {\"timesteps\": 3132, \"rewards\": -5526.634657}, {\"timesteps\": 3133, \"rewards\": -5845.887332}, {\"timesteps\": 3134, \"rewards\": -3447.616429}, {\"timesteps\": 3135, \"rewards\": -863.369624}, {\"timesteps\": 3136, \"rewards\": -3161.261733}, {\"timesteps\": 3137, \"rewards\": -3568.637257}, {\"timesteps\": 3138, \"rewards\": -6786.341123}, {\"timesteps\": 3139, \"rewards\": -3229.950409}, {\"timesteps\": 3140, \"rewards\": -4583.412696}, {\"timesteps\": 3141, \"rewards\": -3682.927082}, {\"timesteps\": 3142, \"rewards\": -5952.000841}, {\"timesteps\": 3143, \"rewards\": -4431.708762}, {\"timesteps\": 3144, \"rewards\": -3760.817502}, {\"timesteps\": 3145, \"rewards\": -4451.284865}, {\"timesteps\": 3146, \"rewards\": -4518.089353}, {\"timesteps\": 3147, \"rewards\": -3716.409073}, {\"timesteps\": 3148, \"rewards\": -3784.361584}, {\"timesteps\": 3149, \"rewards\": -5922.671121}, {\"timesteps\": 3150, \"rewards\": -3439.69208}, {\"timesteps\": 3151, \"rewards\": -3418.850139}, {\"timesteps\": 3152, \"rewards\": -3576.253173}, {\"timesteps\": 3153, \"rewards\": -4856.206376}, {\"timesteps\": 3154, \"rewards\": -3359.42394}, {\"timesteps\": 3155, \"rewards\": -3632.104306}, {\"timesteps\": 3156, \"rewards\": -3382.291961}, {\"timesteps\": 3157, \"rewards\": -5334.640863}, {\"timesteps\": 3158, \"rewards\": -4357.213568}, {\"timesteps\": 3159, \"rewards\": -3470.045689}, {\"timesteps\": 3160, \"rewards\": -3926.199917}, {\"timesteps\": 3161, \"rewards\": -3348.219141}, {\"timesteps\": 3162, \"rewards\": -4552.896552}, {\"timesteps\": 3163, \"rewards\": -4767.335998}, {\"timesteps\": 3164, \"rewards\": -3323.561414}, {\"timesteps\": 3165, \"rewards\": -3349.875985}, {\"timesteps\": 3166, \"rewards\": -3517.015797}, {\"timesteps\": 3167, \"rewards\": -4339.495204}, {\"timesteps\": 3168, \"rewards\": -7684.270895}, {\"timesteps\": 3169, \"rewards\": -3633.566195}, {\"timesteps\": 3170, \"rewards\": -3391.732672}, {\"timesteps\": 3171, \"rewards\": -4658.723517}, {\"timesteps\": 3172, \"rewards\": -5714.127138}, {\"timesteps\": 3173, \"rewards\": -6022.517062}, {\"timesteps\": 3174, \"rewards\": -3749.120848}, {\"timesteps\": 3175, \"rewards\": -3374.947629}, {\"timesteps\": 3176, \"rewards\": -3705.221463}, {\"timesteps\": 3177, \"rewards\": -4421.895846}, {\"timesteps\": 3178, \"rewards\": -5430.505933}, {\"timesteps\": 3179, \"rewards\": -4533.30088}, {\"timesteps\": 3180, \"rewards\": -3625.387889}, {\"timesteps\": 3181, \"rewards\": -3371.126136}, {\"timesteps\": 3182, \"rewards\": -5255.305267}, {\"timesteps\": 3183, \"rewards\": -5799.281082}, {\"timesteps\": 3184, \"rewards\": -3356.657863}, {\"timesteps\": 3185, \"rewards\": -3457.594135}, {\"timesteps\": 3186, \"rewards\": -3654.942559}, {\"timesteps\": 3187, \"rewards\": -3433.853971}, {\"timesteps\": 3188, \"rewards\": -4947.90689}, {\"timesteps\": 3189, \"rewards\": -5183.616792}, {\"timesteps\": 3190, \"rewards\": -3229.471893}, {\"timesteps\": 3191, \"rewards\": -3314.06625}, {\"timesteps\": 3192, \"rewards\": -4310.657136}, {\"timesteps\": 3193, \"rewards\": -6559.5799}, {\"timesteps\": 3194, \"rewards\": -3320.644955}, {\"timesteps\": 3195, \"rewards\": -3369.847038}, {\"timesteps\": 3196, \"rewards\": -4479.703635}, {\"timesteps\": 3197, \"rewards\": -4770.846016}, {\"timesteps\": 3198, \"rewards\": -5803.429661}, {\"timesteps\": 3199, \"rewards\": -4946.414666}, {\"timesteps\": 3200, \"rewards\": -3357.886214}, {\"timesteps\": 3201, \"rewards\": -3342.964532}, {\"timesteps\": 3202, \"rewards\": -3655.999061}, {\"timesteps\": 3203, \"rewards\": -5205.756159}, {\"timesteps\": 3204, \"rewards\": -3272.819587}, {\"timesteps\": 3205, \"rewards\": -3581.844255}, {\"timesteps\": 3206, \"rewards\": -4915.078251}, {\"timesteps\": 3207, \"rewards\": -5746.682915}, {\"timesteps\": 3208, \"rewards\": -5008.486949}, {\"timesteps\": 3209, \"rewards\": -3406.370602}, {\"timesteps\": 3210, \"rewards\": -3594.050982}, {\"timesteps\": 3211, \"rewards\": -3536.726445}, {\"timesteps\": 3212, \"rewards\": -3581.00631}, {\"timesteps\": 3213, \"rewards\": -5181.999612}, {\"timesteps\": 3214, \"rewards\": -3319.731388}, {\"timesteps\": 3215, \"rewards\": -3277.499433}, {\"timesteps\": 3216, \"rewards\": -3314.258453}, {\"timesteps\": 3217, \"rewards\": -3472.561006}, {\"timesteps\": 3218, \"rewards\": -4052.980127}, {\"timesteps\": 3219, \"rewards\": -3457.865403}, {\"timesteps\": 3220, \"rewards\": -3187.671379}, {\"timesteps\": 3221, \"rewards\": -3418.70348}, {\"timesteps\": 3222, \"rewards\": -3558.812441}, {\"timesteps\": 3223, \"rewards\": -5600.02475}, {\"timesteps\": 3224, \"rewards\": -4718.676994}, {\"timesteps\": 3225, \"rewards\": -3193.893908}, {\"timesteps\": 3226, \"rewards\": -3356.577439}, {\"timesteps\": 3227, \"rewards\": -3229.25042}, {\"timesteps\": 3228, \"rewards\": -3191.117934}, {\"timesteps\": 3229, \"rewards\": -4755.72294}, {\"timesteps\": 3230, \"rewards\": -1232.674475}, {\"timesteps\": 3231, \"rewards\": -1230.522841}, {\"timesteps\": 3232, \"rewards\": -1489.851154}, {\"timesteps\": 3233, \"rewards\": -1683.621948}, {\"timesteps\": 3234, \"rewards\": -1620.076424}, {\"timesteps\": 3235, \"rewards\": -1363.552136}, {\"timesteps\": 3236, \"rewards\": -1829.104897}, {\"timesteps\": 3237, \"rewards\": -1842.419704}, {\"timesteps\": 3238, \"rewards\": -1874.014729}, {\"timesteps\": 3239, \"rewards\": -1248.72494}, {\"timesteps\": 3240, \"rewards\": -1319.508503}, {\"timesteps\": 3241, \"rewards\": -1640.979695}, {\"timesteps\": 3242, \"rewards\": -1218.835084}, {\"timesteps\": 3243, \"rewards\": -1897.955476}, {\"timesteps\": 3244, \"rewards\": -1166.5403}, {\"timesteps\": 3245, \"rewards\": -1693.507392}, {\"timesteps\": 3246, \"rewards\": -1616.673532}, {\"timesteps\": 3247, \"rewards\": -1965.803746}, {\"timesteps\": 3248, \"rewards\": -1979.609072}, {\"timesteps\": 3249, \"rewards\": -1873.352077}, {\"timesteps\": 3250, \"rewards\": -1406.92146}, {\"timesteps\": 3251, \"rewards\": -1684.124482}, {\"timesteps\": 3252, \"rewards\": -1394.480619}, {\"timesteps\": 3253, \"rewards\": -1950.273354}, {\"timesteps\": 3254, \"rewards\": -1870.59956}, {\"timesteps\": 3255, \"rewards\": -1100.89698}, {\"timesteps\": 3256, \"rewards\": -1430.480826}, {\"timesteps\": 3257, \"rewards\": -1309.313418}, {\"timesteps\": 3258, \"rewards\": -2019.944983}, {\"timesteps\": 3259, \"rewards\": -1389.665499}, {\"timesteps\": 3260, \"rewards\": -1291.607419}, {\"timesteps\": 3261, \"rewards\": -1853.362142}, {\"timesteps\": 3262, \"rewards\": -1838.041567}, {\"timesteps\": 3263, \"rewards\": -1881.93134}, {\"timesteps\": 3264, \"rewards\": -1827.031594}, {\"timesteps\": 3265, \"rewards\": -1495.384735}, {\"timesteps\": 3266, \"rewards\": -1178.353255}, {\"timesteps\": 3267, \"rewards\": -1477.337319}, {\"timesteps\": 3268, \"rewards\": -1743.079657}, {\"timesteps\": 3269, \"rewards\": -1697.948926}, {\"timesteps\": 3270, \"rewards\": -1482.483589}, {\"timesteps\": 3271, \"rewards\": -1636.79834}, {\"timesteps\": 3272, \"rewards\": -1575.041094}, {\"timesteps\": 3273, \"rewards\": -1932.679925}, {\"timesteps\": 3274, \"rewards\": -1748.70592}, {\"timesteps\": 3275, \"rewards\": -1466.107386}, {\"timesteps\": 3276, \"rewards\": -1349.030285}, {\"timesteps\": 3277, \"rewards\": -1553.164691}, {\"timesteps\": 3278, \"rewards\": -1319.63514}, {\"timesteps\": 3279, \"rewards\": -1734.102343}, {\"timesteps\": 3280, \"rewards\": -1360.285817}, {\"timesteps\": 3281, \"rewards\": -1326.669947}, {\"timesteps\": 3282, \"rewards\": -1543.443343}, {\"timesteps\": 3283, \"rewards\": -1875.337061}, {\"timesteps\": 3284, \"rewards\": -1694.803393}, {\"timesteps\": 3285, \"rewards\": -1347.726479}, {\"timesteps\": 3286, \"rewards\": -1265.92884}, {\"timesteps\": 3287, \"rewards\": -1858.640307}, {\"timesteps\": 3288, \"rewards\": -1967.708948}, {\"timesteps\": 3289, \"rewards\": -1698.185235}, {\"timesteps\": 3290, \"rewards\": -1410.244831}, {\"timesteps\": 3291, \"rewards\": -1486.59613}, {\"timesteps\": 3292, \"rewards\": -1619.840435}, {\"timesteps\": 3293, \"rewards\": -1735.121536}, {\"timesteps\": 3294, \"rewards\": -1967.760928}, {\"timesteps\": 3295, \"rewards\": -1442.678291}, {\"timesteps\": 3296, \"rewards\": -1722.853597}, {\"timesteps\": 3297, \"rewards\": -1884.630008}, {\"timesteps\": 3298, \"rewards\": -1981.994995}, {\"timesteps\": 3299, \"rewards\": -1463.73577}, {\"timesteps\": 3300, \"rewards\": -1448.299274}, {\"timesteps\": 3301, \"rewards\": -1635.413024}, {\"timesteps\": 3302, \"rewards\": -1475.062574}, {\"timesteps\": 3303, \"rewards\": -1920.469161}, {\"timesteps\": 3304, \"rewards\": -1338.961895}, {\"timesteps\": 3305, \"rewards\": -1330.839497}, {\"timesteps\": 3306, \"rewards\": -1527.494072}, {\"timesteps\": 3307, \"rewards\": -1802.048787}, {\"timesteps\": 3308, \"rewards\": -1834.901654}, {\"timesteps\": 3309, \"rewards\": -1043.027601}, {\"timesteps\": 3310, \"rewards\": -1454.106596}, {\"timesteps\": 3311, \"rewards\": -1352.820212}, {\"timesteps\": 3312, \"rewards\": -1715.366471}, {\"timesteps\": 3313, \"rewards\": -1810.890177}, {\"timesteps\": 3314, \"rewards\": -1863.092159}, {\"timesteps\": 3315, \"rewards\": -1074.054035}, {\"timesteps\": 3316, \"rewards\": -1207.46498}, {\"timesteps\": 3317, \"rewards\": -1683.351033}, {\"timesteps\": 3318, \"rewards\": -1435.805293}, {\"timesteps\": 3319, \"rewards\": -1226.077995}, {\"timesteps\": 3320, \"rewards\": -1505.263256}, {\"timesteps\": 3321, \"rewards\": -1149.574421}, {\"timesteps\": 3322, \"rewards\": -1736.481358}, {\"timesteps\": 3323, \"rewards\": -1851.760155}, {\"timesteps\": 3324, \"rewards\": -926.857202}, {\"timesteps\": 3325, \"rewards\": -1060.036622}, {\"timesteps\": 3326, \"rewards\": -1260.319578}, {\"timesteps\": 3327, \"rewards\": -1048.103394}, {\"timesteps\": 3328, \"rewards\": -1853.955577}, {\"timesteps\": 3329, \"rewards\": -1827.150065}, {\"timesteps\": 3330, \"rewards\": -1027.783933}, {\"timesteps\": 3331, \"rewards\": -1607.456333}, {\"timesteps\": 3332, \"rewards\": -1637.410204}, {\"timesteps\": 3333, \"rewards\": -1925.070129}, {\"timesteps\": 3334, \"rewards\": -850.17285}, {\"timesteps\": 3335, \"rewards\": -1392.874362}, {\"timesteps\": 3336, \"rewards\": -1375.528845}, {\"timesteps\": 3337, \"rewards\": -1612.222055}, {\"timesteps\": 3338, \"rewards\": -1796.507243}, {\"timesteps\": 3339, \"rewards\": -1393.738326}, {\"timesteps\": 3340, \"rewards\": -1273.774937}, {\"timesteps\": 3341, \"rewards\": -1354.946817}, {\"timesteps\": 3342, \"rewards\": -1236.183707}, {\"timesteps\": 3343, \"rewards\": -1925.923313}, {\"timesteps\": 3344, \"rewards\": -1628.151598}, {\"timesteps\": 3345, \"rewards\": -1551.870043}, {\"timesteps\": 3346, \"rewards\": -1701.679081}, {\"timesteps\": 3347, \"rewards\": -1528.150632}, {\"timesteps\": 3348, \"rewards\": -1943.838616}, {\"timesteps\": 3349, \"rewards\": -1413.901}, {\"timesteps\": 3350, \"rewards\": -1329.05898}, {\"timesteps\": 3351, \"rewards\": -1311.989329}, {\"timesteps\": 3352, \"rewards\": -1539.625459}, {\"timesteps\": 3353, \"rewards\": -1309.182723}, {\"timesteps\": 3354, \"rewards\": -1590.9335}, {\"timesteps\": 3355, \"rewards\": -1115.076046}, {\"timesteps\": 3356, \"rewards\": -1213.728749}, {\"timesteps\": 3357, \"rewards\": -1593.029408}, {\"timesteps\": 3358, \"rewards\": -1683.574235}, {\"timesteps\": 3359, \"rewards\": -1565.401803}, {\"timesteps\": 3360, \"rewards\": -911.716123}, {\"timesteps\": 3361, \"rewards\": -1457.833668}, {\"timesteps\": 3362, \"rewards\": -1673.174172}, {\"timesteps\": 3363, \"rewards\": -1418.324524}, {\"timesteps\": 3364, \"rewards\": -1180.934698}, {\"timesteps\": 3365, \"rewards\": -1572.416066}, {\"timesteps\": 3366, \"rewards\": -1629.932113}, {\"timesteps\": 3367, \"rewards\": -1674.593587}, {\"timesteps\": 3368, \"rewards\": -1675.514521}, {\"timesteps\": 3369, \"rewards\": -1687.169876}, {\"timesteps\": 3370, \"rewards\": -1125.759736}, {\"timesteps\": 3371, \"rewards\": -1160.484323}, {\"timesteps\": 3372, \"rewards\": -1489.616814}, {\"timesteps\": 3373, \"rewards\": -1159.064075}, {\"timesteps\": 3374, \"rewards\": -1764.075049}, {\"timesteps\": 3375, \"rewards\": -1064.779752}, {\"timesteps\": 3376, \"rewards\": -1795.893391}, {\"timesteps\": 3377, \"rewards\": -1877.8661}, {\"timesteps\": 3378, \"rewards\": -1914.420751}, {\"timesteps\": 3379, \"rewards\": -1432.522582}, {\"timesteps\": 3380, \"rewards\": -850.930345}, {\"timesteps\": 3381, \"rewards\": -1508.903725}, {\"timesteps\": 3382, \"rewards\": -1652.466348}, {\"timesteps\": 3383, \"rewards\": -1664.058722}, {\"timesteps\": 3384, \"rewards\": -1893.669254}, {\"timesteps\": 3385, \"rewards\": -887.907972}, {\"timesteps\": 3386, \"rewards\": -1052.511044}, {\"timesteps\": 3387, \"rewards\": -1218.763349}, {\"timesteps\": 3388, \"rewards\": -1473.970905}, {\"timesteps\": 3389, \"rewards\": -1577.475561}, {\"timesteps\": 3390, \"rewards\": -997.884484}, {\"timesteps\": 3391, \"rewards\": -1520.378588}, {\"timesteps\": 3392, \"rewards\": -1653.818986}, {\"timesteps\": 3393, \"rewards\": -1704.395023}, {\"timesteps\": 3394, \"rewards\": -1544.505318}, {\"timesteps\": 3395, \"rewards\": -1308.677555}, {\"timesteps\": 3396, \"rewards\": -1401.980465}, {\"timesteps\": 3397, \"rewards\": -1638.04914}, {\"timesteps\": 3398, \"rewards\": -1513.890416}, {\"timesteps\": 3399, \"rewards\": -1036.174124}, {\"timesteps\": 3400, \"rewards\": -1026.934925}, {\"timesteps\": 3401, \"rewards\": -1491.328151}, {\"timesteps\": 3402, \"rewards\": -1347.774843}, {\"timesteps\": 3403, \"rewards\": -1696.561828}, {\"timesteps\": 3404, \"rewards\": -935.451153}, {\"timesteps\": 3405, \"rewards\": -1404.203618}, {\"timesteps\": 3406, \"rewards\": -1585.531158}, {\"timesteps\": 3407, \"rewards\": -1672.883039}, {\"timesteps\": 3408, \"rewards\": -1843.011453}, {\"timesteps\": 3409, \"rewards\": -1809.58495}, {\"timesteps\": 3410, \"rewards\": -863.170727}, {\"timesteps\": 3411, \"rewards\": -1109.728055}, {\"timesteps\": 3412, \"rewards\": -1528.900171}, {\"timesteps\": 3413, \"rewards\": -1643.12248}, {\"timesteps\": 3414, \"rewards\": -1550.472646}, {\"timesteps\": 3415, \"rewards\": -1422.635796}, {\"timesteps\": 3416, \"rewards\": -1618.579862}, {\"timesteps\": 3417, \"rewards\": -1608.411535}, {\"timesteps\": 3418, \"rewards\": -1868.057488}, {\"timesteps\": 3419, \"rewards\": -1672.512771}, {\"timesteps\": 3420, \"rewards\": -1541.244883}, {\"timesteps\": 3421, \"rewards\": -1563.084864}, {\"timesteps\": 3422, \"rewards\": -1630.952086}, {\"timesteps\": 3423, \"rewards\": -1673.767266}, {\"timesteps\": 3424, \"rewards\": -1341.530209}, {\"timesteps\": 3425, \"rewards\": -1348.747243}, {\"timesteps\": 3426, \"rewards\": -1410.160212}, {\"timesteps\": 3427, \"rewards\": -1500.504659}, {\"timesteps\": 3428, \"rewards\": -1787.506462}, {\"timesteps\": 3429, \"rewards\": -1254.14234}, {\"timesteps\": 3430, \"rewards\": -854.118619}, {\"timesteps\": 3431, \"rewards\": -1263.14155}, {\"timesteps\": 3432, \"rewards\": -1247.110892}, {\"timesteps\": 3433, \"rewards\": -1434.65945}, {\"timesteps\": 3434, \"rewards\": -1577.179484}, {\"timesteps\": 3435, \"rewards\": -1349.924573}, {\"timesteps\": 3436, \"rewards\": -1546.457198}, {\"timesteps\": 3437, \"rewards\": -1841.934061}, {\"timesteps\": 3438, \"rewards\": -1788.300473}, {\"timesteps\": 3439, \"rewards\": -1417.201438}, {\"timesteps\": 3440, \"rewards\": -1251.468944}, {\"timesteps\": 3441, \"rewards\": -1540.777278}, {\"timesteps\": 3442, \"rewards\": -1455.724227}, {\"timesteps\": 3443, \"rewards\": -1603.505419}, {\"timesteps\": 3444, \"rewards\": -1809.894706}, {\"timesteps\": 3445, \"rewards\": -920.259409}, {\"timesteps\": 3446, \"rewards\": -1367.528544}, {\"timesteps\": 3447, \"rewards\": -1595.136232}, {\"timesteps\": 3448, \"rewards\": -1810.830686}, {\"timesteps\": 3449, \"rewards\": -1471.522478}, {\"timesteps\": 3450, \"rewards\": -830.437525}, {\"timesteps\": 3451, \"rewards\": -1215.704305}, {\"timesteps\": 3452, \"rewards\": -1298.957817}, {\"timesteps\": 3453, \"rewards\": -1744.014853}, {\"timesteps\": 3454, \"rewards\": -1811.578018}, {\"timesteps\": 3455, \"rewards\": -1456.963241}, {\"timesteps\": 3456, \"rewards\": -1337.191695}, {\"timesteps\": 3457, \"rewards\": -1595.13701}, {\"timesteps\": 3458, \"rewards\": -1634.042651}, {\"timesteps\": 3459, \"rewards\": -1590.971352}, {\"timesteps\": 3460, \"rewards\": -870.640623}, {\"timesteps\": 3461, \"rewards\": -1037.448077}, {\"timesteps\": 3462, \"rewards\": -1409.405801}, {\"timesteps\": 3463, \"rewards\": -1684.528305}, {\"timesteps\": 3464, \"rewards\": -1178.688723}, {\"timesteps\": 3465, \"rewards\": -1259.990759}, {\"timesteps\": 3466, \"rewards\": -1603.526129}, {\"timesteps\": 3467, \"rewards\": -1613.058497}, {\"timesteps\": 3468, \"rewards\": -1656.72303}, {\"timesteps\": 3469, \"rewards\": -1502.335461}, {\"timesteps\": 3470, \"rewards\": -1058.965988}, {\"timesteps\": 3471, \"rewards\": -1374.808828}, {\"timesteps\": 3472, \"rewards\": -1660.278654}, {\"timesteps\": 3473, \"rewards\": -1664.703916}, {\"timesteps\": 3474, \"rewards\": -1507.515851}, {\"timesteps\": 3475, \"rewards\": -1085.306326}, {\"timesteps\": 3476, \"rewards\": -1407.191781}, {\"timesteps\": 3477, \"rewards\": -1555.885757}, {\"timesteps\": 3478, \"rewards\": -1574.163063}, {\"timesteps\": 3479, \"rewards\": -1795.405777}, {\"timesteps\": 3480, \"rewards\": -843.861072}, {\"timesteps\": 3481, \"rewards\": -1003.257194}, {\"timesteps\": 3482, \"rewards\": -1460.995559}, {\"timesteps\": 3483, \"rewards\": -1517.181451}, {\"timesteps\": 3484, \"rewards\": -1341.013637}, {\"timesteps\": 3485, \"rewards\": -999.351842}, {\"timesteps\": 3486, \"rewards\": -1328.640768}, {\"timesteps\": 3487, \"rewards\": -1393.307347}, {\"timesteps\": 3488, \"rewards\": -1744.517857}, {\"timesteps\": 3489, \"rewards\": -1666.00725}, {\"timesteps\": 3490, \"rewards\": -780.820555}, {\"timesteps\": 3491, \"rewards\": -985.679056}, {\"timesteps\": 3492, \"rewards\": -1295.688732}, {\"timesteps\": 3493, \"rewards\": -1530.793791}, {\"timesteps\": 3494, \"rewards\": -1053.996891}, {\"timesteps\": 3495, \"rewards\": -1112.539772}, {\"timesteps\": 3496, \"rewards\": -1492.981553}, {\"timesteps\": 3497, \"rewards\": -1556.338511}, {\"timesteps\": 3498, \"rewards\": -1695.936045}, {\"timesteps\": 3499, \"rewards\": -1844.205321}, {\"timesteps\": 3500, \"rewards\": -1338.06276}, {\"timesteps\": 3501, \"rewards\": -1428.623194}, {\"timesteps\": 3502, \"rewards\": -1641.136462}, {\"timesteps\": 3503, \"rewards\": -1764.38403}, {\"timesteps\": 3504, \"rewards\": -1137.213418}, {\"timesteps\": 3505, \"rewards\": -840.893397}, {\"timesteps\": 3506, \"rewards\": -1287.182501}, {\"timesteps\": 3507, \"rewards\": -1278.733807}, {\"timesteps\": 3508, \"rewards\": -1715.840982}, {\"timesteps\": 3509, \"rewards\": -1085.440158}, {\"timesteps\": 3510, \"rewards\": -1141.702897}, {\"timesteps\": 3511, \"rewards\": -1254.776307}, {\"timesteps\": 3512, \"rewards\": -1524.294165}, {\"timesteps\": 3513, \"rewards\": -1648.450031}, {\"timesteps\": 3514, \"rewards\": -1380.685872}, {\"timesteps\": 3515, \"rewards\": -1160.259764}, {\"timesteps\": 3516, \"rewards\": -1145.451831}, {\"timesteps\": 3517, \"rewards\": -1495.925966}, {\"timesteps\": 3518, \"rewards\": -1660.504408}, {\"timesteps\": 3519, \"rewards\": -1346.536179}, {\"timesteps\": 3520, \"rewards\": -934.45786}, {\"timesteps\": 3521, \"rewards\": -1490.66845}, {\"timesteps\": 3522, \"rewards\": -1274.44109}, {\"timesteps\": 3523, \"rewards\": -1542.838436}, {\"timesteps\": 3524, \"rewards\": -1261.137857}, {\"timesteps\": 3525, \"rewards\": -833.385714}, {\"timesteps\": 3526, \"rewards\": -972.696537}, {\"timesteps\": 3527, \"rewards\": -1240.704869}, {\"timesteps\": 3528, \"rewards\": -1585.049114}, {\"timesteps\": 3529, \"rewards\": -1527.288337}, {\"timesteps\": 3530, \"rewards\": -1109.004588}, {\"timesteps\": 3531, \"rewards\": -1228.107198}, {\"timesteps\": 3532, \"rewards\": -1173.7579}, {\"timesteps\": 3533, \"rewards\": -1375.598746}, {\"timesteps\": 3534, \"rewards\": -1353.983805}, {\"timesteps\": 3535, \"rewards\": -822.034907}, {\"timesteps\": 3536, \"rewards\": -1156.307391}, {\"timesteps\": 3537, \"rewards\": -1603.341028}, {\"timesteps\": 3538, \"rewards\": -1466.041682}, {\"timesteps\": 3539, \"rewards\": -2008.764728}, {\"timesteps\": 3540, \"rewards\": -1092.231369}, {\"timesteps\": 3541, \"rewards\": -1235.501122}, {\"timesteps\": 3542, \"rewards\": -1410.138307}, {\"timesteps\": 3543, \"rewards\": -1844.419397}, {\"timesteps\": 3544, \"rewards\": -1493.848404}, {\"timesteps\": 3545, \"rewards\": -986.543482}, {\"timesteps\": 3546, \"rewards\": -1082.93305}, {\"timesteps\": 3547, \"rewards\": -1712.792702}, {\"timesteps\": 3548, \"rewards\": -2168.964628}, {\"timesteps\": 3549, \"rewards\": -1729.351211}, {\"timesteps\": 3550, \"rewards\": -837.346968}, {\"timesteps\": 3551, \"rewards\": -1053.429442}, {\"timesteps\": 3552, \"rewards\": -1443.208989}, {\"timesteps\": 3553, \"rewards\": -1649.018971}, {\"timesteps\": 3554, \"rewards\": -1834.859125}, {\"timesteps\": 3555, \"rewards\": -1321.412565}, {\"timesteps\": 3556, \"rewards\": -1441.536241}, {\"timesteps\": 3557, \"rewards\": -1700.768077}, {\"timesteps\": 3558, \"rewards\": -2187.364918}, {\"timesteps\": 3559, \"rewards\": -1844.933234}, {\"timesteps\": 3560, \"rewards\": -728.371908}, {\"timesteps\": 3561, \"rewards\": -1018.645135}, {\"timesteps\": 3562, \"rewards\": -1770.998866}, {\"timesteps\": 3563, \"rewards\": -2294.029981}, {\"timesteps\": 3564, \"rewards\": -1922.737307}, {\"timesteps\": 3565, \"rewards\": -830.728331}, {\"timesteps\": 3566, \"rewards\": -1171.585284}, {\"timesteps\": 3567, \"rewards\": -1410.421039}, {\"timesteps\": 3568, \"rewards\": -1552.396576}, {\"timesteps\": 3569, \"rewards\": -1033.696728}, {\"timesteps\": 3570, \"rewards\": -1186.074418}, {\"timesteps\": 3571, \"rewards\": -1319.776474}, {\"timesteps\": 3572, \"rewards\": -1781.393977}, {\"timesteps\": 3573, \"rewards\": -1986.949493}, {\"timesteps\": 3574, \"rewards\": -1715.32911}, {\"timesteps\": 3575, \"rewards\": -1290.852347}, {\"timesteps\": 3576, \"rewards\": -1195.325538}, {\"timesteps\": 3577, \"rewards\": -1663.608197}, {\"timesteps\": 3578, \"rewards\": -1675.877681}, {\"timesteps\": 3579, \"rewards\": -1154.202029}, {\"timesteps\": 3580, \"rewards\": -1444.113697}, {\"timesteps\": 3581, \"rewards\": -1624.655628}, {\"timesteps\": 3582, \"rewards\": -1793.791857}, {\"timesteps\": 3583, \"rewards\": -1805.778561}, {\"timesteps\": 3584, \"rewards\": -1317.835737}, {\"timesteps\": 3585, \"rewards\": -1400.835918}, {\"timesteps\": 3586, \"rewards\": -1499.82028}, {\"timesteps\": 3587, \"rewards\": -1736.52069}, {\"timesteps\": 3588, \"rewards\": -1936.351219}, {\"timesteps\": 3589, \"rewards\": -1724.188374}, {\"timesteps\": 3590, \"rewards\": -1417.913674}, {\"timesteps\": 3591, \"rewards\": -1882.79904}, {\"timesteps\": 3592, \"rewards\": -1887.900935}, {\"timesteps\": 3593, \"rewards\": -1990.081368}, {\"timesteps\": 3594, \"rewards\": -1920.00798}, {\"timesteps\": 3595, \"rewards\": -1354.187139}, {\"timesteps\": 3596, \"rewards\": -1677.936957}, {\"timesteps\": 3597, \"rewards\": -1782.066402}, {\"timesteps\": 3598, \"rewards\": -1964.768579}, {\"timesteps\": 3599, \"rewards\": -1247.149495}, {\"timesteps\": 3600, \"rewards\": -951.585222}, {\"timesteps\": 3601, \"rewards\": -1063.30524}, {\"timesteps\": 3602, \"rewards\": -1229.245279}, {\"timesteps\": 3603, \"rewards\": -1959.944741}, {\"timesteps\": 3604, \"rewards\": -1655.584452}, {\"timesteps\": 3605, \"rewards\": -1375.770996}, {\"timesteps\": 3606, \"rewards\": -1338.344425}, {\"timesteps\": 3607, \"rewards\": -2002.744877}, {\"timesteps\": 3608, \"rewards\": -2148.483366}, {\"timesteps\": 3609, \"rewards\": -1352.359782}, {\"timesteps\": 3610, \"rewards\": -1331.264605}, {\"timesteps\": 3611, \"rewards\": -1700.54844}, {\"timesteps\": 3612, \"rewards\": -1757.117156}, {\"timesteps\": 3613, \"rewards\": -2232.224051}, {\"timesteps\": 3614, \"rewards\": -856.551921}, {\"timesteps\": 3615, \"rewards\": -1638.342058}, {\"timesteps\": 3616, \"rewards\": -1818.476233}, {\"timesteps\": 3617, \"rewards\": -2013.318182}, {\"timesteps\": 3618, \"rewards\": -2134.310308}, {\"timesteps\": 3619, \"rewards\": -1731.319415}, {\"timesteps\": 3620, \"rewards\": -1096.332652}, {\"timesteps\": 3621, \"rewards\": -1335.454815}, {\"timesteps\": 3622, \"rewards\": -1419.150199}, {\"timesteps\": 3623, \"rewards\": -1572.862314}, {\"timesteps\": 3624, \"rewards\": -1692.641711}, {\"timesteps\": 3625, \"rewards\": -1136.107438}, {\"timesteps\": 3626, \"rewards\": -1194.663162}, {\"timesteps\": 3627, \"rewards\": -1241.511081}, {\"timesteps\": 3628, \"rewards\": -1377.290632}, {\"timesteps\": 3629, \"rewards\": -1782.043541}, {\"timesteps\": 3630, \"rewards\": -1128.287095}, {\"timesteps\": 3631, \"rewards\": -1730.712554}, {\"timesteps\": 3632, \"rewards\": -1644.562334}, {\"timesteps\": 3633, \"rewards\": -2028.320192}, {\"timesteps\": 3634, \"rewards\": -1788.8462}, {\"timesteps\": 3635, \"rewards\": -1115.029184}, {\"timesteps\": 3636, \"rewards\": -1212.873227}, {\"timesteps\": 3637, \"rewards\": -1682.0723}, {\"timesteps\": 3638, \"rewards\": -1900.33725}, {\"timesteps\": 3639, \"rewards\": -2002.416238}, {\"timesteps\": 3640, \"rewards\": -1096.474531}, {\"timesteps\": 3641, \"rewards\": -1537.081115}, {\"timesteps\": 3642, \"rewards\": -1556.472391}, {\"timesteps\": 3643, \"rewards\": -1508.122453}, {\"timesteps\": 3644, \"rewards\": -1476.772583}, {\"timesteps\": 3645, \"rewards\": -517.390998}, {\"timesteps\": 3646, \"rewards\": -954.890434}, {\"timesteps\": 3647, \"rewards\": -1469.369236}, {\"timesteps\": 3648, \"rewards\": -1513.336196}, {\"timesteps\": 3649, \"rewards\": -1451.591529}, {\"timesteps\": 3650, \"rewards\": -1164.132486}, {\"timesteps\": 3651, \"rewards\": -1154.927693}, {\"timesteps\": 3652, \"rewards\": -1229.999985}, {\"timesteps\": 3653, \"rewards\": -1367.907573}, {\"timesteps\": 3654, \"rewards\": -1666.581847}, {\"timesteps\": 3655, \"rewards\": -937.641078}, {\"timesteps\": 3656, \"rewards\": -1402.249269}, {\"timesteps\": 3657, \"rewards\": -1512.446292}, {\"timesteps\": 3658, \"rewards\": -1586.695727}, {\"timesteps\": 3659, \"rewards\": -1159.872952}, {\"timesteps\": 3660, \"rewards\": -1233.961188}, {\"timesteps\": 3661, \"rewards\": -1275.938993}, {\"timesteps\": 3662, \"rewards\": -1296.712399}, {\"timesteps\": 3663, \"rewards\": -1549.836832}, {\"timesteps\": 3664, \"rewards\": -1287.721624}, {\"timesteps\": 3665, \"rewards\": -318.402326}, {\"timesteps\": 3666, \"rewards\": -1438.054246}, {\"timesteps\": 3667, \"rewards\": -1569.577378}, {\"timesteps\": 3668, \"rewards\": -1467.981996}, {\"timesteps\": 3669, \"rewards\": -1028.476459}, {\"timesteps\": 3670, \"rewards\": -1048.406971}, {\"timesteps\": 3671, \"rewards\": -1119.221231}, {\"timesteps\": 3672, \"rewards\": -1299.91194}, {\"timesteps\": 3673, \"rewards\": -1484.570387}, {\"timesteps\": 3674, \"rewards\": -804.675544}, {\"timesteps\": 3675, \"rewards\": -428.733352}, {\"timesteps\": 3676, \"rewards\": -990.255513}, {\"timesteps\": 3677, \"rewards\": -1490.373265}, {\"timesteps\": 3678, \"rewards\": -1515.427144}, {\"timesteps\": 3679, \"rewards\": -1467.279216}, {\"timesteps\": 3680, \"rewards\": -515.95882}, {\"timesteps\": 3681, \"rewards\": -775.604999}, {\"timesteps\": 3682, \"rewards\": -1373.828997}, {\"timesteps\": 3683, \"rewards\": -1424.870222}, {\"timesteps\": 3684, \"rewards\": -1540.132301}, {\"timesteps\": 3685, \"rewards\": -1010.09988}, {\"timesteps\": 3686, \"rewards\": -1140.452349}, {\"timesteps\": 3687, \"rewards\": -1331.969482}, {\"timesteps\": 3688, \"rewards\": -1523.967195}, {\"timesteps\": 3689, \"rewards\": -1027.028738}, {\"timesteps\": 3690, \"rewards\": -798.717425}, {\"timesteps\": 3691, \"rewards\": -844.276196}, {\"timesteps\": 3692, \"rewards\": -1064.296869}, {\"timesteps\": 3693, \"rewards\": -1505.876982}, {\"timesteps\": 3694, \"rewards\": -1249.441289}, {\"timesteps\": 3695, \"rewards\": -782.273474}, {\"timesteps\": 3696, \"rewards\": -970.132257}, {\"timesteps\": 3697, \"rewards\": -1541.75383}, {\"timesteps\": 3698, \"rewards\": -1486.92872}, {\"timesteps\": 3699, \"rewards\": -1327.980641}, {\"timesteps\": 3700, \"rewards\": -1225.514273}, {\"timesteps\": 3701, \"rewards\": -1434.506913}, {\"timesteps\": 3702, \"rewards\": -1552.684249}, {\"timesteps\": 3703, \"rewards\": -1465.008639}, {\"timesteps\": 3704, \"rewards\": -1220.890509}, {\"timesteps\": 3705, \"rewards\": -483.594077}, {\"timesteps\": 3706, \"rewards\": -915.063271}, {\"timesteps\": 3707, \"rewards\": -1421.54648}, {\"timesteps\": 3708, \"rewards\": -1544.818646}, {\"timesteps\": 3709, \"rewards\": -516.411929}, {\"timesteps\": 3710, \"rewards\": -930.82057}, {\"timesteps\": 3711, \"rewards\": -1172.336072}, {\"timesteps\": 3712, \"rewards\": -1598.333382}, {\"timesteps\": 3713, \"rewards\": -1636.284423}, {\"timesteps\": 3714, \"rewards\": -910.51994}, {\"timesteps\": 3715, \"rewards\": -885.02702}, {\"timesteps\": 3716, \"rewards\": -1054.497959}, {\"timesteps\": 3717, \"rewards\": -1486.119917}, {\"timesteps\": 3718, \"rewards\": -1544.472401}, {\"timesteps\": 3719, \"rewards\": -825.038799}, {\"timesteps\": 3720, \"rewards\": -625.294866}, {\"timesteps\": 3721, \"rewards\": -1201.571073}, {\"timesteps\": 3722, \"rewards\": -1284.291645}, {\"timesteps\": 3723, \"rewards\": -1382.400969}, {\"timesteps\": 3724, \"rewards\": -838.627511}, {\"timesteps\": 3725, \"rewards\": -900.502913}, {\"timesteps\": 3726, \"rewards\": -1050.293399}, {\"timesteps\": 3727, \"rewards\": -1548.942249}, {\"timesteps\": 3728, \"rewards\": -1566.526534}, {\"timesteps\": 3729, \"rewards\": -907.927088}, {\"timesteps\": 3730, \"rewards\": -433.218339}, {\"timesteps\": 3731, \"rewards\": -918.345589}, {\"timesteps\": 3732, \"rewards\": -1421.243591}, {\"timesteps\": 3733, \"rewards\": -1442.765039}, {\"timesteps\": 3734, \"rewards\": -1389.862396}, {\"timesteps\": 3735, \"rewards\": -566.294538}, {\"timesteps\": 3736, \"rewards\": -1146.241365}, {\"timesteps\": 3737, \"rewards\": -1410.008209}, {\"timesteps\": 3738, \"rewards\": -1581.439625}, {\"timesteps\": 3739, \"rewards\": -1572.162711}, {\"timesteps\": 3740, \"rewards\": -333.387142}, {\"timesteps\": 3741, \"rewards\": -709.45009}, {\"timesteps\": 3742, \"rewards\": -886.905552}, {\"timesteps\": 3743, \"rewards\": -1075.628258}, {\"timesteps\": 3744, \"rewards\": -1006.108753}, {\"timesteps\": 3745, \"rewards\": -479.24838}, {\"timesteps\": 3746, \"rewards\": -631.628278}, {\"timesteps\": 3747, \"rewards\": -935.549306}, {\"timesteps\": 3748, \"rewards\": -306.099622}, {\"timesteps\": 3749, \"rewards\": -1265.654107}, {\"timesteps\": 3750, \"rewards\": -830.503253}, {\"timesteps\": 3751, \"rewards\": -944.737679}, {\"timesteps\": 3752, \"rewards\": -958.225454}, {\"timesteps\": 3753, \"rewards\": -1052.290971}, {\"timesteps\": 3754, \"rewards\": -971.003846}, {\"timesteps\": 3755, \"rewards\": -412.707124}, {\"timesteps\": 3756, \"rewards\": -861.300071}, {\"timesteps\": 3757, \"rewards\": -1053.088357}, {\"timesteps\": 3758, \"rewards\": -1230.817579}, {\"timesteps\": 3759, \"rewards\": -413.294032}, {\"timesteps\": 3760, \"rewards\": -823.837607}, {\"timesteps\": 3761, \"rewards\": -879.091382}, {\"timesteps\": 3762, \"rewards\": -1132.570149}, {\"timesteps\": 3763, \"rewards\": -1372.637623}, {\"timesteps\": 3764, \"rewards\": -820.627595}, {\"timesteps\": 3765, \"rewards\": -591.811141}, {\"timesteps\": 3766, \"rewards\": -865.181015}, {\"timesteps\": 3767, \"rewards\": -906.022128}, {\"timesteps\": 3768, \"rewards\": -1058.621359}, {\"timesteps\": 3769, \"rewards\": -872.473996}, {\"timesteps\": 3770, \"rewards\": -985.955946}, {\"timesteps\": 3771, \"rewards\": -1014.948512}, {\"timesteps\": 3772, \"rewards\": -1044.778672}, {\"timesteps\": 3773, \"rewards\": -1338.274376}, {\"timesteps\": 3774, \"rewards\": -1388.237638}, {\"timesteps\": 3775, \"rewards\": -499.444671}, {\"timesteps\": 3776, \"rewards\": -799.374288}, {\"timesteps\": 3777, \"rewards\": -965.692241}, {\"timesteps\": 3778, \"rewards\": -1063.773424}, {\"timesteps\": 3779, \"rewards\": -702.320815}, {\"timesteps\": 3780, \"rewards\": -198.149325}, {\"timesteps\": 3781, \"rewards\": -419.022054}, {\"timesteps\": 3782, \"rewards\": -839.349195}, {\"timesteps\": 3783, \"rewards\": -855.566788}, {\"timesteps\": 3784, \"rewards\": -1123.684555}, {\"timesteps\": 3785, \"rewards\": -467.870502}, {\"timesteps\": 3786, \"rewards\": -840.738192}, {\"timesteps\": 3787, \"rewards\": -930.495681}, {\"timesteps\": 3788, \"rewards\": -1106.906685}, {\"timesteps\": 3789, \"rewards\": -612.995098}, {\"timesteps\": 3790, \"rewards\": -512.770401}, {\"timesteps\": 3791, \"rewards\": -663.922642}, {\"timesteps\": 3792, \"rewards\": -1027.264473}, {\"timesteps\": 3793, \"rewards\": -995.993149}, {\"timesteps\": 3794, \"rewards\": -728.33644}, {\"timesteps\": 3795, \"rewards\": -608.458375}, {\"timesteps\": 3796, \"rewards\": -643.563101}, {\"timesteps\": 3797, \"rewards\": -925.381807}, {\"timesteps\": 3798, \"rewards\": -1232.219323}, {\"timesteps\": 3799, \"rewards\": -538.639734}, {\"timesteps\": 3800, \"rewards\": -606.597429}, {\"timesteps\": 3801, \"rewards\": -1152.369903}, {\"timesteps\": 3802, \"rewards\": -1235.629222}, {\"timesteps\": 3803, \"rewards\": -1325.835925}, {\"timesteps\": 3804, \"rewards\": -627.035021}, {\"timesteps\": 3805, \"rewards\": -384.253789}, {\"timesteps\": 3806, \"rewards\": -425.940255}, {\"timesteps\": 3807, \"rewards\": -690.801175}, {\"timesteps\": 3808, \"rewards\": -995.113085}, {\"timesteps\": 3809, \"rewards\": -887.893797}, {\"timesteps\": 3810, \"rewards\": -437.748938}, {\"timesteps\": 3811, \"rewards\": -952.211342}, {\"timesteps\": 3812, \"rewards\": -949.034296}, {\"timesteps\": 3813, \"rewards\": -1048.647321}, {\"timesteps\": 3814, \"rewards\": -1142.228855}, {\"timesteps\": 3815, \"rewards\": -905.500915}, {\"timesteps\": 3816, \"rewards\": -1041.939197}, {\"timesteps\": 3817, \"rewards\": -1126.230168}, {\"timesteps\": 3818, \"rewards\": -1356.459422}, {\"timesteps\": 3819, \"rewards\": -750.407507}, {\"timesteps\": 3820, \"rewards\": -535.808167}, {\"timesteps\": 3821, \"rewards\": -625.787887}, {\"timesteps\": 3822, \"rewards\": -541.131037}, {\"timesteps\": 3823, \"rewards\": -539.641661}, {\"timesteps\": 3824, \"rewards\": -1514.99708}, {\"timesteps\": 3825, \"rewards\": -184.501068}, {\"timesteps\": 3826, \"rewards\": -803.695579}, {\"timesteps\": 3827, \"rewards\": -985.267381}, {\"timesteps\": 3828, \"rewards\": -1240.364185}, {\"timesteps\": 3829, \"rewards\": -1308.200512}, {\"timesteps\": 3830, \"rewards\": -597.630657}, {\"timesteps\": 3831, \"rewards\": -950.031409}, {\"timesteps\": 3832, \"rewards\": -1006.674912}, {\"timesteps\": 3833, \"rewards\": -1147.471354}, {\"timesteps\": 3834, \"rewards\": -1015.037562}, {\"timesteps\": 3835, \"rewards\": -456.695628}, {\"timesteps\": 3836, \"rewards\": -668.012838}, {\"timesteps\": 3837, \"rewards\": -927.691147}, {\"timesteps\": 3838, \"rewards\": -988.274286}, {\"timesteps\": 3839, \"rewards\": -947.373211}, {\"timesteps\": 3840, \"rewards\": -627.224983}, {\"timesteps\": 3841, \"rewards\": -757.953778}, {\"timesteps\": 3842, \"rewards\": -1308.18406}, {\"timesteps\": 3843, \"rewards\": -1316.589211}, {\"timesteps\": 3844, \"rewards\": -950.02754}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.HConcatChart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}